Index: pom.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pom.xml	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ pom.xml	(revision c8c4456601d5c057e1de2272ffa2dfc1ea81e8b8)
@@ -33,6 +33,7 @@
     <maven.compiler.source>1.8</maven.compiler.source>
     <maven.compiler.target>1.8</maven.compiler.target>
     <encoding>UTF-8</encoding>
+      <kotlin.version>1.3.41</kotlin.version>
   </properties>
   <dependencies>
 
@@ -96,7 +97,14 @@
       <version>2.9.4</version>
     </dependency>
 
-    <dependency>
+      <dependency>
+          <groupId>com.opencsv</groupId>
+          <artifactId>opencsv</artifactId>
+          <version>4.0</version>
+      </dependency>
+
+
+      <dependency>
       <groupId>com.googlecode.efficient-java-matrix-library</groupId>
       <artifactId>ejml</artifactId>
       <version>0.23</version>
@@ -110,7 +118,7 @@
 
     <dependency>
       <groupId>org.slf4j</groupId>
-      <artifactId>slf4j-api</artifactId>
+      <artifactId>slf4j-simple</artifactId>
       <version>1.7.12</version>
     </dependency>
 
@@ -164,6 +172,22 @@
       <artifactId>jaxb-impl</artifactId>
       <version>2.4.0-b180830.0438</version>
     </dependency>
+      <dependency>
+          <groupId>org.jetbrains.kotlin</groupId>
+          <artifactId>kotlin-stdlib-jdk8</artifactId>
+          <version>${kotlin.version}</version>
+      </dependency>
+      <dependency>
+          <groupId>org.jetbrains.kotlin</groupId>
+          <artifactId>kotlin-test</artifactId>
+          <version>${kotlin.version}</version>
+          <scope>test</scope>
+      </dependency>
+      <dependency>
+          <groupId>org.jetbrains.kotlin</groupId>
+          <artifactId>kotlin-stdlib-jdk8</artifactId>
+          <version>${kotlin.version}</version>
+      </dependency>
 
   </dependencies>
   <build>
@@ -193,6 +217,54 @@
           </execution>
         </executions>
       </plugin>
+        <plugin>
+            <groupId>org.jetbrains.kotlin</groupId>
+            <artifactId>kotlin-maven-plugin</artifactId>
+            <version>${kotlin.version}</version>
+            <executions>
+                <execution>
+                    <id>compile</id>
+                    <phase>compile</phase>
+                    <goals>
+                        <goal>compile</goal>
+                    </goals>
+                </execution>
+                <execution>
+                    <id>test-compile</id>
+                    <phase>test-compile</phase>
+                    <goals>
+                        <goal>test-compile</goal>
+                    </goals>
+                </execution>
+            </executions>
+            <configuration>
+                <jvmTarget>1.8</jvmTarget>
+            </configuration>
+        </plugin>
+        <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-compiler-plugin</artifactId>
+            <executions>
+                <execution>
+                    <id>compile</id>
+                    <phase>compile</phase>
+                    <goals>
+                        <goal>compile</goal>
+                    </goals>
+                </execution>
+                <execution>
+                    <id>testCompile</id>
+                    <phase>test-compile</phase>
+                    <goals>
+                        <goal>testCompile</goal>
+                    </goals>
+                </execution>
+            </executions>
+            <configuration>
+                <source>9</source>
+                <target>9</target>
+            </configuration>
+        </plugin>
     </plugins>
   </build>
 </project>
Index: src/edu/stanford/nlp/sequences/ExactBestSequenceFinder.java
===================================================================
--- src/edu/stanford/nlp/sequences/ExactBestSequenceFinder.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/sequences/ExactBestSequenceFinder.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
@@ -1,201 +0,0 @@
-package edu.stanford.nlp.sequences; 
-
-import edu.stanford.nlp.util.Pair;
-import edu.stanford.nlp.util.RuntimeInterruptedException;
-import edu.stanford.nlp.util.logging.Redwood;
-
-import java.util.Arrays;
-
-
-/**
- * A class capable of computing the best sequence given a SequenceModel.
- * Uses the Viterbi algorithm.
- *
- * @author Dan Klein
- * @author Teg Grenager (grenager@stanford.edu)
- */
-public class ExactBestSequenceFinder implements BestSequenceFinder  {
-
-  /** A logger for this class */
-  private static final Redwood.RedwoodChannels log = Redwood.channels(ExactBestSequenceFinder.class);
-
-  private static final boolean DEBUG = false;
-
-  public static Pair<int[], Double> bestSequenceWithLinearConstraints(SequenceModel ts, double[][] linearConstraints) {
-    return bestSequence(ts, linearConstraints);
-  }
-
-  /**
-   * Runs the Viterbi algorithm on the sequence model given by the TagScorer
-   * in order to find the best sequence.
-   *
-   * @param ts The SequenceModel to be used for scoring
-   * @return An array containing the int tags of the best sequence
-   */
-  @Override
-  public int[] bestSequence(SequenceModel ts) {
-    return bestSequence(ts, null).first();
-  }
-
-  private static Pair<int[], Double> bestSequence(SequenceModel ts, double[][] linearConstraints) {
-    // Set up tag options
-    int length = ts.length();
-    int leftWindow = ts.leftWindow();
-    int rightWindow = ts.rightWindow();
-    int padLength = length + leftWindow + rightWindow;
-    if (linearConstraints != null && linearConstraints.length != padLength)
-      throw new RuntimeException("linearConstraints.length (" +  linearConstraints.length + ") does not match padLength (" + padLength + ") of SequenceModel" + ", length=="+length+", leftW="+leftWindow+", rightW="+rightWindow);
-    int[][] tags = new int[padLength][];
-    int[] tagNum = new int[padLength];
-    if (DEBUG) { log.info("Doing bestSequence length " + length + "; leftWin " + leftWindow + "; rightWin " + rightWindow + "; padLength " + padLength); }
-    for (int pos = 0; pos < padLength; pos++) {
-      if (Thread.interrupted()) {  // Allow interrupting
-        throw new RuntimeInterruptedException();
-      }
-      tags[pos] = ts.getPossibleValues(pos);
-      tagNum[pos] = tags[pos].length;
-      if (DEBUG) { log.info("There are " + tagNum[pos] + " values at position " + pos + ": " + Arrays.toString(tags[pos])); }
-    }
-
-    int[] tempTags = new int[padLength];
-
-    // Set up product space sizes
-    int[] productSizes = new int[padLength];
-
-    int curProduct = 1;
-    for (int i = 0; i < leftWindow + rightWindow; i++) {
-      curProduct *= tagNum[i];
-    }
-    for (int pos = leftWindow + rightWindow; pos < padLength; pos++) {
-      if (Thread.interrupted()) {  // Allow interrupting
-        throw new RuntimeInterruptedException();
-      }
-      if (pos > leftWindow + rightWindow) {
-        curProduct /= tagNum[pos - leftWindow - rightWindow - 1]; // shift off
-      }
-      curProduct *= tagNum[pos]; // shift on
-      productSizes[pos - rightWindow] = curProduct;
-    }
-
-    // Score all of each window's options
-    double[][] windowScore = new double[padLength][];
-    for (int pos = leftWindow; pos < leftWindow + length; pos++) {
-      if (Thread.interrupted()) {  // Allow interrupting
-        throw new RuntimeInterruptedException();
-      }
-      if (DEBUG) { log.info("scoring word " + pos + " / " + (leftWindow + length) + ", productSizes =  " + productSizes[pos] + ", tagNum = " + tagNum[pos] + "..."); }
-      windowScore[pos] = new double[productSizes[pos]];
-      Arrays.fill(tempTags, tags[0][0]);
-      if (DEBUG) { log.info("windowScore[" + pos + "] has size (productSizes[pos]) " + windowScore[pos].length); }
-
-      for (int product = 0; product < productSizes[pos]; product++) {
-        int p = product;
-        int shift = 1;
-        for (int curPos = pos + rightWindow; curPos >= pos - leftWindow; curPos--) {
-          tempTags[curPos] = tags[curPos][p % tagNum[curPos]];
-          p /= tagNum[curPos];
-          if (curPos > pos) {
-            shift *= tagNum[curPos];
-          }
-        }
-
-        // Here now you get ts.scoresOf() for all classifications at a position at once, whereas the old code called ts.scoreOf() on each item.
-        // CDM May 2007: The way this is done gives incorrect results if there are repeated values in the values of ts.getPossibleValues(pos) -- in particular if the first value of the array is repeated later.  I tried replacing it with the modulo version, but that only worked for left-to-right, not bidirectional inference, but I still think that if you sorted things out, you should be able to do it with modulos and the result would be conceptually simpler and robust to repeated values.  But in the meantime, I fixed the POS tagger to not give repeated values (which was a bug in the tagger).
-        if (tempTags[pos] == tags[pos][0]) {
-          // get all tags at once
-          double[] scores = ts.scoresOf(tempTags, pos);
-          if (DEBUG) { log.info("Matched at array index [product] " + product + "; tempTags[pos] == tags[pos][0] == " + tempTags[pos]); }
-          if (DEBUG) { log.info("For pos " + pos + " scores.length is " + scores.length + "; tagNum[pos] = " + tagNum[pos] + "; windowScore[pos].length = " + windowScore[pos].length); }
-          if (DEBUG) { log.info("scores: " + Arrays.toString(scores)); }
-          // fill in the relevant windowScores
-          for (int t = 0; t < tagNum[pos]; t++) {
-            if (DEBUG) { log.info("Setting value of windowScore[" + pos + "][" + product + "+" + t + "*" + shift + "] = " + scores[t]); }
-            windowScore[pos][product + t * shift] = scores[t];
-          }
-        }
-      }
-    }
-
-    // Set up score and backtrace arrays
-    double[][] score = new double[padLength][];
-    int[][] trace = new int[padLength][];
-    for (int pos = 0; pos < padLength; pos++) {
-      score[pos] = new double[productSizes[pos]];
-      trace[pos] = new int[productSizes[pos]];
-    }
-
-    // Do forward Viterbi algorithm
-
-    // loop over the classification spot
-    //log.info();
-    for (int pos = leftWindow; pos < length + leftWindow; pos++) {
-      //log.info(".");
-      // loop over window product types
-      for (int product = 0; product < productSizes[pos]; product++) {
-        if (Thread.interrupted()) {  // Allow interrupting
-          throw new RuntimeInterruptedException();
-        }
-        // check for initial spot
-        if (pos == leftWindow) {
-          // no predecessor type
-          score[pos][product] = windowScore[pos][product];
-          if (linearConstraints != null) {
-            if (DEBUG) {
-              if (linearConstraints[pos][product % tagNum[pos]] != 0) {
-                log.info("Applying linear constraints=" + linearConstraints[pos][product % tagNum[pos]] + " to preScore="+ windowScore[pos][product] + " at pos="+pos+" for tag="+(product % tagNum[pos]));
-              }
-            }
-            score[pos][product] += linearConstraints[pos][product % tagNum[pos]];
-          }
-          trace[pos][product] = -1;
-        } else {
-          // loop over possible predecessor types
-          score[pos][product] = Double.NEGATIVE_INFINITY;
-          trace[pos][product] = -1;
-          int sharedProduct = product / tagNum[pos + rightWindow];
-          int factor = productSizes[pos] / tagNum[pos + rightWindow];
-          for (int newTagNum = 0; newTagNum < tagNum[pos - leftWindow - 1]; newTagNum++) {
-            int predProduct = newTagNum * factor + sharedProduct;
-            double predScore = score[pos - 1][predProduct] + windowScore[pos][product];
-
-            if (linearConstraints != null) {
-              if (DEBUG) {
-                if (pos == 2 && linearConstraints[pos][product % tagNum[pos]] != 0) {
-                  log.info("Applying linear constraints=" + linearConstraints[pos][product % tagNum[pos]] + " to preScore="+ predScore + " at pos="+pos+" for tag="+(product % tagNum[pos]));
-                  log.info("predScore:" + predScore + " = score["+(pos - 1)+"]["+predProduct+"]:" + score[pos - 1][predProduct] + " + windowScore["+pos+"]["+product+"]:" + windowScore[pos][product]);
-                }
-              }
-              predScore += linearConstraints[pos][product % tagNum[pos]];
-            }
-
-            if (predScore > score[pos][product]) {
-              score[pos][product] = predScore;
-              trace[pos][product] = predProduct;
-            }
-          }
-        }
-      }
-    }
-
-    // Project the actual tag sequence
-    double bestFinalScore = Double.NEGATIVE_INFINITY;
-    int bestCurrentProduct = -1;
-    for (int product = 0; product < productSizes[leftWindow + length - 1]; product++) {
-      if (score[leftWindow + length - 1][product] > bestFinalScore) {
-        bestCurrentProduct = product;
-        bestFinalScore = score[leftWindow + length - 1][product];
-      }
-    }
-    int lastProduct = bestCurrentProduct;
-    for (int last = padLength - 1; last >= length - 1 && last >= 0; last--) {
-      tempTags[last] = tags[last][lastProduct % tagNum[last]];
-      lastProduct /= tagNum[last];
-    }
-    for (int pos = leftWindow + length - 2; pos >= leftWindow; pos--) {
-      int bestNextProduct = bestCurrentProduct;
-      bestCurrentProduct = trace[pos + 1][bestNextProduct];
-      tempTags[pos - leftWindow] = tags[pos - leftWindow][bestCurrentProduct / (productSizes[pos] / tagNum[pos - leftWindow])];
-    }
-    return new Pair<>(tempTags, bestFinalScore);
-  }
-}
Index: src/edu/stanford/nlp/tagger/maxent/AmbiguityClass.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/AmbiguityClass.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/AmbiguityClass.java	(revision f3bf217addd8a9e5a5e4216a488f94870df00bec)
@@ -26,7 +26,6 @@
 //http://www-nlp.stanford.edu/software/tagger.shtml
 
 
-
 package edu.stanford.nlp.tagger.maxent;
 
 
@@ -41,96 +40,66 @@
  */
 public class AmbiguityClass {
 
-  // private final HashSet<String> s;
-  private final List<Integer> sortedIds;
-  private final String key;
-  private final String word;
-  private final boolean single;
+    // private final HashSet<String> s;
+    private final List<Integer> sortedIds;
+    private final String key;
+    private final String word;
+    private final boolean single;
 
-  protected AmbiguityClass(String word, boolean single, Dictionary dict, TTags ttags) {
-    this.single = single;
-    if (single) {
-      this.word = word;
-      sortedIds = Collections.emptyList();
-    } else {
-      this.word = null;
-      String[] tags = dict.getTags(word);
-      sortedIds = new ArrayList<>(tags.length);
-      for (String tag : tags) {
-        add(ttags.getIndex(tag));
-      }
-      // s = Generics.newHashSet();
-      // for (Integer sortedId : sortedIds) {
-      //   s.add(ttags.getTag(sortedId));
-      // }
-    }
-    key = this.toString();
-  }
+    protected AmbiguityClass(String word, boolean single, Dictionary dict, TTags ttags) {
+        this.single = single;
+        if (single) {
+            this.word = word;
+            sortedIds = Collections.emptyList();
+        } else {
+            this.word = null;
+            String[] tags = dict.getTags(word);
+            sortedIds = new ArrayList<>(tags.length);
+            for (String tag : tags) {
+                add(ttags.indexOf(tag));
+            }
+        }
+        key = this.toString();
+    }
 
-  public String getWord() {
-    return word;
-  }
+    public String getWord() {
+        return word;
+    }
 
-  /*
-  public boolean belongs(String word) {
-    String[] tags = GlobalHolder.dict.getTags(word);
-    if (tags.length != sortedIds.size()) {
-      return false;
-    }
-    for (int i = 0; i < tags.length; i++) {
-      if (!s.contains(tags[i])) {
-        return false;
-      }
-    }
-    members++;
-    return true;
-  } // belongs
-  */
-
-  private boolean add(int tagId) {
-    for (int j = 0; j < sortedIds.size(); j++) {
-      if (tagId < sortedIds.get(j)) {
-        sortedIds.add(j, tagId);
-        return true;
-      }
-      if (tagId == sortedIds.get(j)) {
-        return false;
-      }
-    }
-    sortedIds.add(tagId);
-    return true;
-  }
+    private boolean add(int tagId) {
+        for (int j = 0; j < sortedIds.size(); j++) {
+            if (tagId < sortedIds.get(j)) {
+                sortedIds.add(j, tagId);
+                return true;
+            }
+            if (tagId == sortedIds.get(j)) {
+                return false;
+            }
+        }
+        sortedIds.add(tagId);
+        return true;
+    }
 
-  @Override
-  public String toString() {
-    if (single) {
-      return word;
-    }
-    StringBuilder sb = new StringBuilder();
-    for (Integer sID : sortedIds) {
-      sb.append(':').append(sID.intValue());
-    }
-    return sb.toString();
-  }
+    @Override
+    public String toString() {
+        if (single) {
+            return word;
+        }
+        StringBuilder sb = new StringBuilder();
+        for (Integer sID : sortedIds) {
+            sb.append(':').append(sID.intValue());
+        }
+        return sb.toString();
+    }
 
-  /*
-  public void print() {
-    //System.out.print(word + " ");
-    for (Integer sortedId : sortedIds) {
-      System.out.print(GlobalHolder.tags.getTag(sortedId.intValue()));
-    }
-    System.out.println();
-  }
-  */
-
-  @Override
-  public int hashCode() {
-    return key.hashCode();
-  }
+    @Override
+    public int hashCode() {
+        return key.hashCode();
+    }
 
-  @Override
-  public boolean equals(Object o) {
-    return o instanceof AmbiguityClass && key.equals(((AmbiguityClass) o).key);
-  }
+    @Override
+    public boolean equals(Object o) {
+        return o instanceof AmbiguityClass && key.equals(((AmbiguityClass) o).key);
+    }
 
 }
Index: src/edu/stanford/nlp/tagger/maxent/AmbiguityClasses.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/AmbiguityClasses.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/AmbiguityClasses.java	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
@@ -44,14 +44,14 @@
 public class AmbiguityClasses {
 
   private final Index<AmbiguityClass> classes;
-  private static final String naWord = "NA";
+  private static final String naWord = Defaults.naTag;
 
   // TODO: this isn't used anywhere, either
   // protected final AmbiguityClass naClass = new AmbiguityClass(null, false, null, null);
 
   public AmbiguityClasses(TTags ttags) {
     classes = new HashIndex<>();
-    // naClass.init(naWord, ttags);
+    // naClass.updatePointers(naWord, ttags);
   }
 
   private int add(AmbiguityClass a) {
@@ -79,85 +79,4 @@
     return add(a);
   }
 
-  /*
-  public void print() {
-    Object[] arrClasses = classes.objectsList().toArray();//s.keySet().toArray();
-    System.out.println(arrClasses.length);
-//    System.out.println("Number of ambiguity classes is " + arrClasses.length);
-//    for (int i = 0; i < arrClasses.length; i++) {
-//      ((AmbiguityClass) arrClasses[i]).print();
-//    }
-  }
-
-  public void save(String filename) {
-    try {
-      DataOutputStream rf = IOUtils.getDataOutputStream(filename);
-      Object[] arrClasses = classes.objectsList().toArray();//s.keySet().toArray();
-//      System.out.println("Number of ambiguity classes is " + arrClasses.length);
-//      rf.writeInt(arrClasses.length);
-      // for (int i = 0; i < arrClasses.length; i++) {
-        //rf.writeUTF(((AmbiguityClass) (arrClasses[i])).getWord());
-      // }
-      rf.close();
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-
-  }// save
-
-  public void save(DataOutputStream file) {
-    try {
-      Object[] arrClasses = classes.objectsList().toArray();//s.keySet().toArray();
-//      System.out.println("Number of ambiguity classes is " + arrClasses.length);
-//      file.writeInt(arrClasses.length);
-      for (int i = 0; i < arrClasses.length; i++) {
-        //rf.writeUTF(((AmbiguityClass) (arrClasses[i])).getWord());
-        AmbiguityClass cur = (AmbiguityClass) arrClasses[i];
-        file.writeBoolean(cur.single);
-          file.writeUTF(cur.getWord());
-      }
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-
-  }// save
-
-
-  public void read(String filename) {
-    try {
-      InDataStreamFile rf = new InDataStreamFile(filename);
-      int len = rf.readInt();//this is the number of ambiguity classes
-      for (int i = 0; i < len; i++) {
-        boolean singleton = rf.readBoolean();
-//        int len_buff = rf.readInt();
-//        byte[] buff = new byte[len_buff];
-//        rf.read(buff);
-        String word = rf.readUTF();//new String(buff);
-        word = TestSentence.toNice(word);
-        add(new AmbiguityClass(word, singleton));
-        //init();
-      }//i
-
-      rf.close();
-    } catch (IOException e) {
-      e.printStackTrace();
-    }
-  }
-
-  public void read(InDataStreamFile file) {
-    try {
-      int len = file.readInt();//this is the number of ambiguity classes
-      for (int i = 0; i < len; i++) {
-        boolean singleton = file.readBoolean();
-        String word = file.readUTF();//new String(buff);
-        word = TestSentence.toNice(word);
-        add(new AmbiguityClass(word, singleton));
-      }//i
-
-    } catch (IOException e) {
-      e.printStackTrace();
-    }
-  }
-  */
-
 }
Index: src/edu/stanford/nlp/tagger/maxent/Dictionary.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/Dictionary.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/Dictionary.java	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
@@ -5,7 +5,7 @@
  * Copyright:    Copyright (c) The Board of Trustees of Leland Stanford Junior University<p>
  * Company:      Stanford University<p>
  */
-package edu.stanford.nlp.tagger.maxent; 
+package edu.stanford.nlp.tagger.maxent;
 
 import edu.stanford.nlp.io.IOUtils;
 import edu.stanford.nlp.stats.IntCounter;
@@ -18,153 +18,123 @@
 import java.util.Map;
 
 
-/** Maintains a map from words to tags and their counts.
+/**
+ * Maintains a map from words to tags and their counts.
  *
- *  @author Kristina Toutanova
- *  @version 1.0
+ * @author Kristina Toutanova
+ * @version 1.0
  */
-public class Dictionary  {
+public class Dictionary {
 
-  /** A logger for this class */
-  private static final Redwood.RedwoodChannels log = Redwood.channels(Dictionary.class);
+    /**
+     * A logger for this class
+     */
+    private static final Redwood.RedwoodChannels log = Redwood.channels(Dictionary.class);
 
-  private final Map<String,TagCount> dict = Generics.newHashMap();
-  private final Map<Integer,CountWrapper> partTakingVerbs = Generics.newHashMap();
-  private static final String naWord = "NA";
-  private static final boolean VERBOSE = false;
+    private final Map<String, TagCount> dict = Generics.newHashMap();
+    private final Map<Integer, CountWrapper> partTakingVerbs = Generics.newHashMap();
+    private static final String naWord = Defaults.naTag;
+    private static final boolean VERBOSE = false;
 
-  public Dictionary() {
-  }
+    public Dictionary() {
+    }
 
-  void fillWordTagCounts(Map<String, IntCounter<String>> wordTagCounts) {
-    for (Map.Entry<String, IntCounter<String>> wordTagCount : wordTagCounts.entrySet()) {
-      TagCount count = new TagCount(wordTagCount.getValue());
-      dict.put(wordTagCount.getKey(), count);
-    }
-  }
+    void fillWordTagCounts(Map<String, IntCounter<String>> wordTagCounts) {
+        for (Map.Entry<String, IntCounter<String>> wordTagCount : wordTagCounts.entrySet()) {
+            TagCount count = new TagCount(wordTagCount.getValue());
+            dict.put(wordTagCount.getKey(), count);
+        }
+    }
 
-  /*
-  public void release() {
-    dict.clear();
-  }
-
-  public void addVPTaking(String verb, String tag, String partWord) {
-    int h = verb.hashCode();
-    Integer i = Integer.valueOf(h);
-    if (tag.startsWith("RP")) {
-      if (this.partTakingVerbs.containsKey(i)) {
-        this.partTakingVerbs.get(i).incPart(partWord);
-      } else {
-        this.partTakingVerbs.put(i, new CountWrapper(verb, 0, 0, 0, 0));
-        this.partTakingVerbs.get(i).incPart(partWord);
-      }
-    } else if (tag.startsWith("RB")) {
-      if (this.partTakingVerbs.containsKey(i)) {
-        this.partTakingVerbs.get(i).incRB(partWord);
-      } else {
-        this.partTakingVerbs.put(i, new CountWrapper(verb, 0, 0, 0, 0));
-        this.partTakingVerbs.get(i).incRB(partWord);
-      }
-    } else if (tag.startsWith("IN")) {
-      if (this.partTakingVerbs.containsKey(i)) {
-        this.partTakingVerbs.get(i).incIn(partWord);
-      } else {
-        this.partTakingVerbs.put(i, new CountWrapper(verb, 0, 0, 0, 0));
-        this.partTakingVerbs.get(i).incIn(partWord);
-      }
-    }
-  }
-  */
-
-  protected void addVThatTaking(String verb) {
-    int i = verb.hashCode();
-    if (this.partTakingVerbs.containsKey(i)) {
-      this.partTakingVerbs.get(i).incThat();
-    } else {
-      this.partTakingVerbs.put(i, new CountWrapper(verb, 0, 1, 0, 0));
-    }
-  }
+    protected void addVThatTaking(String verb) {
+        int i = verb.hashCode();
+        if (this.partTakingVerbs.containsKey(i)) {
+            this.partTakingVerbs.get(i).incThat();
+        } else {
+            this.partTakingVerbs.put(i, new CountWrapper(verb, 0, 1, 0, 0));
+        }
+    }
 
-  protected int getCountPart(String verb) {
-    int i = verb.hashCode();
-    if (this.partTakingVerbs.containsKey(i)) {
-      return this.partTakingVerbs.get(i).getCountPart();
-    }
-    return 0;
-  }
+    protected int getCountPart(String verb) {
+        int i = verb.hashCode();
+        if (this.partTakingVerbs.containsKey(i)) {
+            return this.partTakingVerbs.get(i).getCountPart();
+        }
+        return 0;
+    }
 
 
-  protected int getCountThat(String verb) {
-    int i = verb.hashCode();
-    if (this.partTakingVerbs.containsKey(i)) {
-      return this.partTakingVerbs.get(i).getCountThat();
-    }
-    return 0;
-  }
+    protected int getCountThat(String verb) {
+        int i = verb.hashCode();
+        if (this.partTakingVerbs.containsKey(i)) {
+            return this.partTakingVerbs.get(i).getCountThat();
+        }
+        return 0;
+    }
 
 
-  protected int getCountIn(String verb) {
-    int i = verb.hashCode();
-    if (this.partTakingVerbs.containsKey(i)) {
-      return this.partTakingVerbs.get(i).getCountIn();
-    }
-    return 0;
-  }
+    protected int getCountIn(String verb) {
+        int i = verb.hashCode();
+        if (this.partTakingVerbs.containsKey(i)) {
+            return this.partTakingVerbs.get(i).getCountIn();
+        }
+        return 0;
+    }
 
 
-  protected int getCountRB(String verb) {
-    int i = verb.hashCode();
-    if (this.partTakingVerbs.containsKey(i)) {
-      return this.partTakingVerbs.get(i).getCountRB();
-    }
-    return 0;
-  }
+    protected int getCountRB(String verb) {
+        int i = verb.hashCode();
+        if (this.partTakingVerbs.containsKey(i)) {
+            return this.partTakingVerbs.get(i).getCountRB();
+        }
+        return 0;
+    }
 
 
-  protected int getCount(String word, String tag) {
-    TagCount count = dict.get(word);
-    if (count == null) {
-      return 0;
-    } else {
-      return count.get(tag);
-    }
-  }
+    protected int getCount(String word, String tag) {
+        TagCount count = dict.get(word);
+        if (count == null) {
+            return 0;
+        } else {
+            return count.get(tag);
+        }
+    }
 
 
-  protected String[] getTags(String word) {
-    TagCount count = get(word);
-    if (count == null) {
-      return null;
-    }
-    return count.getTags();
-  }
+    protected String[] getTags(String word) {
+        TagCount count = get(word);
+        if (count == null) {
+            return null;
+        }
+        return count.getTags();
+    }
 
 
-  protected TagCount get(String word) {
-    return dict.get(word);
-  }
+    protected TagCount get(String word) {
+        return dict.get(word);
+    }
 
 
-  String getFirstTag(String word) {
-    TagCount count = dict.get(word);
-    if (count != null) {
-      return count.getFirstTag();
-    }
-    return null;
-  }
+    String getFirstTag(String word) {
+        TagCount count = dict.get(word);
+        if (count != null) {
+            return count.getFirstTag();
+        }
+        return null;
+    }
 
 
-  protected int sum(String word) {
-    TagCount count = dict.get(word);
-    if (count != null) {
-      return count.sum();
-    }
-    return 0;
-  }
+    protected int sum(String word) {
+        TagCount count = dict.get(word);
+        if (count != null) {
+            return count.sum();
+        }
+        return 0;
+    }
 
-  boolean isUnknown(String word) {
-    return ! dict.containsKey(word);
-  }
+    boolean isUnknown(String word) {
+        return !dict.containsKey(word);
+    }
 
 
   /*
@@ -179,183 +149,147 @@
   }
   */
 
-  void save(DataOutputStream file) {
-    String[] arr = dict.keySet().toArray(new String[dict.keySet().size()]);
-    try {
-      file.writeInt(arr.length);
-      log.info("Saving dictionary of " + arr.length + " words ...");
-      for (String word : arr) {
-        TagCount count = get(word);
-        file.writeUTF(word);
-        count.save(file);
-      }
-      Integer[] arrverbs = this.partTakingVerbs.keySet().toArray(new Integer[partTakingVerbs.keySet().size()]);
-      file.writeInt(arrverbs.length);
-      for (Integer iO : arrverbs) {
-        CountWrapper tC = this.partTakingVerbs.get(iO);
-        file.writeInt(iO.intValue());
-        tC.save(file);
-      }
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
+    void save(DataOutputStream file) {
+        String[] arr = dict.keySet().toArray(new String[dict.keySet().size()]);
+        try {
+            file.writeInt(arr.length);
+            log.info("Saving dictionary of " + arr.length + " words ...");
+            for (String word : arr) {
+                TagCount count = get(word);
+                file.writeUTF(word);
+                count.save(file);
+            }
+            Integer[] arrverbs = this.partTakingVerbs.keySet().toArray(new Integer[partTakingVerbs.keySet().size()]);
+            file.writeInt(arrverbs.length);
+            for (Integer iO : arrverbs) {
+                CountWrapper tC = this.partTakingVerbs.get(iO);
+                file.writeInt(iO.intValue());
+                tC.save(file);
+            }
+        } catch (Exception e) {
+            e.printStackTrace();
+        }
+    }
 
-  private void read(DataInputStream rf, String filename) throws IOException {
-    // Object[] arr=dict.keySet().toArray();
+    private void read(DataInputStream rf, String filename) throws IOException {
+        // Object[] arr=dict.keySet().toArray();
 
-    int maxNumTags = 0;
-    int len = rf.readInt();
-    if (VERBOSE) {
-      log.info("Reading Dictionary of " + len + " words from " + filename + '.');
-    }
+        int maxNumTags = 0;
+        int len = rf.readInt();
+        if (VERBOSE) {
+            log.info("Reading Dictionary of " + len + " words from " + filename + '.');
+        }
 
-    for (int i = 0; i < len; i++) {
-      String word = rf.readUTF();
-      TagCount count = TagCount.readTagCount(rf);
-      int numTags = count.numTags();
-      if (numTags > maxNumTags) {
-        maxNumTags = numTags;
-      }
-      this.dict.put(word, count);
-      if (VERBOSE) {
-        log.info("  " + word + " [idx=" + i + "]: " + count);
-      }
-    }
-    if (VERBOSE) {
-      log.info("Read dictionary of " + len + " words; max tags for word was " + maxNumTags + '.');
-    }
-  }
+        for (int i = 0; i < len; i++) {
+            String word = rf.readUTF();
+            TagCount count = TagCount.readTagCount(rf);
+            int numTags = count.numTags();
+            if (numTags > maxNumTags) {
+                maxNumTags = numTags;
+            }
+            this.dict.put(word, count);
+            if (VERBOSE) {
+                log.info("  " + word + " [idx=" + i + "]: " + count);
+            }
+        }
+        if (VERBOSE) {
+            log.info("Read dictionary of " + len + " words; max tags for word was " + maxNumTags + '.');
+        }
+    }
 
-  private void readTags(DataInputStream rf) throws IOException {
-    // Object[] arr=dict.keySet().toArray();
+    private void readTags(DataInputStream rf) throws IOException {
+        // Object[] arr=dict.keySet().toArray();
 
-    int maxNumTags = 0;
-    int len = rf.readInt();
-    if (VERBOSE) {
-      log.info("Reading Dictionary of " + len + " words.");
-    }
+        int maxNumTags = 0;
+        int len = rf.readInt();
+        if (VERBOSE) {
+            log.info("Reading Dictionary of " + len + " words.");
+        }
 
-    for (int i = 0; i < len; i++) {
-      String word = rf.readUTF();
-      TagCount count = TagCount.readTagCount(rf);
-      int numTags = count.numTags();
-      if (numTags > maxNumTags) {
-        maxNumTags = numTags;
-      }
-      this.dict.put(word, count);
-      if (VERBOSE) {
-        log.info("  " + word + " [idx=" + i + "]: " + count);
-      }
-    }
-    if (VERBOSE) {
-      log.info("Read dictionary of " + len + " words; max tags for word was " + maxNumTags + '.');
-    }
-  }
+        for (int i = 0; i < len; i++) {
+            String word = rf.readUTF();
+            TagCount count = TagCount.readTagCount(rf);
+            int numTags = count.numTags();
+            if (numTags > maxNumTags) {
+                maxNumTags = numTags;
+            }
+            this.dict.put(word, count);
+            if (VERBOSE) {
+                log.info("  " + word + " [idx=" + i + "]: " + count);
+            }
+        }
+        if (VERBOSE) {
+            log.info("Read dictionary of " + len + " words; max tags for word was " + maxNumTags + '.');
+        }
+    }
 
-  protected void read(String filename) {
-    try {
-      DataInputStream rf = IOUtils.getDataInputStream(filename);
-      read(rf, filename);
+    protected void read(String filename) {
+        try {
+            DataInputStream rf = IOUtils.getDataInputStream(filename);
+            read(rf, filename);
 
-      int len1 = rf.readInt();
-      for (int i = 0; i < len1; i++) {
-        int iO = rf.readInt();
-        CountWrapper tC = new CountWrapper();
-        tC.read(rf);
+            int len1 = rf.readInt();
+            for (int i = 0; i < len1; i++) {
+                int iO = rf.readInt();
+                CountWrapper tC = new CountWrapper();
+                tC.read(rf);
 
-        this.partTakingVerbs.put(iO, tC);
-      }
-      rf.close();
-    } catch (IOException e) {
-      e.printStackTrace();
-    }
-  }
+                this.partTakingVerbs.put(iO, tC);
+            }
+            rf.close();
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
 
-  protected void read(DataInputStream file) {
-    try {
-      readTags(file);
+    protected void read(DataInputStream file) {
+        try {
+            readTags(file);
 
-      int len1 = file.readInt();
-      for (int i = 0; i < len1; i++) {
-        int iO = file.readInt();
-        CountWrapper tC = new CountWrapper();
-        tC.read(file);
+            int len1 = file.readInt();
+            for (int i = 0; i < len1; i++) {
+                int iO = file.readInt();
+                CountWrapper tC = new CountWrapper();
+                tC.read(file);
 
-        this.partTakingVerbs.put(iO, tC);
-      }
-    } catch (IOException e) {
-      e.printStackTrace();
-    }
-  }
-
-  /*
-  public void printAmbiguous() {
-    String[] arr = dict.keySet().toArray(new String[dict.keySet().size()]);
-    try {
-      int countAmbiguous = 0;
-      int countUnAmbiguous = 0;
-      int countAmbDisamb = 0;
-      for (String word : arr) {
-        if (word.indexOf('|') == -1) {
-          continue;
-        }
-        TagCount count = get(word);
-        if (count.numTags() > 1) {
-          System.out.print(word);
-          countAmbiguous++;
-          tC.print();
-          System.out.println();
-        } else {
-          String wordA = word.substring(0, word.indexOf('|'));
-          if (get(wordA).numTags() > 1) {
-            System.out.print(word);
-            countAmbDisamb++;
-            countUnAmbiguous++;
-            tC.print();
-            System.out.println();
-          } else {
-            countUnAmbiguous++;
-          }
-        }// else
-      }
-      System.out.println(" ambg " + countAmbiguous + " unambg " + countUnAmbiguous + " disamb " + countAmbDisamb);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-  */
+                this.partTakingVerbs.put(iO, tC);
+            }
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
 
-  /**
-   * This makes ambiguity classes from all words in the dictionary and remembers
-   * their classes in the TagCounts
-   */
-  protected void setAmbClasses(AmbiguityClasses ambClasses, int veryCommonWordThresh, TTags ttags) {
-    for (Map.Entry<String,TagCount> entry : dict.entrySet()) {
-      String w = entry.getKey();
-      TagCount count = entry.getValue();
-      int ambClassId = ambClasses.getClass(w, this, veryCommonWordThresh, ttags);
-      count.setAmbClassId(ambClassId);
-    }
-  }
+    /**
+     * This makes ambiguity classes from all words in the dictionary and remembers
+     * their classes in the TagCounts
+     */
+    protected void setAmbClasses(AmbiguityClasses ambClasses, int veryCommonWordThresh, TTags ttags) {
+        for (Map.Entry<String, TagCount> entry : dict.entrySet()) {
+            String w = entry.getKey();
+            TagCount count = entry.getValue();
+            int ambClassId = ambClasses.getClass(w, this, veryCommonWordThresh, ttags);
+            count.setAmbClassId(ambClassId);
+        }
+    }
 
-  protected int getAmbClass(String word) {
-    if (word.equals(naWord)) {
-      return -2;
-    }
-    if (get(word) == null) {
-      return -1;
-    }
-    return get(word).getAmbClassId();
-  }
+    protected int getAmbClass(String word) {
+        if (word.equals(naWord)) {
+            return -2;
+        }
+        if (get(word) == null) {
+            return -1;
+        }
+        return get(word).getAmbClassId();
+    }
 
-  public static void main(String[] args) {
-    String s = "word";
-    String tag = "tag";
-    Dictionary d = new Dictionary();
+    public static void main(String[] args) {
+        String s = "word";
+        String tag = "tag";
+        Dictionary d = new Dictionary();
 
-    System.out.println(d.getCount(s, tag));
-    System.out.println(d.getFirstTag(s));
-  }
+        System.out.println(d.getCount(s, tag));
+        System.out.println(d.getFirstTag(s));
+    }
 
 }
Index: src/edu/stanford/nlp/tagger/maxent/ExtractorFrames.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/ExtractorFrames.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/ExtractorFrames.java	(revision 108c365276c71c4be9040ccfff65d298300ae639)
@@ -40,63 +40,63 @@
  * tag sequences (and interaction terms) for the MaxentTagger, but not the
  * feature extractors explicitly targeting generalization for rare or unknown
  * words.
- *
+ * <p>
  * The following options are supported:
  * <table>
  * <caption>Supported feature options for the maxent tagger</caption>
  * <tr><td>Name</td><td>Args</td><td>Effect</td></tr>
  * <tr><td>words</td><td>begin, end</td>
- *     <td>Individual features for words begin ... end.
- *     If just one argument words(-2) is given, then end is taken as 0. If
- *     begin is not less than or equal to end, no features are made.</td></tr>
+ * <td>Individual features for words begin ... end.
+ * If just one argument words(-2) is given, then end is taken as 0. If
+ * begin is not less than or equal to end, no features are made.</td></tr>
  * <tr><td>tags</td><td>begin, end</td>
- *     <td>Individual features for tags begin ... end.
- *     If just one argument words(-2) is given, then end is taken as 0. </td></tr>
+ * <td>Individual features for tags begin ... end.
+ * If just one argument words(-2) is given, then end is taken as 0. </td></tr>
  * <tr><td>biword</td><td>w1, w2</td>
- *     <td>One feature for the pair of words w1, w2</td></tr>
+ * <td>One feature for the pair of words w1, w2</td></tr>
  * <tr><td>biwords</td><td>begin, end</td>
- *     <td>One feature for each sequential pair of words
- *         from begin to end</td></tr>
+ * <td>One feature for each sequential pair of words
+ * from begin to end</td></tr>
  * <tr><td>twoTags</td><td>t1, t2</td>
- *     <td>One feature for the pair of tags t1, t2</td></tr>
+ * <td>One feature for the pair of tags t1, t2</td></tr>
  * <tr><td>lowercasewords</td><td>begin, end</td>
- *     <td>One feature for each word begin ... end, lowercased</td></tr>
+ * <td>One feature for each word begin ... end, lowercased</td></tr>
  * <tr><td>order</td><td>left, right</td>
- *     <td>A feature for tags left through 0 and a feature for
- *         tags 0 through right (not including 0).  Lower order left and right features are also added.
- *         This gets very expensive for higher order terms.</td></tr>
+ * <td>A feature for tags left through 0 and a feature for
+ * tags 0 through right (not including 0).  Lower order left and right features are also added.
+ * This gets very expensive for higher order terms.</td></tr>
  * <tr><td>wordTag</td><td>w, t</td>
- *     <td>A feature combining word w and tag t.</td></tr>
+ * <td>A feature combining word w and tag t.</td></tr>
  * <tr><td>wordTwoTags</td><td>w, t1, t2</td>
- *     <td>A feature combining word w and tags t1, t2.</td></tr>
+ * <td>A feature combining word w and tags t1, t2.</td></tr>
  * <tr><td>threeTags</td><td>t1, t2, t3</td>
- *     <td>A feature combining tags t1, t2, t3.</td></tr>
+ * <td>A feature combining tags t1, t2, t3.</td></tr>
  * <tr><td>vbn</td><td>length</td>
- *     <td>A feature that looks at the left length words for something that
- *         appears to be a VBN (in English) without looking at the actual tags.
- *         It is zeroeth order, as it does not look at the tag predictions.
- *         It also is never used, since it doesn't seem to help.</td></tr>
+ * <td>A feature that looks at the left length words for something that
+ * appears to be a VBN (in English) without looking at the actual tags.
+ * It is zeroeth order, as it does not look at the tag predictions.
+ * It also is never used, since it doesn't seem to help.</td></tr>
  * <tr><td>allwordshapes</td><td>left, right</td>
- *     <td>Word shape features, eg transform Foo5 into Xxx#
- *         (not exactly like that, but that general idea).
- *         Creates individual features for each word left ... right.
- *         Compare with the feature "wordshapes" in ExtractorFramesRare,
- *         which is only applied to rare words. Fairly English-specific.
- *         Slightly increases accuracy.</td></tr>
+ * <td>Word shape features, eg transform Foo5 into Xxx#
+ * (not exactly like that, but that general idea).
+ * Creates individual features for each word left ... right.
+ * Compare with the feature "wordshapes" in ExtractorFramesRare,
+ * which is only applied to rare words. Fairly English-specific.
+ * Slightly increases accuracy.</td></tr>
  * <tr><td>allunicodeshapes</td><td>left, right</td>
- *     <td>Same thing, but works for unicode characters more generally.</td></tr>
+ * <td>Same thing, but works for unicode characters more generally.</td></tr>
  * <tr><td>allwordshapeconjunction</td><td>left, right</td>
- *     <td>Instead of individual token word shape features, combines several
- *         word shapes into one feature.</td></tr>
+ * <td>Instead of individual token word shape features, combines several
+ * word shapes into one feature.</td></tr>
  * <tr><td>allunicodeshapeconjunction</td><td>left, right</td>
- *     <td>Instead of individual token word shape features, combines several
- *         word shapes into one feature.</td></tr>
+ * <td>Instead of individual token word shape features, combines several
+ * word shapes into one feature.</td></tr>
  * <tr><td>spanishauxiliaries</td><td></td>
- *     <td>Add feature detectors for Spanish auxiliaries.</td></tr>
+ * <td>Add feature detectors for Spanish auxiliaries.</td></tr>
  * <tr><td>extractor</td><td>classname</td>
- *     <td>Add a feature extractor by classname, which will be applied to all words.</td></tr>
+ * <td>Add a feature extractor by classname, which will be applied to all words.</td></tr>
  * </table>
- *
+ * <p>
  * See {@link ExtractorFramesRare} for more options.
  * <br>
  * There are also macro features:
@@ -105,11 +105,11 @@
  * left5words = words(-2,2),order(2) <br>
  * generic = words(-1,1),order(2),biwords(-1,0),wordTag(0,-1) <br>
  * bidirectional5words =
- *   words(-2,2),order(-2,2),twoTags(-1,1),
- *   wordTag(0,-1),wordTag(0,1),biwords(-1,1) <br>
+ * words(-2,2),order(-2,2),twoTags(-1,1),
+ * wordTag(0,-1),wordTag(0,1),biwords(-1,1) <br>
  * bidirectional =
- *   words(-1,1),order(-2,2),twoTags(-1,1),
- *   wordTag(0,-1),wordTag(0,1),biwords(-1,1) <br>
+ * words(-1,1),order(-2,2),twoTags(-1,1),
+ * wordTag(0,-1),wordTag(0,1),biwords(-1,1) <br>
  * german = some random stuff <br>
  * sighan2005 = some other random stuff <br>
  * The left3words architectures are faster, but slightly less
@@ -117,578 +117,617 @@
  * 'naacl2003unknowns' was our traditional set of unknown word
  * features, but you can now specify features more flexibility via the
  * various other supported keywords defined in {@link ExtractorFramesRare}.
- *
+ * <p>
  * Note: All features are implicitly conjoined with the current tag.
  *
  * @author Kristina Toutanova
  * @author Michel Galley
  * @version 1.0
  */
-public class ExtractorFrames  {
-
-  /** A logger for this class */
-  private static final Redwood.RedwoodChannels log = Redwood.channels(ExtractorFrames.class);
+public class ExtractorFrames {
+    /**
+     * A logger for this class
+     */
+    private static final Redwood.RedwoodChannels log = Redwood.channels(ExtractorFrames.class);
 
-  // Used in various testing for properties of current word elsewhere
-  static final Extractor cWord = new Extractor(0, false);
+    // Used in various testing for properties of current word elsewhere
+    static final Extractor cWord = new Extractor(0, false);
 
-  /**
-   * This class is not meant to be instantiated.
-   */
-  private ExtractorFrames() { }
+    /**
+     * This class is not meant to be instantiated.
+     */
+    private ExtractorFrames() {
+    }
 
 
-  @SuppressWarnings("StatementWithEmptyBody")
-  protected static Extractor[] getExtractorFrames(String arch) {
-    // handle some traditional macro options
-    // left3words: a simple trigram CMM tagger (similar to the baseline EMNLP 2000 tagger)
-    // left5words: a simple trigram CMM tagger, like left3words, with 5 word context
-    // generic: our standard multilingual CMM baseline
+    @SuppressWarnings("StatementWithEmptyBody")
+    static Extractor[] getExtractorFrames(String arch) {
+        // handle some traditional macro options
+        // left3words: a simple trigram CMM tagger (similar to the baseline EMNLP 2000 tagger)
+        // left5words: a simple trigram CMM tagger, like left3words, with 5 word context
+        // generic: our standard multilingual CMM baseline
 
-    arch = arch.replaceAll("left3words", "words(-1,1),order(2)");
-    arch = arch.replaceAll("left5words", "words(-2,2),order(2)");
-    arch = arch.replaceAll("generic", "words(-1,1),order(2),biwords(-1,0),wordTag(0,-1)");
-    arch = arch.replaceAll("bidirectional5words", "words(-2,2),order(-2,2),twoTags(-1,1),wordTag(0,-1),wordTag(0,1),biwords(-1,1)");
-    arch = arch.replaceAll("bidirectional", "words(-1,1),order(-2,2),twoTags(-1,1),wordTag(0,-1),wordTag(0,1),biwords(-1,1)");
-    // There was an historical bidirectional "german" option which was: "words(-1,1),order(-2,1),wordTag(0,-1),biword(-1,-0)"
-    // There was an historical left "sighan2005" option which was: "words(-2,2),order(2),tags(-2, -2)"
+        arch = arch.replaceAll("left3words", "words(-1,1),order(2)");
+        arch = arch.replaceAll("left5words", "words(-2,2),order(2)");
+        arch = arch.replaceAll("generic", "words(-1,1),order(2),biwords(-1,0),wordTag(0,-1)");
+        arch = arch.replaceAll("bidirectional5words", "words(-2,2),order(-2,2),twoTags(-1,1),wordTag(0,-1),wordTag(0,1),biwords(-1,1)");
+        arch = arch.replaceAll("bidirectional", "words(-1,1),order(-2,2),twoTags(-1,1),wordTag(0,-1),wordTag(0,1),biwords(-1,1)");
+        // There was an historical bidirectional "german" option which was: "words(-1,1),order(-2,1),wordTag(0,-1),biword(-1,-0)"
+        // There was an historical left "sighan2005" option which was: "words(-2,2),order(2),tags(-2, -2)"
 
-    ArrayList<Extractor> extrs = new ArrayList<>();
-    List<String> args = StringUtils.valueSplit(arch, "[a-zA-Z0-9]*(?:\\([^)]*\\))?", "\\s*,\\s*");
-    for (String arg : args) {
-      if (arg.startsWith("words(")) {
-        // non-sequence features with just a certain number of words to the
-        // left and right; e.g., words(-2,2) or words(-2,-1)
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        for (int i = lWindow; i <= rWindow; i++) {
-          extrs.add(new Extractor(i, false));
-        }
-      } else if (arg.startsWith("tags(")) {
-        // non-sequence features with just a certain number of words to the
-        // left and right; e.g., tags(-2,2) or tags(-2,-1)
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        for (int i = lWindow; i <= rWindow; i++) {
-          // refuse to add a tag extractor at position 0 -- that's what we're predicting
-          if (i != 0) {
-            extrs.add(new Extractor(i, true));
-          }
-        }
-      } else if (arg.startsWith("biwords(")) {
-        // non-sequence features of word pairs.
-        // biwords(-2,1) would give you 3 extractors for w-2w-1, w-1,w0, w0w1
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        for (int i = lWindow; i < rWindow; i++) {
-          extrs.add(new ExtractorTwoWords(i));
-        }
-      } else if (arg.startsWith("biword(")) {
-        // non-sequence feature of a word pair.
-        // biwords(-2,1) would give you 1 extractor for w-2, w+1
-        int left = Extractor.getParenthesizedNum(arg, 1);
-        int right = Extractor.getParenthesizedNum(arg, 2);
-        extrs.add(new ExtractorTwoWords(left, right));
-      } else if (arg.startsWith("twoTags(")) {
-        // non-sequence feature of a tag pair.
-        // twoTags(-2,1) would give you 1 extractor for t-2, t+1
-        int left = Extractor.getParenthesizedNum(arg, 1);
-        int right = Extractor.getParenthesizedNum(arg, 2);
-        extrs.add(new ExtractorTwoTags(left, right));
-      } else if (arg.startsWith("lowercasewords(")) {
-        // non-sequence features with just a certain number of lowercase words
-        // to the left and right
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        for (int i = lWindow; i <= rWindow; i++) {
-          extrs.add(new ExtractorWordLowerCase(i));
-        }
-      } else if (arg.startsWith("order(")) {
-        // anything like order(2), order(-4), order(0,3), or
-        // order(-2,1) are okay.
-        int leftOrder = Extractor.getParenthesizedNum(arg, 1);
-        int rightOrder = Extractor.getParenthesizedNum(arg, 2);
-        if (leftOrder > 0) { leftOrder = -leftOrder; }
-	if (rightOrder < 0) { throw new IllegalArgumentException("Right order must be non-negative, not " + rightOrder); }
-        // cdm 2009: We only add successively higher order tag k-grams
-        // ending adjacent to t0.  Adding lower order features at a distance
-        // appears not to help (Dec 2009). But they can now be added with tags().
+        ArrayList<Extractor> extrs = new ArrayList<>();
+        List<String> args = StringUtils.valueSplit(arch, "[a-zA-Z0-9]*(?:\\([^)]*\\))?", "\\s*,\\s*");
+        for (String arg : args) {
+            if (arg.startsWith("words(")) {
+                // non-sequence features with just a certain number of words to the
+                // left and right; e.g., words(-2,2) or words(-2,-1)
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                for (int i = lWindow; i <= rWindow; i++) {
+                    extrs.add(new Extractor(i, false));
+                }
+            } else if (arg.startsWith("tags(")) {
+                // non-sequence features with just a certain number of words to the
+                // left and right; e.g., tags(-2,2) or tags(-2,-1)
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                for (int i = lWindow; i <= rWindow; i++) {
+                    // refuse to add a tag extractor at position 0 -- that's what we're predicting
+                    if (i != 0) {
+                        extrs.add(new Extractor(i, true));
+                    }
+                }
+            } else if (arg.startsWith("biwords(")) {
+                // non-sequence features of word pairs.
+                // biwords(-2,1) would give you 3 extractors for w-2w-1, w-1,w0, w0w1
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                for (int i = lWindow; i < rWindow; i++) {
+                    extrs.add(new ExtractorTwoWords(i));
+                }
+            } else if (arg.startsWith("biword(")) {
+                // non-sequence feature of a word pair.
+                // biwords(-2,1) would give you 1 extractor for w-2, w+1
+                int left = Extractor.getParenthesizedNum(arg, 1);
+                int right = Extractor.getParenthesizedNum(arg, 2);
+                extrs.add(new ExtractorTwoWords(left, right));
+            } else if (arg.startsWith("twoTags(")) {
+                // non-sequence feature of a tag pair.
+                // twoTags(-2,1) would give you 1 extractor for t-2, t+1
+                int left = Extractor.getParenthesizedNum(arg, 1);
+                int right = Extractor.getParenthesizedNum(arg, 2);
+                extrs.add(new ExtractorTwoTags(left, right));
+            } else if (arg.startsWith("lowercasewords(")) {
+                // non-sequence features with just a certain number of lowercase words
+                // to the left and right
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                for (int i = lWindow; i <= rWindow; i++) {
+                    extrs.add(new ExtractorWordLowerCase(i));
+                }
+            } else if (arg.startsWith("order(")) {
+                // anything like order(2), order(-4), order(0,3), or
+                // order(-2,1) are okay.
+                int leftOrder = Extractor.getParenthesizedNum(arg, 1);
+                int rightOrder = Extractor.getParenthesizedNum(arg, 2);
+                if (leftOrder > 0) {
+                    leftOrder = -leftOrder;
+                }
+                if (rightOrder < 0) {
+                    throw new IllegalArgumentException("Right order must be non-negative, not " + rightOrder);
+                }
+                // cdm 2009: We only add successively higher order tag k-grams
+                // ending adjacent to t0.  Adding lower order features at a distance
+                // appears not to help (Dec 2009). But they can now be added with tags().
 
-        for (int idx = leftOrder ; idx <= rightOrder; idx++) {
-          if (idx == -1 || idx == 1) {
-            extrs.add(new Extractor(idx, true));
-          } else if (idx != 0) {
-            extrs.add(new ExtractorContinuousTagConjunction(idx));
-          }
-          // do nothing if idx = 0. You can't use the  tag to infer itself!
-        }
-      } else if (arg.startsWith("wordTag(")) {
-        // sequence feature of a word and a tag: wordTag(-1,1)
-        int posW = Extractor.getParenthesizedNum(arg, 1);
-        int posT = Extractor.getParenthesizedNum(arg, 2);
-        extrs.add(new ExtractorWordTag(posW, posT));
-      } else if (arg.startsWith("wordTwoTags(")) {
-        int word = Extractor.getParenthesizedNum(arg, 1);
-        int tag1 = Extractor.getParenthesizedNum(arg, 2);
-        int tag2 = Extractor.getParenthesizedNum(arg, 3);
-        extrs.add(new ExtractorWordTwoTags(word,tag1,tag2));
-      } else if (arg.startsWith("threeTags(")) {
-        int pos1 = Extractor.getParenthesizedNum(arg, 1);
-        int pos2 = Extractor.getParenthesizedNum(arg, 2);
-        int pos3 = Extractor.getParenthesizedNum(arg, 3);
-        extrs.add(new ExtractorThreeTags(pos1,pos2,pos3));
-      } else if (arg.startsWith("vbn(")) {
-        int order = Extractor.getParenthesizedNum(arg, 1);
-        extrs.add(new ExtractorVerbalVBNZero(order));
-      } else if (arg.startsWith("allwordshapes(")) {
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        String wsc = Extractor.getParenthesizedArg(arg, 3);
-        if (wsc == null) {
-          wsc = "chris2";
-        }
-        for (int i = lWindow; i <= rWindow; i++) {
-          extrs.add(new ExtractorWordShapeClassifier(i, wsc));
-        }
-      } else if (arg.startsWith("allwordshapeconjunction(")) {
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        String wsc = Extractor.getParenthesizedArg(arg, 3);
-        if (wsc == null) {
-          wsc = "chris2";
-        }
-        extrs.add(new ExtractorWordShapeConjunction(lWindow, rWindow, wsc));
-      } else if (arg.startsWith("allunicodeshapes(")) {
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        for (int i = lWindow; i <= rWindow; i++) {
-          extrs.add(new ExtractorWordShapeClassifier(i, "chris4"));
-        }
-      } else if (arg.startsWith("allunicodeshapeconjunction(")) {
-        int lWindow = Extractor.getParenthesizedNum(arg, 1);
-        int rWindow = Extractor.getParenthesizedNum(arg, 2);
-        extrs.add(new ExtractorWordShapeConjunction(lWindow, rWindow, "chris4"));
-      } else if (arg.equalsIgnoreCase("spanishauxiliaries")) {
-        extrs.add(new ExtractorSpanishAuxiliaryTag());
-        extrs.add(new ExtractorSpanishSemiauxiliaryTag());
-      } else if (arg.startsWith("extractor(")) {
-        String className = Extractor.getParenthesizedArg(arg, 1);
-        try {
-          Extractor e = (Extractor) Class.forName(className).getDeclaredConstructor().newInstance();
-          extrs.add(e);
-        } catch (Exception e) {
-          throw new RuntimeException("Couldn't create POS tagger extractor class " + className, e);
-        }
-      } else if (arg.equalsIgnoreCase("naacl2003unknowns") ||
-                 arg.equalsIgnoreCase("lnaacl2003unknowns") ||
-                 arg.equalsIgnoreCase("caselessnaacl2003unknowns") ||
-                 arg.equalsIgnoreCase("naacl2003conjunctions") ||
-                 arg.equalsIgnoreCase("frenchunknowns") ||
-                 arg.equalsIgnoreCase("spanishunknowns") ||
-                 arg.startsWith("wordshapes(") ||
-                 arg.startsWith("wordshapeconjunction(") ||
-                 arg.equalsIgnoreCase("motleyUnknown") ||
-                 arg.startsWith("suffix(") ||
-                 arg.startsWith("prefix(") ||
-                 arg.startsWith("prefixsuffix") ||
-                 arg.startsWith("capitalizationsuffix(") ||
-                 arg.startsWith("distsim(") ||
-                 arg.startsWith("distsimconjunction(") ||
-                 arg.equalsIgnoreCase("lctagfeatures") ||
-                 arg.startsWith("rareExtractor(") ||
-                 arg.startsWith("unicodeshapes(") ||
-                 arg.startsWith("chinesedictionaryfeatures(") ||
-                 arg.startsWith("unicodeshapeconjunction(")) {
-        // okay; known unknown keyword
-      } else {
-        log.info("Unrecognized ExtractorFrames identifier (ignored): " + arg);
-      }
-    } // end for
-    return extrs.toArray(Extractor.EMPTY_EXTRACTOR_ARRAY);
-  }
+                for (int idx = leftOrder; idx <= rightOrder; idx++) {
+                    if (idx == -1 || idx == 1) {
+                        extrs.add(new Extractor(idx, true));
+                    } else if (idx != 0) {
+                        extrs.add(new ExtractorContinuousTagConjunction(idx));
+                    }
+                    // do nothing if idx = 0. You can't use the  tag to infer itself!
+                }
+            } else if (arg.startsWith("wordTag(")) {
+                // sequence feature of a word and a tag: wordTag(-1,1)
+                int posW = Extractor.getParenthesizedNum(arg, 1);
+                int posT = Extractor.getParenthesizedNum(arg, 2);
+                extrs.add(new ExtractorWordTag(posW, posT));
+            } else if (arg.startsWith("wordTwoTags(")) {
+                int word = Extractor.getParenthesizedNum(arg, 1);
+                int tag1 = Extractor.getParenthesizedNum(arg, 2);
+                int tag2 = Extractor.getParenthesizedNum(arg, 3);
+                extrs.add(new ExtractorWordTwoTags(word, tag1, tag2));
+            } else if (arg.startsWith("threeTags(")) {
+                int pos1 = Extractor.getParenthesizedNum(arg, 1);
+                int pos2 = Extractor.getParenthesizedNum(arg, 2);
+                int pos3 = Extractor.getParenthesizedNum(arg, 3);
+                extrs.add(new ExtractorThreeTags(pos1, pos2, pos3));
+            } else if (arg.startsWith("vbn(")) {
+                int order = Extractor.getParenthesizedNum(arg, 1);
+                extrs.add(new ExtractorVerbalVBNZero(order));
+            } else if (arg.startsWith("allwordshapes(")) {
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                String wsc = Extractor.getParenthesizedArg(arg, 3);
+                if (wsc == null) {
+                    wsc = "chris2";
+                }
+                for (int i = lWindow; i <= rWindow; i++) {
+                    extrs.add(new ExtractorWordShapeClassifier(i, wsc));
+                }
+            } else if (arg.startsWith("allwordshapeconjunction(")) {
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                String wsc = Extractor.getParenthesizedArg(arg, 3);
+                if (wsc == null) {
+                    wsc = "chris2";
+                }
+                extrs.add(new ExtractorWordShapeConjunction(lWindow, rWindow, wsc));
+            } else if (arg.startsWith("allunicodeshapes(")) {
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                for (int i = lWindow; i <= rWindow; i++) {
+                    extrs.add(new ExtractorWordShapeClassifier(i, "chris4"));
+                }
+            } else if (arg.startsWith("allunicodeshapeconjunction(")) {
+                int lWindow = Extractor.getParenthesizedNum(arg, 1);
+                int rWindow = Extractor.getParenthesizedNum(arg, 2);
+                extrs.add(new ExtractorWordShapeConjunction(lWindow, rWindow, "chris4"));
+            } else if (arg.equalsIgnoreCase("spanishauxiliaries")) {
+                extrs.add(new ExtractorSpanishAuxiliaryTag());
+                extrs.add(new ExtractorSpanishSemiauxiliaryTag());
+            } else if (arg.startsWith("extractor(")) {
+                String className = Extractor.getParenthesizedArg(arg, 1);
+                try {
+                    Extractor e = (Extractor) Class.forName(className).getDeclaredConstructor().newInstance();
+                    extrs.add(e);
+                } catch (Exception e) {
+                    throw new RuntimeException("Couldn't create POS tagger extractor class " + className, e);
+                }
+            } else if (arg.equalsIgnoreCase("naacl2003unknowns") ||
+                    arg.equalsIgnoreCase("lnaacl2003unknowns") ||
+                    arg.equalsIgnoreCase("caselessnaacl2003unknowns") ||
+                    arg.equalsIgnoreCase("naacl2003conjunctions") ||
+                    arg.equalsIgnoreCase("frenchunknowns") ||
+                    arg.equalsIgnoreCase("spanishunknowns") ||
+                    arg.startsWith("wordshapes(") ||
+                    arg.startsWith("wordshapeconjunction(") ||
+                    arg.equalsIgnoreCase("motleyUnknown") ||
+                    arg.startsWith("suffix(") ||
+                    arg.startsWith("prefix(") ||
+                    arg.startsWith("prefixsuffix") ||
+                    arg.startsWith("capitalizationsuffix(") ||
+                    arg.startsWith("distsim(") ||
+                    arg.startsWith("distsimconjunction(") ||
+                    arg.equalsIgnoreCase("lctagfeatures") ||
+                    arg.startsWith("rareExtractor(") ||
+                    arg.startsWith("unicodeshapes(") ||
+                    arg.startsWith("chinesedictionaryfeatures(") ||
+                    arg.startsWith("unicodeshapeconjunction(")) {
+                // okay; known unknown keyword
+            } else {
+                log.info("Unrecognized ExtractorFrames identifier (ignored): " + arg);
+            }
+        } // end for
+        return extrs.toArray(Extractor.EMPTY_EXTRACTOR_ARRAY);
+    }
 
 
-  /**
-   * This extractor extracts a word and tag in conjunction.
-   */
-  static class ExtractorWordTag extends Extractor {
-
-    private static final long serialVersionUID = 3L;
+    /**
+     * This extractor extracts a word and tag in conjunction.
+     */
+    static class ExtractorWordTag extends Extractor {
+        private static final long serialVersionUID = 3L;
 
-    private final int wordPosition;
-    public ExtractorWordTag(int posW, int posT) {
-      super(posT, true);
-      wordPosition = posW;
-    }
+        private final int wordPosition;
+
+        ExtractorWordTag(int posW, int posT) {
+            super(posT, true);
+            wordPosition = posW;
+        }
 
-    @Override
-    String extract(History h, PairsHolder pH) {
-      return pH.getTag(h, position) + '!' + pH.getWord(h, wordPosition);
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            return pH.getTag(h, position) + '!' + pH.getWord(h, wordPosition);
+        }
 
-    @Override
-    public String toString() {
-      return (getClass().getName() + "(w" + wordPosition +
-              ",t" + position + ')');
-    }
-  }
+        @Override
+        public String toString() {
+            return (getClass().getName() + "(w" + wordPosition +
+                    ",t" + position + ')');
+        }
+    }
 
 
-  /**
-   * The word in lower-cased version.
-   * Always uses Locale.ENGLISH.
-   */
-  static class ExtractorWordLowerCase extends Extractor {
-
-    private static final long serialVersionUID = -7847524200422095441L;
+    /**
+     * The word in lower-cased version.
+     * Always uses Locale.ENGLISH.
+     */
+    static class ExtractorWordLowerCase extends Extractor {
+        private static final long serialVersionUID = -7847524200422095441L;
 
-    public ExtractorWordLowerCase(int position) {
-      super(position, false);
-    }
+        ExtractorWordLowerCase(int position) {
+            super(position, false);
+        }
 
-    @Override
-      String extract(History h, PairsHolder pH) {
-      return pH.getWord(h, position).toLowerCase(Locale.ENGLISH);
-    }
-
-  }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            return pH.getWord(h, position).toLowerCase(Locale.ENGLISH);
+        }
+    }
 
-  /**
-   * The current word if it is capitalized, zero otherwise.
-   * Always uses Locale.ENGLISH.
-   */
-  static class ExtractorCWordCapCase extends Extractor {
-
-    private static final long serialVersionUID = -2393096135964969744L;
+    /**
+     * The current word if it is capitalized, zero otherwise.
+     * Always uses Locale.ENGLISH.
+     */
+    static class ExtractorCWordCapCase extends Extractor {
+        private static final long serialVersionUID = -2393096135964969744L;
 
-    @Override
-      String extract(History h, PairsHolder pH) {
-      String cw = pH.getWord(h, 0);
-      String lk = cw.toLowerCase(Locale.ENGLISH);
-      if (lk.equals(cw)) {
-        return zeroSt;
-      }
-      return cw;
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            String cw = pH.getWord(h, 0);
+            String lk = cw.toLowerCase(Locale.ENGLISH);
+            if (lk.equals(cw)) {
+                return zeroSt;
+            }
+            return cw;
+        }
 
-    @Override public boolean isLocal() { return true; }
-    @Override public boolean isDynamic() { return false; }
-  }
+        @Override
+        public boolean isLocal() {
+            return true;
+        }
+
+        @Override
+        public boolean isDynamic() {
+            return false;
+        }
+    }
 
 
-  /**
-   * This extractor extracts two words in conjunction.
-   * The one argument constructor gives you leftPosition and
-   * leftPosition+1, but with the two argument constructor,
-   * they can be any pair of word positions.
-   */
-  static class ExtractorTwoWords extends Extractor {
+    /**
+     * This extractor extracts two words in conjunction.
+     * The one argument constructor gives you leftPosition and
+     * leftPosition+1, but with the two argument constructor,
+     * they can be any pair of word positions.
+     */
+    static class ExtractorTwoWords extends Extractor {
 
-    private static final long serialVersionUID = -1034112287022504917L;
+        private static final long serialVersionUID = -1034112287022504917L;
 
-    private final int leftPosition;
-    private final int rightPosition;
+        private final int leftPosition;
+        private final int rightPosition;
 
-    public ExtractorTwoWords(int leftPosition) {
-      this(leftPosition, leftPosition+1);
-    }
+        ExtractorTwoWords(int leftPosition) {
+            this(leftPosition, leftPosition + 1);
+        }
 
-    public ExtractorTwoWords(int position1, int position2) {
-      super(0, false);
-      if (position1 > position2) {
-        leftPosition = position1;
-        rightPosition = position2;
-      } else {
-        leftPosition = position2;
-        rightPosition = position1;
-      }
-    }
+        ExtractorTwoWords(int position1, int position2) {
+            super(0, false);
+            if (position1 > position2) {
+                leftPosition = position1;
+                rightPosition = position2;
+            } else {
+                leftPosition = position2;
+                rightPosition = position1;
+            }
+        }
 
-    @Override
-      String extract(History h, PairsHolder pH) {
-      // I ran a bunch of timing tests that seem to indicate it is
-      // cheaper to simply add string + char + string than use a
-      // StringBuilder or go through the StringBuildMemoizer -horatio
-      return pH.getWord(h, leftPosition) + '!' + pH.getWord(h, rightPosition);
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            // I ran a bunch of timing tests that seem to indicate it is
+            // cheaper to simply add string + char + string than use a
+            // StringBuilder or go through the StringBuildMemoizer -horatio
+            return pH.getWord(h, leftPosition) + '!' + pH.getWord(h, rightPosition);
+        }
 
-    @Override public boolean isLocal() { return false; }
+        @Override
+        public boolean isLocal() {
+            return false;
+        }
 
-    // isDynamic --> false, but no need to override
+        // isDynamic --> false, but no need to override
 
 
-    @Override
-    public String toString() {
-      return (getClass().getName() + "(w" + leftPosition +
-              ",w" + rightPosition + ')');
-    }
-  }
+        @Override
+        public String toString() {
+            return (getClass().getName() + "(w" + leftPosition +
+                    ",w" + rightPosition + ')');
+        }
+    }
 
 
-
-  /**
-   * This extractor extracts two tags in conjunction.
-   * The one argument constructor gives you leftPosition and
-   * leftPosition+1, but with the two argument constructor,
-   * they can be any pair of tag positions.
-   */
-  static class ExtractorTwoTags extends Extractor {
+    /**
+     * This extractor extracts two tags in conjunction.
+     * The one argument constructor gives you leftPosition and
+     * leftPosition+1, but with the two argument constructor,
+     * they can be any pair of tag positions.
+     */
+    static class ExtractorTwoTags extends Extractor {
 
-    private static final long serialVersionUID = -7342144764725605134L;
+        private static final long serialVersionUID = -7342144764725605134L;
 
-    private final int leftPosition;
-    private final int rightPosition;
-    private final int leftContext, rightContext;
+        private final int leftPosition;
+        private final int rightPosition;
+        private final int leftContext, rightContext;
 
-    public ExtractorTwoTags(int position1, int position2) {
-      leftPosition = Math.min(position1, position2);
-      rightPosition = Math.max(position1, position2);
+        ExtractorTwoTags(int position1, int position2) {
+            leftPosition = Math.min(position1, position2);
+            rightPosition = Math.max(position1, position2);
 
-      leftContext = -Math.min(leftPosition, 0);
-      rightContext = Math.max(rightPosition, 0);
-    }
+            leftContext = -Math.min(leftPosition, 0);
+            rightContext = Math.max(rightPosition, 0);
+        }
 
-    @Override
-    public int rightContext() {
-      return rightContext;
-    }
+        @Override
+        public int rightContext() {
+            return rightContext;
+        }
 
-    @Override
-    public int leftContext() {
-      return leftContext;
-    }
+        @Override
+        public int leftContext() {
+            return leftContext;
+        }
 
-    @Override
-    String extract(History h, PairsHolder pH) {
-      // I ran a bunch of timing tests that seem to indicate it is
-      // cheaper to simply add string + char + string than use a
-      // StringBuilder or go through the StringBuildMemoizer -horatio
-      return pH.getTag(h, leftPosition) + '!' + pH.getTag(h, rightPosition);
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            // I ran a bunch of timing tests that seem to indicate it is
+            // cheaper to simply add string + char + string than use a
+            // StringBuilder or go through the StringBuildMemoizer -horatio
+            return pH.getTag(h, leftPosition) + '!' + pH.getTag(h, rightPosition);
+        }
 
-    @Override public boolean isLocal() { return false; }
-    @Override public boolean isDynamic() { return true; }
+        @Override
+        public boolean isLocal() {
+            return false;
+        }
+
+        @Override
+        public boolean isDynamic() {
+            return true;
+        }
 
-    @Override
-    public String toString() {
-      return (getClass().getName() + "(t" + leftPosition +
-              ",t" + rightPosition + ')');
-    }
-  }
+        @Override
+        public String toString() {
+            return (getClass().getName() + "(t" + leftPosition +
+                    ",t" + rightPosition + ')');
+        }
+    }
 
 
-  /**
-   * This extractor extracts two words and a tag in conjunction.
-   */
-  static class ExtractorTwoWordsTag extends Extractor {
+    /**
+     * This extractor extracts two words and a tag in conjunction.
+     */
+    static class ExtractorTwoWordsTag extends Extractor {
 
-    private static final long serialVersionUID = 277004119652781188L;
+        private static final long serialVersionUID = 277004119652781188L;
 
-    private final int leftWord, rightWord, tag;
-    private final int rightContext, leftContext;
+        private final int leftWord, rightWord, tag;
+        private final int rightContext, leftContext;
 
-    public ExtractorTwoWordsTag(int leftWord, int rightWord, int tag) {
-      this.leftWord = Math.min(leftWord, rightWord);
-      this.rightWord = Math.max(leftWord, rightWord);
-      this.tag = tag;
+        public ExtractorTwoWordsTag(int leftWord, int rightWord, int tag) {
+            this.leftWord = Math.min(leftWord, rightWord);
+            this.rightWord = Math.max(leftWord, rightWord);
+            this.tag = tag;
 
-      this.rightContext = Math.max(tag, 0);
-      this.leftContext = -Math.min(tag, 0);
-    }
+            this.rightContext = Math.max(tag, 0);
+            this.leftContext = -Math.min(tag, 0);
+        }
 
-    @Override
-    public int rightContext() {
-      return rightContext;
-    }
+        @Override
+        public int rightContext() {
+            return rightContext;
+        }
 
-    @Override
-    public int leftContext() {
-      return leftContext;
-    }
+        @Override
+        public int leftContext() {
+            return leftContext;
+        }
 
-    @Override
-      String extract(History h, PairsHolder pH) {
-      return (pH.getWord(h, leftWord) + '!' + pH.getTag(h, tag) + '!' +
-              pH.getWord(h, rightWord));
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            return (pH.getWord(h, leftWord) + '!' + pH.getTag(h, tag) + '!' +
+                    pH.getWord(h, rightWord));
+        }
 
-    @Override public boolean isLocal() { return false; }
-    @Override public boolean isDynamic() { return true; }
+        @Override
+        public boolean isLocal() {
+            return false;
+        }
+
+        @Override
+        public boolean isDynamic() {
+            return true;
+        }
 
-    @Override
-    public String toString() {
-      return (getClass().getName() + "(w" + leftWord +
-              ",t" + tag + ",w" + rightWord + ')');
-    }
-  }
+        @Override
+        public String toString() {
+            return (getClass().getName() + "(w" + leftWord +
+                    ",t" + tag + ",w" + rightWord + ')');
+        }
+    }
 
 
-
-  /**
-   * This extractor extracts several contiguous tags only on one side of position 0.
-   * E.g., use constructor argument -3 for an order 3 predictor on the left.
-   * isLocal=false, isDynamic=true (through super call)
-   */
-  static class ExtractorContinuousTagConjunction extends Extractor {
+    /**
+     * This extractor extracts several contiguous tags only on one side of position 0.
+     * E.g., use constructor argument -3 for an order 3 predictor on the left.
+     * isLocal=false, isDynamic=true (through super call)
+     */
+    static class ExtractorContinuousTagConjunction extends Extractor {
 
-    private static final long serialVersionUID = 3;
+        private static final long serialVersionUID = 3;
 
-    public ExtractorContinuousTagConjunction(int maxPosition) {
-      super(maxPosition, true);
-    }
+        public ExtractorContinuousTagConjunction(int maxPosition) {
+            super(maxPosition, true);
+        }
 
-    @Override
-    String extract(History h, PairsHolder pH) {
-      StringBuilder sb = new StringBuilder();
-      if (position < 0) {
-        for (int idx = position; idx < 0; idx++) {
-          if (idx != position) {
-            sb.append('!');
-          }
-          sb.append(pH.getTag(h, idx));
-        }
-      } else {
-        for (int idx = position; idx > 0; idx--) {
-          if (idx != position) {
-            sb.append('!');
-          }
-          sb.append(pH.getTag(h, idx));
-        }
-      }
-      return sb.toString();
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            StringBuilder sb = new StringBuilder();
+            if (position < 0) {
+                for (int idx = position; idx < 0; idx++) {
+                    if (idx != position) {
+                        sb.append('!');
+                    }
+                    sb.append(pH.getTag(h, idx));
+                }
+            } else {
+                for (int idx = position; idx > 0; idx--) {
+                    if (idx != position) {
+                        sb.append('!');
+                    }
+                    sb.append(pH.getTag(h, idx));
+                }
+            }
+            return sb.toString();
+        }
 
-    @Override
-    public String toString() {
-      return "ExtractorContinuousTagConj(" + (position < 0 ? position + " ... -1": "1 ... " + position) + ')';
-    }
+        @Override
+        public String toString() {
+            return "ExtractorContinuousTagConj(" + (position < 0 ? position + " ... -1" : "1 ... " + position) + ')';
+        }
 
-  }
+    }
 
 
-  /**
-   * This extractor extracts three tags.
-   */
-  static class ExtractorThreeTags extends Extractor {
+    /**
+     * This extractor extracts three tags.
+     */
+    static class ExtractorThreeTags extends Extractor {
 
-    private static final long serialVersionUID = 8563584394721620568L;
+        private static final long serialVersionUID = 8563584394721620568L;
 
-    private int position1;
-    private int position2;
-    private int position3;
+        private int position1;
+        private int position2;
+        private int position3;
 
-    public ExtractorThreeTags(int position1, int position2, int position3) {
-      // bubblesort them!
-      int x;
-      if (position1 > position2) {
-        x = position2;
-        position2 = position1;
-        position1 = x;
-      }
-      if (position2 > position3) {
-        x = position3;
-        position3 = position2;
-        position2 = x;
-      }
-      if (position1 > position2) {
-        x = position2;
-        position2 = position1;
-        position1 = x;
-      }
-      this.position1 = position1;
-      this.position2 = position2;
-      this.position3 = position3;
-    }
+        public ExtractorThreeTags(int position1, int position2, int position3) {
+            // bubblesort them!
+            int x;
+            if (position1 > position2) {
+                x = position2;
+                position2 = position1;
+                position1 = x;
+            }
+            if (position2 > position3) {
+                x = position3;
+                position3 = position2;
+                position2 = x;
+            }
+            if (position1 > position2) {
+                x = position2;
+                position2 = position1;
+                position1 = x;
+            }
+            this.position1 = position1;
+            this.position2 = position2;
+            this.position3 = position3;
+        }
 
-    @Override
-      public int rightContext() {
-      if (position3 > 0) {
-        return position3;
-      } else {
-        return 0;
-      }
-    }
+        @Override
+        public int rightContext() {
+            if (position3 > 0) {
+                return position3;
+            } else {
+                return 0;
+            }
+        }
 
-    @Override
-      public int leftContext() {
-      if (position1 < 0) {
-        return -position1;
-      } else {
-        return 0;
-      }
-    }
+        @Override
+        public int leftContext() {
+            if (position1 < 0) {
+                return -position1;
+            } else {
+                return 0;
+            }
+        }
 
-    @Override
-      String extract(History h, PairsHolder pH) {
-      return pH.getTag(h, position1) + '!' + pH.getTag(h, position2) + '!' + pH.getTag(h, position3);
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            return pH.getTag(h, position1) + '!' + pH.getTag(h, position2) + '!' + pH.getTag(h, position3);
+        }
 
-    @Override public boolean isLocal() { return false; }
-    @Override public boolean isDynamic() { return true; }
-
-    @Override
-    public String toString() {
-      return (getClass().getName() + "(t" + position1 +
-              ",t" + position2 + ",t" + position3 + ')');
-    }
-  }
+        @Override
+        public boolean isLocal() {
+            return false;
+        }
+
+        @Override
+        public boolean isDynamic() {
+            return true;
+        }
+
+        @Override
+        public String toString() {
+            return (getClass().getName() + "(t" + position1 +
+                    ",t" + position2 + ",t" + position3 + ')');
+        }
+    }
 
 
-  /**
-   * This extractor extracts two tags and the a word in conjunction.
-   */
-  static class ExtractorWordTwoTags extends Extractor {
+    /**
+     * This extractor extracts two tags and the a word in conjunction.
+     */
+    static class ExtractorWordTwoTags extends Extractor {
 
-    private static final long serialVersionUID = -4942654091455804176L;
+        private static final long serialVersionUID = -4942654091455804176L;
 
-    // We sort so that position1 <= position2 and then rely on that.
-    private int position1;
-    private int position2;
-    private int word;
+        // We sort so that position1 <= position2 and then rely on that.
+        private int position1;
+        private int position2;
+        private int word;
 
-    public ExtractorWordTwoTags(int word, int position1, int position2) {
-      if (position1 < position2) {
-        this.position1 = position1;
-        this.position2 = position1;
-      } else {
-        this.position1 = position2;
-        this.position2 = position1;
-      }
-      this.word = word;
-    }
+        public ExtractorWordTwoTags(int word, int position1, int position2) {
+            if (position1 < position2) {
+                this.position1 = position1;
+                this.position2 = position1;
+            } else {
+                this.position1 = position2;
+                this.position2 = position1;
+            }
+            this.word = word;
+        }
 
-    @Override
-      public int leftContext() {
-      if (position1 < 0) {
-        return  -position1;
-      } else {
-        return 0;
-      }
-    }
+        @Override
+        public int leftContext() {
+            if (position1 < 0) {
+                return -position1;
+            } else {
+                return 0;
+            }
+        }
 
-    @Override
-      public int rightContext() {
-      if (position2 > 0) {
-        return position2;
-      } else {
-        return 0;
-      }
-    }
+        @Override
+        public int rightContext() {
+            if (position2 > 0) {
+                return position2;
+            } else {
+                return 0;
+            }
+        }
 
-    @Override
-      String extract(History h, PairsHolder pH) {
-      return pH.getTag(h, position1) + '!' + pH.getWord(h, word) + '!' + pH.getTag(h, position2);
-    }
+        @Override
+        String extract(History h, PairsHolder pH) {
+            return pH.getTag(h, position1) + '!' + pH.getWord(h, word) + '!' + pH.getTag(h, position2);
+        }
 
-    @Override public boolean isLocal() { return false; }
-    @Override public boolean isDynamic() { return true; }
+        @Override
+        public boolean isLocal() {
+            return false;
+        }
+
+        @Override
+        public boolean isDynamic() {
+            return true;
+        }
 
-    @Override
-    public String toString() {
-      return (getClass().getName() + "(t" + position1 +
-              ",t" + position2 + ",w" + word + ')');
-    }
-  }
+        @Override
+        public String toString() {
+            return (getClass().getName() + "(t" + position1 +
+                    ",t" + position2 + ",w" + word + ')');
+        }
+    }
 
 
 } // end class ExtractorFrames
@@ -696,51 +735,58 @@
 
 class ExtractorWordShapeClassifier extends Extractor {
 
-  private final int wordShaper;
-  private final String name;
+    private final int wordShaper;
+    private final String name;
 
-  // This cache speeds things up a little bit.  I used
-  // -Xrunhprof:cpu=samples,interval=1 when using the "distsim" tagger
-  // on the training set to measure roughly how much time was spent in
-  // this method.  I concluded that with the cache, 1.24% of the time
-  // is spent here, and without the cache, 1.26% of the time is spent
-  // here.  This is a very small savings, which would be even smaller
-  // if we make the cache thread safe.  It turns out that, as written,
-  // the cache is not thread safe for various reasons.  In particular,
-  // it assumes only one wordshape classifier is ever used, which
-  // might not be true even with just one tagger, and has an even
-  // higher chance of not being true if there are multiple taggers.
-  // Furthermore, access to the cache should really be synchronized
-  // regardless.  The easiest solution is to comment out the cache and
-  // note that if you want to bring it back, make it a map from wsc to
-  // cache rather than just a single cache.  -- horatio
-  //private static final Map<String, String> shapes =
-  //  Generics.newHashMap();
-  // --- should be:
-  //private static final Map<String, Map<String, String>> ...
+    // This cache speeds things up a little bit.  I used
+    // -Xrunhprof:cpu=samples,interval=1 when using the "distsim" tagger
+    // on the training set to measure roughly how much time was spent in
+    // this method.  I concluded that with the cache, 1.24% of the time
+    // is spent here, and without the cache, 1.26% of the time is spent
+    // here.  This is a very small savings, which would be even smaller
+    // if we make the cache thread safe.  It turns out that, as written,
+    // the cache is not thread safe for various reasons.  In particular,
+    // it assumes only one wordshape classifier is ever used, which
+    // might not be true even with just one tagger, and has an even
+    // higher chance of not being true if there are multiple taggers.
+    // Furthermore, access to the cache should really be synchronized
+    // regardless.  The easiest solution is to comment out the cache and
+    // note that if you want to bring it back, make it a map from wsc to
+    // cache rather than just a single cache.  -- horatio
+    //private static final Map<String, String> shapes =
+    //  Generics.newHashMap();
+    // --- should be:
+    //private static final Map<String, Map<String, String>> ...
 
-  ExtractorWordShapeClassifier(int position, String wsc) {
-    super(position, false);
-    wordShaper = WordShapeClassifier.lookupShaper(wsc);
-    name = "ExtractorWordShapeClassifier(" + position+ ',' + wsc + ')';
-  }
+    ExtractorWordShapeClassifier(int position, String wsc) {
+        super(position, false);
+        wordShaper = WordShapeClassifier.lookupShaper(wsc);
+        name = "ExtractorWordShapeClassifier(" + position + ',' + wsc + ')';
+    }
 
-  @Override
-  String extract(History h, PairsHolder pH) {
-    String s = super.extract(h, pH);
-    String shape = WordShapeClassifier.wordShape(s, wordShaper);
-    return shape;
-  }
+    @Override
+    String extract(History h, PairsHolder pH) {
+        String s = super.extract(h, pH);
+        String shape = WordShapeClassifier.wordShape(s, wordShaper);
+        return shape;
+    }
 
-  private static final long serialVersionUID = 101L;
+    private static final long serialVersionUID = 101L;
 
-  @Override
-  public String toString() {
-    return name;
-  }
+    @Override
+    public String toString() {
+        return name;
+    }
 
-  @Override public boolean isLocal() { return position == 0; }
-  @Override public boolean isDynamic() { return false; }
+    @Override
+    public boolean isLocal() {
+        return position == 0;
+    }
+
+    @Override
+    public boolean isDynamic() {
+        return false;
+    }
 }
 
 
@@ -749,41 +795,48 @@
  */
 class ExtractorWordShapeConjunction extends Extractor {
 
-  private static final long serialVersionUID = -49L;
+    private static final long serialVersionUID = -49L;
 
-  private final int wordShaper;
-  private final int left;
-  private final int right;
-  private final String name;
+    private final int wordShaper;
+    private final int left;
+    private final int right;
+    private final String name;
 
-  ExtractorWordShapeConjunction(int left, int right, String wsc) {
-    super();
-    this.left = left;
-    this.right = right;
-    wordShaper = WordShapeClassifier.lookupShaper(wsc);
-    name = "ExtractorWordShapeConjunction(" + left + ',' + right + ',' + wsc + ')';
-  }
+    ExtractorWordShapeConjunction(int left, int right, String wsc) {
+        super();
+        this.left = left;
+        this.right = right;
+        wordShaper = WordShapeClassifier.lookupShaper(wsc);
+        name = "ExtractorWordShapeConjunction(" + left + ',' + right + ',' + wsc + ')';
+    }
 
-  @Override
-  String extract(History h, PairsHolder pH) {
-    StringBuilder sb = new StringBuilder();
-    for (int j = left; j <= right; j++) {
-      String s = pH.getWord(h, j);
-      sb.append(WordShapeClassifier.wordShape(s, wordShaper));
-      if (j < right) {
-        sb.append('|');
-      }
-    }
-    return sb.toString();
-  }
+    @Override
+    String extract(History h, PairsHolder pH) {
+        StringBuilder sb = new StringBuilder();
+        for (int j = left; j <= right; j++) {
+            String s = pH.getWord(h, j);
+            sb.append(WordShapeClassifier.wordShape(s, wordShaper));
+            if (j < right) {
+                sb.append('|');
+            }
+        }
+        return sb.toString();
+    }
 
-  @Override
-  public String toString() {
-    return name;
-  }
+    @Override
+    public String toString() {
+        return name;
+    }
 
-  @Override public boolean isLocal() { return false; }
-  @Override public boolean isDynamic() { return false; }
+    @Override
+    public boolean isLocal() {
+        return false;
+    }
+
+    @Override
+    public boolean isDynamic() {
+        return false;
+    }
 
 }
 
@@ -794,22 +847,22 @@
  */
 class ExtractorSpanishAuxiliaryTag extends Extractor {
 
-  private static final long serialVersionUID = -3352770856914897103L;
+    private static final long serialVersionUID = -3352770856914897103L;
 
-  public ExtractorSpanishAuxiliaryTag() {
-    super(-1, true);
-  }
+    public ExtractorSpanishAuxiliaryTag() {
+        super(-1, true);
+    }
 
-  @Override
-  String extract(History h, PairsHolder pH) {
-    String tag = super.extract(h, pH);
-    return tag.startsWith("va") ? "1" : "0";
-  }
+    @Override
+    String extract(History h, PairsHolder pH) {
+        String tag = super.extract(h, pH);
+        return tag.startsWith("va") ? "1" : "0";
+    }
 
-  @Override
-  public String toString() {
-    return "ExtractorSpanishAuxiliaryTag";
-  }
+    @Override
+    public String toString() {
+        return "ExtractorSpanishAuxiliaryTag";
+    }
 
 }
 
@@ -820,21 +873,21 @@
  */
 class ExtractorSpanishSemiauxiliaryTag extends Extractor {
 
-  private static final long serialVersionUID = -164942945521643734L;
+    private static final long serialVersionUID = -164942945521643734L;
 
-  public ExtractorSpanishSemiauxiliaryTag() {
-    super(-1, true);
-  }
+    public ExtractorSpanishSemiauxiliaryTag() {
+        super(-1, true);
+    }
 
-  @Override
-  String extract(History h, PairsHolder pH) {
-    String tag = super.extract(h, pH);
-    return tag.startsWith("vs") ? "1" : "0";
-  }
+    @Override
+    String extract(History h, PairsHolder pH) {
+        String tag = super.extract(h, pH);
+        return tag.startsWith("vs") ? "1" : "0";
+    }
 
-  @Override
-  public String toString() {
-    return "ExtractorSpanishSemiauxiliaryTag";
-  }
+    @Override
+    public String toString() {
+        return "ExtractorSpanishSemiauxiliaryTag";
+    }
 
 }
Index: src/edu/stanford/nlp/tagger/maxent/Extractors.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/Extractors.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/Extractors.java	(revision 108c365276c71c4be9040ccfff65d298300ae639)
@@ -33,7 +33,7 @@
   /**
    * Set the extractors from an array.
    *
-   * @param extrs The array of extractors.  It is copied in this init.
+   * @param extrs The array of extractors.  It is copied in this updatePointers.
    */
   public Extractors(Extractor[] extrs) {
     v = new Extractor[extrs.length];
@@ -156,7 +156,7 @@
       InDataStreamFile rf = new InDataStreamFile(filename);
       int len = rf.readInt();
       v = new Extractor[len];
-      //GlobalHolder.init();
+      //GlobalHolder.updatePointers();
     } catch (IOException e) {
       e.printStackTrace();
     }
Index: src/edu/stanford/nlp/tagger/maxent/History.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/History.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/History.java	(revision 37343aa19406c7e0ad1722ac9e25511c7175e7c2)
@@ -5,127 +5,83 @@
  */
 package edu.stanford.nlp.tagger.maxent;
 
-/** A History is used to look inside a PairsHolder with indices for the start and end of a sentence and
- *  the current position. It may be looking at the PairsHolder contained within it or another one.
+/**
+ * A History is used to look inside a PairsHolder with indices for the start and end of a sentence and
+ * the current position. It may be looking at the PairsHolder contained within it or another one.
  *
- *  @author Kristina Toutanova
- *  @version 1.0
+ * @author Kristina Toutanova
+ * @version 1.0
  */
 public class History {
-  int start;  // this is the index of the first word of the sentence
-  int end;    //this is the index of the last word in the sentence - the dot
-  int current; // this is the index of the current word
-  final PairsHolder pairs;
-  // todo [cdm 2018]: Is extractors even needed or could it be eliminated??? It seems like it is used in tagger construction but it still confuses me since this only stores regular extractors not rareExtractors
-  final Extractors extractors;
+    int start;  // this is the index of the first word of the sentence
+    int end;    //this is the index of the last word in the sentence - the dot
+    int current; // this is the index of the current word
+    final PairsHolder pairs;
+    // todo [cdm 2018]: Is extractors even needed or could it be eliminated??? It seems like it is used in tagger construction but it still confuses me since this only stores regular extractors not rareExtractors
+    private final Extractors extractors;
 
-  History(PairsHolder pairs, Extractors extractors) {
-    this.pairs = pairs;
-    this.extractors = extractors;
-  }
+    History(PairsHolder pairs, Extractors extractors) {
+        this.pairs = pairs;
+        this.extractors = extractors;
+    }
 
-  History(int start, int end, int current, PairsHolder pairs, Extractors extractors) {
-    this.pairs = pairs;
-    this.extractors = extractors;
-    init(start,end,current);
-  }
+    History(int start, int end, int current, PairsHolder pairs, Extractors extractors) {
+        this.pairs = pairs;
+        this.extractors = extractors;
+        updatePointers(start, end, current);
+    }
 
-  void init(int start, int end, int current) {
-    this.start = start;
-    this.end = end;
-    this.current = current;
-  }
+    void updatePointers(int start, int end, int current) {
+        this.start = start;
+        this.end = end;
+        this.current = current;
+    }
 
-  /*
-  public void save(DataOutputStream rf) {
-    try {
-      rf.writeInt(start);
-      rf.writeInt(end);
-      rf.writeInt(current);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  public void read(InDataStreamFile rf) {
-    try {
-      start = rf.readInt();
-      end = rf.readInt();
-      current = rf.readInt();
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-  */
-
-  private String getX(int index) {
-    // get the string by the index in x
-    return extractors.get(index).extract(this);
-  }
+    private String getX(int index) {
+        // get the string by the index in x
+        return extractors.get(index).extract(this);
+    }
 
-  public String[] getX() {
-    String[] x = new String[extractors.size()];
-    for (int i = 0; i < x.length; i++) {
-      x[i] = getX(i);
-    }
-    return x;
-  }
+    public String[] getX() {
+        String[] x = new String[extractors.size()];
+        for (int i = 0; i < x.length; i++) {
+            x[i] = getX(i);
+        }
+        return x;
+    }
 
-
-  /*
-  void print(PrintStream ps) {
-    String[] str = getX();
-    for (String aStr : str) {
-      ps.print(aStr);
-      ps.print('\t');
-    }
-    ps.println();
-  }
-
-  public void printSent() {
-    print(System.out);
-
-    for (int i = this.start; i < this.end; i++) {
-      System.out.print(pairs.getTag(i) + ' ' + pairs.getWord(i) + '\t');
-    }
-    System.out.println();
-  }
-  */
-
-  protected void setTag(int pos, String tag) {
-    pairs.setTag(pos + start, tag);
-  }
+    protected void setTag(int pos, String tag) {
+        pairs.setTag(pos + start, tag);
+    }
 
-
-  protected void set(int start, int end, int current) {
-    this.start = start;
-    this.end = end;
-    this.current = current;
-  }
+    protected void set(int start, int end, int current) {
+        this.start = start;
+        this.end = end;
+        this.current = current;
+    }
 
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    String[] str = getX();
-    for (String aStr : str) {
-      sb.append(aStr).append('\t');
-    }
-    return sb.toString();
-  }
+    @Override
+    public String toString() {
+        StringBuilder sb = new StringBuilder();
+        String[] str = getX();
+        for (String aStr : str) {
+            sb.append(aStr).append('\t');
+        }
+        return sb.toString();
+    }
 
-  // maybe not needed? Are histories hashed? (Yes, I think they currently are in tagger construction, though not sure why....)
-  @Override
-  public int hashCode() {
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0, sz = extractors.size(); i < sz; i++) {
-      sb.append(getX(i));
-    }
-    return sb.toString().hashCode();
-  }
+    // maybe not needed? Are histories hashed? (Yes, I think they currently are in tagger construction, though not sure why....)
+    @Override
+    public int hashCode() {
+        StringBuilder sb = new StringBuilder();
+        for (int i = 0, sz = extractors.size(); i < sz; i++) {
+            sb.append(getX(i));
+        }
+        return sb.toString().hashCode();
+    }
 
-  @Override
-  public boolean equals(Object h1) {
-    return h1 instanceof History && extractors.equals(this, (History) h1);
-  }
-
+    @Override
+    public boolean equals(Object h1) {
+        return h1 instanceof History && extractors.equals(this, (History) h1);
+    }
 }
Index: src/edu/stanford/nlp/tagger/maxent/LambdaSolveTagger.kt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/LambdaSolveTagger.kt	(revision 108c365276c71c4be9040ccfff65d298300ae639)
+++ src/edu/stanford/nlp/tagger/maxent/LambdaSolveTagger.kt	(revision 108c365276c71c4be9040ccfff65d298300ae639)
@@ -0,0 +1,159 @@
+package edu.stanford.nlp.tagger.maxent
+
+import edu.stanford.nlp.util.logging.Redwood
+
+import edu.stanford.nlp.maxent.Feature
+import edu.stanford.nlp.maxent.Problem
+import edu.stanford.nlp.maxent.iis.LambdaSolve
+
+import java.text.NumberFormat
+import java.io.DataInputStream
+
+
+/**
+ * This module does the working out of lambda parameters for binary tagger
+ * features.  It can use either IIS or CG.
+ *
+ * @author Kristina Toutanova
+ * @version 1.0
+ */
+class LambdaSolveTagger : LambdaSolve {
+
+    /**
+     * Suppress extraneous printouts
+     */
+    //@SuppressWarnings("unused")
+    //private static final boolean VERBOSE = false;
+
+
+    internal constructor(p1: Problem, eps1: Double, fnumArr: Array<ByteArray>) {
+        p = p1
+        eps = eps1
+        // newtonerr = nerr1;
+        lambda = DoubleArray(p1.fSize)
+        // lambda_converged = new boolean[p1.fSize];
+        // cdm 2008: Below line is memory hog. Is there anything we can do to avoid this square array allocation?
+        probConds = Array(p1.data.xSize) { DoubleArray(p1.data.ySize) }
+        this.fnumArr = fnumArr
+        zlambda = DoubleArray(p1.data.xSize)
+        ftildeArr = DoubleArray(p.fSize)
+        initCondsZlambdaEtc()
+        super.setBinary()
+    }
+
+
+    /** Initialize a trained LambdaSolveTagger.
+     * This is the version used when loading a saved tagger.
+     * Only the lambda array is used, and the rest is irrelevant, CDM thinks.
+     *
+     * @param dataStream Stream to load lambda parameters from.
+     */
+    internal constructor(dataStream: DataInputStream) {
+        lambda = LambdaSolve.read_lambdas(dataStream)
+        super.setBinary()
+    }
+
+    /** Initialize a trained LambdaSolveTagger.
+     * This is the version used when creating a LambdaSolveTagger from
+     * a condensed lambda array.
+     * Only the lambda array is used, and the rest is irrelevant, CDM thinks.
+     *
+     * @param lambda Array used as the lambda parameters (directly; no safety copy is made).
+     */
+    internal constructor(lambda: DoubleArray) {
+        this.lambda = lambda
+        super.setBinary()
+    }
+
+    private fun initCondsZlambdaEtc() {
+        // updatePointers pcond
+        for (x in 0 until p.data.xSize) {
+            for (y in 0 until p.data.ySize) {
+                probConds[x][y] = 1.0 / p.data.ySize
+            }
+        }
+        log.info(" pcond initialized ")
+        // updatePointers zlambda
+        for (x in 0 until p.data.xSize) {
+            zlambda[x] = p.data.ySize.toDouble()
+        }
+        log.info(" zlambda initialized ")
+        // updatePointers ftildeArr
+        for (i in 0 until p.fSize) {
+            ftildeArr[i] = p.functions.get(i).ftilde()
+            if (ftildeArr[i] == 0.0) {
+                log.info(" Empirical expectation 0 for feature $i")
+            }
+        }
+        log.info(" ftildeArr initialized ")
+    }
+
+    internal fun g(lambdaP: Double, index: Int): Double {
+        var s = 0.0
+        for (i in 0 until p.functions.get(index).len()) {
+            val y = (p.functions.get(index) as TaggerFeature).yTag
+            val x = p.functions.get(index).getX(i)
+            s = s + p.data.ptildeX(x) * pcond(y, x) * 1.0 * Math.exp(lambdaP * fnum(x, y))
+        }
+        s = s - ftildeArr[index]
+
+        return s
+    }
+
+    internal fun fExpected(f: Feature): Double {
+        val tF = f as TaggerFeature
+        var s = 0.0
+        val y = tF.yTag
+        for (i in 0 until f.len()) {
+            val x = tF.getX(i)
+            s = s + p.data.ptildeX(x) * pcond(y, x)
+        }
+        return s
+    }
+
+
+    /** Works out whether the model expectations match the empirical
+     * expectations.
+     * @return Whether the model is correct
+     */
+    override fun checkCorrectness(): Boolean {
+        log.info("Checking model correctness; x size " + p.data.xSize + ' '.toString() + ", ysize " + p.data.ySize)
+
+        val nf = NumberFormat.getNumberInstance()
+        nf.maximumFractionDigits = 4
+        var flag = true
+        for (f in lambda.indices) {
+            if (Math.abs(lambda[f]) > 100) {
+                log.info(" Lambda too big " + lambda[f])
+                log.info(" empirical " + ftildeArr[f] + " expected " + fExpected(p.functions.get(f)))
+            }
+        }
+
+        for (i in ftildeArr.indices) {
+            val exp = Math.abs(ftildeArr[i] - fExpected(p.functions.get(i)))
+            if (exp > 0.001) {
+                flag = false
+                log.info("Constraint " + i + " not satisfied emp " + nf.format(ftildeArr[i]) + " exp " + nf.format(fExpected(p.functions.get(i))) + " diff " + nf.format(exp) + " lambda " + nf.format(lambda[i]))
+            }
+        }
+        for (x in 0 until p.data.xSize) {
+            var s = 0.0
+            for (y in 0 until p.data.ySize) {
+                s = s + probConds[x][y]
+            }
+            if (Math.abs(s - 1) > 0.0001) {
+                for (y in 0 until p.data.ySize) {
+                    log.info("$y : " + probConds[x][y])
+                }
+                log.info("probabilities do not sum to one " + x + ' '.toString() + s.toFloat())
+            }
+        }
+        return flag
+    }
+
+    companion object {
+        /** A logger for this class  */
+        private val log = Redwood.channels(LambdaSolveTagger::class.java)
+    }
+}
+
Index: src/edu/stanford/nlp/tagger/maxent/MaxentTagger.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/MaxentTagger.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/MaxentTagger.java	(revision f3bf217addd8a9e5a5e4216a488f94870df00bec)
@@ -67,19 +67,19 @@
 
 /**
  * The main class for users to run, train, and test the part of speech tagger.
- *
+ * <p>
  * You can tag things through the Java API or from the command line.
  * The two English taggers included in this distribution are:
  *
  * <ul>
  * <li> A bi-directional dependency network tagger in
- *      {@code edu/stanford/nlp/models/pos-tagger/english-left3words/english-bidirectional-distsim.tagger}.
- *      Its accuracy was 97.32% on Penn Treebank WSJ secs. 22-24.</li>
+ * {@code edu/stanford/nlp/models/pos-tagger/english-left3words/english-bidirectional-distsim.tagger}.
+ * Its accuracy was 97.32% on Penn Treebank WSJ secs. 22-24.</li>
  * <li> A model using only left second-order sequence information and similar but less
- *      unknown words and lexical features as the previous model in
- *      {@code edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger}
- *      This tagger runs a lot faster, and is recommended for general use.
- *      Its accuracy was 96.92% on Penn Treebank WSJ secs. 22-24.</li>
+ * unknown words and lexical features as the previous model in
+ * {@code edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger}
+ * This tagger runs a lot faster, and is recommended for general use.
+ * Its accuracy was 96.92% on Penn Treebank WSJ secs. 22-24.</li>
  * </ul>
  *
  * <h3>Using the Java API</h3>
@@ -109,7 +109,7 @@
  * <dt>To tag a string of <i>correctly tokenized</i>, whitespace-separated words and get a string of tagged words back:</dt>
  * <dd> {@code String taggedString = tagger.tagTokenizedString("Here 's a tagged string .")}</dd>
  * </dl>
- *
+ * <p>
  * The {@code tagString} method uses the default tokenizer (PTBTokenizer).
  * If you wish to control tokenization, you may wish to call
  * {@link #tokenizeText(Reader, TokenizerFactory)} and then to call
@@ -117,26 +117,26 @@
  *
  *
  * <h3>Using the command line</h3>
- *
+ * <p>
  * Tagging, testing, and training can all also be done via the command line.
  * <h3>Training from the command line</h3>
  * To train a model from the command line, first generate a property file:
  * <pre>java edu.stanford.nlp.tagger.maxent.MaxentTagger -genprops </pre>
- *
+ * <p>
  * This gets you a default properties file with descriptions of each parameter you can set in
  * your trained model.  You can modify the properties file, or use the default options.  To train, run:
  * <pre>java -mx1g edu.stanford.nlp.tagger.maxent.MaxentTagger -props myPropertiesFile.props </pre>
- *
- *  with the appropriate properties file specified. Any argument you give in the properties file can also
- *  be specified on the command line.  You must have specified a model using -model, either in the properties file
- *  or on the command line, as well as a file containing tagged words using -trainFile.
- *
+ * <p>
+ * with the appropriate properties file specified. Any argument you give in the properties file can also
+ * be specified on the command line.  You must have specified a model using -model, either in the properties file
+ * or on the command line, as well as a file containing tagged words using -trainFile.
+ * <p>
  * Useful flags for controlling the amount of output are -verbose, which prints extra debugging information,
  * and -verboseResults, which prints full information about intermediate results.  -verbose defaults to false
  * and -verboseResults defaults to true.
  *
  * <h3>Tagging and Testing from the command line</h3>
- *
+ * <p>
  * Usage:
  * For tagging (plain text):
  * <pre>java edu.stanford.nlp.tagger.maxent.MaxentTagger -model modelFile -textFile textfile </pre>
@@ -146,11 +146,11 @@
  * if you pass it in with the "-props" argument. The most important
  * arguments for tagging (besides "model" and "file") are "tokenize"
  * and "tokenizerFactory". See below for more details.
- *
+ * <p>
  * Note that the tagger assumes input has not yet been tokenized and
  * by default tokenizes it using a default English tokenizer.  If your
  * input has already been tokenized, use the flag "-tokenize false".
- *
+ * <p>
  * Parameters can be defined using a Properties file
  * (specified on the command-line with {@code -prop} <i>propFile</i>),
  * or directly on the command line (by preceding their name with a minus sign
@@ -161,27 +161,27 @@
  * <tr><td><b>Property Name</b></td><td><b>Type</b></td><td><b>Default Value</b></td><td><b>Relevant Phase(s)</b></td><td><b>Description</b></td></tr>
  * <tr><td>model</td><td>String</td><td>N/A</td><td>All</td><td>Path and filename where you would like to save the model (training) or where the model should be loaded from (testing, tagging).</td></tr>
  * <tr><td>trainFile</td><td>String</td><td>N/A</td><td>Train</td>
-     <td>
-       Path to the file holding the training data; specifying this option puts the tagger in training mode.  Only one of 'trainFile','testFile','textFile', and 'dump' may be specified.<br>
-       There are three formats possible.  The first is a text file of tagged data. Each line is considered a separate sentence.  In each sentence, words are separated by whitespace.
-       Each word must have a tag, which is separated from the token using the specified {@code tagSeparator}.  This format, called TEXT, is the default format. <br>
-       The second format is a file of Penn Treebank formatted (i.e., s-expression) tree files.  Trees are loaded one at a time and the tagged words in a tree are used as a training sentence.
-       To specify this format, preface the filename with "{@code format=TREES,}".  <br>
-       The final possible format is TSV files (tab-separated columns).  To specify a TSV file, set {@code trainFile} to "{@code format=TSV,wordColumn=x,tagColumn=y,filename}".
-       Column numbers are indexed from 0, and sentences are separated with blank lines. The default wordColumn is 0 and default tagColumn is 1.
-       <br>
-       A file can be in a different character set encoding than the tagger's default encoding by prefacing the filename with {@code "encoding=ENC,"}.
-       You can specify the tagSeparator character in a TEXT file by prefacing the filename with "tagSeparator=c,". <br>
-       Tree files can be fed through TreeTransformers and TreeNormalizers.  To specify a transformer, preface the filename with "treeTransformer=CLASSNAME,".
-       To specify a normalizer, preface the filename with "treeNormalizer=CLASSNAME,".
-       You can also filter trees using a {@code Filter<Tree>}, which can be specified with "treeFilter=CLASSNAME,".
-       A specific range of trees to be used can be specified with treeRange=X-Y.  Multiple parts of the range can be separated by : as opposed to the normal separator of ,.
-       For example, one could use the argument "-treeRange=25-50:75-100".
-       You can specify a TreeReaderFactory by prefacing the filename with "trf=CLASSNAME,". Note: If it includes a TreeNormalizer, you want to specify it as the treeNormalizer as well.<br>
-       Multiple files can be specified by making a semicolon separated list of files.  Each file can have its own format specifiers as above.<br>
-       You will note that none of , ; or = can be in filenames.
-     </td>
-   </tr>
+ * <td>
+ * Path to the file holding the training data; specifying this option puts the tagger in training mode.  Only one of 'trainFile','testFile','textFile', and 'dump' may be specified.<br>
+ * There are three formats possible.  The first is a text file of tagged data. Each line is considered a separate sentence.  In each sentence, words are separated by whitespace.
+ * Each word must have a tag, which is separated from the token using the specified {@code tagSeparator}.  This format, called TEXT, is the default format. <br>
+ * The second format is a file of Penn Treebank formatted (i.e., s-expression) tree files.  Trees are loaded one at a time and the tagged words in a tree are used as a training sentence.
+ * To specify this format, preface the filename with "{@code format=TREES,}".  <br>
+ * The final possible format is TSV files (tab-separated columns).  To specify a TSV file, set {@code trainFile} to "{@code format=TSV,wordColumn=x,tagColumn=y,filename}".
+ * Column numbers are indexed from 0, and sentences are separated with blank lines. The default wordColumn is 0 and default tagColumn is 1.
+ * <br>
+ * A file can be in a different character set encoding than the tagger's default encoding by prefacing the filename with {@code "encoding=ENC,"}.
+ * You can specify the tagSeparator character in a TEXT file by prefacing the filename with "tagSeparator=c,". <br>
+ * Tree files can be fed through TreeTransformers and TreeNormalizers.  To specify a transformer, preface the filename with "treeTransformer=CLASSNAME,".
+ * To specify a normalizer, preface the filename with "treeNormalizer=CLASSNAME,".
+ * You can also filter trees using a {@code Filter<Tree>}, which can be specified with "treeFilter=CLASSNAME,".
+ * A specific range of trees to be used can be specified with treeRange=X-Y.  Multiple parts of the range can be separated by : as opposed to the normal separator of ,.
+ * For example, one could use the argument "-treeRange=25-50:75-100".
+ * You can specify a TreeReaderFactory by prefacing the filename with "trf=CLASSNAME,". Note: If it includes a TreeNormalizer, you want to specify it as the treeNormalizer as well.<br>
+ * Multiple files can be specified by making a semicolon separated list of files.  Each file can have its own format specifiers as above.<br>
+ * You will note that none of , ; or = can be in filenames.
+ * </td>
+ * </tr>
  * <tr><td>testFile</td><td>String</td><td>N/A</td><td>Test</td><td>Path to the file holding the test data; specifying this option puts the tagger in testing mode.  Only one of 'trainFile','testFile','textFile', and 'dump' may be specified.  The same format as trainFile applies, but only one file can be specified.</td></tr>
  * <tr><td>textFile</td><td>String</td><td>N/A</td><td>Tag</td><td>Path to the file holding the text to tag; specifying this option puts the tagger in tagging mode.  Only one of 'trainFile','testFile','textFile', and 'dump' may be specified.  No file reading options may be specified for textFile</td></tr>
  * <tr><td>dump</td><td>String</td><td>N/A</td><td>Dump</td><td>Path to the file holding the model to dump; specifying this option puts the tagger in dumping mode.  Only one of 'trainFile','testFile','textFile', and 'dump' may be specified.</td></tr>
@@ -218,7 +218,6 @@
  * <tr><td>nthreads</td><td>int</td><td>1</td><td>Test,Text</td><td>Number of threads to use when processing text.</td></tr>
  * </table>
  *
- *
  * @author Kristina Toutanova
  * @author Miler Lee
  * @author Joseph Smarr
@@ -227,1715 +226,1616 @@
  * @author Christopher Manning
  * @author John Bauer
  */
-public class MaxentTagger extends Tagger implements ListProcessor<List<? extends HasWord>,List<TaggedWord>>, Serializable  {
+public class MaxentTagger extends Tagger implements ListProcessor<List<? extends HasWord>, List<TaggedWord>>, Serializable {
 
-  /** A logger for this class */
-  private static final Redwood.RedwoodChannels log = Redwood.channels(MaxentTagger.class);
+    /**
+     * A logger for this class
+     */
+    private static final Redwood.RedwoodChannels log = Redwood.channels(MaxentTagger.class);
 
-  /**
-   * The directory from which to get taggers when using
-   * DEFAULT_NLP_GROUP_MODEL_PATH.  Normally set to the location of
-   * the latest left3words tagger on the NLP machines, but can be
-   * changed by setting the environment variable NLP_DATA_HOME.
-   */
-  public static final String BASE_TAGGER_HOME =
-    "$NLP_DATA_HOME/data/pos-tagger/distrib";
-  public static final String TAGGER_HOME =
-    DataFilePaths.convert(BASE_TAGGER_HOME);
+    /**
+     * The directory from which to get taggers when using
+     * DEFAULT_NLP_GROUP_MODEL_PATH.  Normally set to the location of
+     * the latest left3words tagger on the NLP machines, but can be
+     * changed by setting the environment variable NLP_DATA_HOME.
+     */
+
+    public static final String BASE_TAGGER_HOME =
+            "$NLP_DATA_HOME/data/pos-tagger/distrib";
+    public static final String TAGGER_HOME =
+            DataFilePaths.convert(BASE_TAGGER_HOME);
 
-  public static final String DEFAULT_NLP_GROUP_MODEL_PATH =
-    new File(TAGGER_HOME, "english-left3words-distsim.tagger").getPath();
-  public static final String DEFAULT_JAR_PATH =
-    "edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger";
-  public static final String DEFAULT_DISTRIBUTION_PATH =
-    "models/english-left3words-distsim.tagger";
+    public static final String DEFAULT_NLP_GROUP_MODEL_PATH =
+            new File(TAGGER_HOME, "english-left3words-distsim.tagger").getPath();
+    public static final String DEFAULT_JAR_PATH =
+            "edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger";
+    public static final String DEFAULT_DISTRIBUTION_PATH =
+            "models/english-left3words-distsim.tagger";
 
 
-  public MaxentTagger() {
-  }
+    public MaxentTagger() {
+    }
 
-  public MaxentTagger(TaggerConfig config) {
-    // todo: maybe this shouldn't do this but replace the zero arg constructor.
-    // i.e., call init() not readModelAndInit(). This method is currently UNUSED. Make non-public.
-    this(config.getModel(), config);
-  }
+    public MaxentTagger(TaggerConfig config) {
+        // todo: maybe this shouldn't do this but replace the zero arg constructor.
+        // i.e., call updatePointers() not readModelAndInit(). This method is currently UNUSED. Make non-public.
+        this(config.getModel(), config);
+    }
 
-  /**
-   * Constructor for a tagger, loading a model stored in a particular file,
-   * classpath resource, or URL.
-   * The tagger data is loaded when the constructor is called (this can be
-   * slow). This constructor first constructs a TaggerConfig object, which
-   * loads the tagger options from the modelFile.
-   *
-   * @param modelFile Filename, classpath resource, or URL for the trained model
-   * @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  public MaxentTagger(String modelFile) {
-    this(modelFile, StringUtils.argsToProperties("-model", modelFile), true);
-  }
+    /**
+     * Constructor for a tagger, loading a model stored in a particular file,
+     * classpath resource, or URL.
+     * The tagger data is loaded when the constructor is called (this can be
+     * slow). This constructor first constructs a TaggerConfig object, which
+     * loads the tagger options from the modelFile.
+     *
+     * @param modelFile Filename, classpath resource, or URL for the trained model
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    public MaxentTagger(String modelFile) {
+        this(modelFile, StringUtils.argsToProperties("-model", modelFile), true);
+    }
 
-  /**
-   * Constructor for a tagger, loading a model stored in a particular file,
-   * classpath resource, or URL.
-   * The tagger data is loaded when the constructor is called (this can be
-   * slow). This constructor first constructs a TaggerConfig object, which
-   * loads the tagger options from the modelFile.
-   *
-   * @param modelStream The InputStream from which to read the model
-   * @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  public MaxentTagger(InputStream modelStream) {
-    this(modelStream, new Properties(), true);
-  }
+    /**
+     * Constructor for a tagger, loading a model stored in a particular file,
+     * classpath resource, or URL.
+     * The tagger data is loaded when the constructor is called (this can be
+     * slow). This constructor first constructs a TaggerConfig object, which
+     * loads the tagger options from the modelFile.
+     *
+     * @param modelStream The InputStream from which to read the model
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    public MaxentTagger(InputStream modelStream) {
+        this(modelStream, new Properties(), true);
+    }
 
-  /**
-   * Constructor for a tagger using a model stored in a particular file,
-   * with options taken from the supplied TaggerConfig.
-   * The tagger data is loaded when the
-   * constructor is called (this can be slow).
-   * This version assumes that the tagger options in the modelFile have
-   * already been loaded into the TaggerConfig (if that is desired).
-   *
-   * @param modelFile Filename, classpath resource, or URL for the trained model
-   * @param config The configuration for the tagger
-   * @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  public MaxentTagger(String modelFile, Properties config) {
-    this(modelFile, config, true);
-  }
+    /**
+     * Constructor for a tagger using a model stored in a particular file,
+     * with options taken from the supplied TaggerConfig.
+     * The tagger data is loaded when the
+     * constructor is called (this can be slow).
+     * This version assumes that the tagger options in the modelFile have
+     * already been loaded into the TaggerConfig (if that is desired).
+     *
+     * @param modelFile Filename, classpath resource, or URL for the trained model
+     * @param config    The configuration for the tagger
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    public MaxentTagger(String modelFile, Properties config) {
+        this(modelFile, config, true);
+    }
 
-  /**
-   * Initializer that loads the tagger.
-   *
-   * @param modelFile Where to initialize the tagger from.
-   *        Most commonly, this is the filename of the trained model, for example,
-   *        {@code /u/nlp/data/pos-tagger/wsj3t0-18-left3words/left3words-wsj-0-18.tagger}.
-   *        However, if it starts with "https?://" it will be interpreted as a URL.
-   *        One can also load models directly from the classpath, as in loading from
-   *        {@code edu/stanford/nlp/models/pos-tagger/wsj3t0-18-bidirectional/bidirectional-distsim-wsj-0-18.tagger}.
-   * @param config TaggerConfig based on command-line arguments
-   * @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
-   * @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  public MaxentTagger(String modelFile, Properties config, boolean printLoading) {
-    readModelAndInit(config, modelFile, printLoading);
-  }
+    /**
+     * Initializer that loads the tagger.
+     *
+     * @param modelFile    Where to initialize the tagger from.
+     *                     Most commonly, this is the filename of the trained model, for example,
+     *                     {@code /u/nlp/data/pos-tagger/wsj3t0-18-left3words/left3words-wsj-0-18.tagger}.
+     *                     However, if it starts with "https?://" it will be interpreted as a URL.
+     *                     One can also load models directly from the classpath, as in loading from
+     *                     {@code edu/stanford/nlp/models/pos-tagger/wsj3t0-18-bidirectional/bidirectional-distsim-wsj-0-18.tagger}.
+     * @param config       TaggerConfig based on command-line arguments
+     * @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    public MaxentTagger(String modelFile, Properties config, boolean printLoading) {
+        readModelAndInit(config, modelFile, printLoading);
+    }
 
-  /**
-   * Initializer that loads the tagger.
-   *
-   * @param modelStream An InputStream for reading the model file
-   * @param config TaggerConfig based on command-line arguments
-   * @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
-   * @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  public MaxentTagger(InputStream modelStream, Properties config, boolean printLoading) {
-    readModelAndInit(config, modelStream, printLoading);
-  }
+    /**
+     * Initializer that loads the tagger.
+     *
+     * @param modelStream  An InputStream for reading the model file
+     * @param config       TaggerConfig based on command-line arguments
+     * @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    public MaxentTagger(InputStream modelStream, Properties config, boolean printLoading) {
+        readModelAndInit(config, modelStream, printLoading);
+    }
 
-  final Dictionary dict = new Dictionary();
-  TTags tags;
+    final Dictionary dict = new Dictionary();
+    TTags tags;
 
-  /**
-   * Will return the index of a tag, adding it if it doesn't already exist
-   */
-  public int addTag(String tag) {
-    return tags.add(tag);
-  }
-  /**
-   * Will return the index of a tag if known, -1 if not already known
-   */
-  public int getTagIndex(String tag) {
-    return tags.getIndex(tag);
-  }
+    /**
+     * Will return the index of a tag, adding it if it doesn't already exist
+     */
+    public int addTag(String tag) {
+        return tags.add(tag);
+    }
+
+    /**
+     * Will return the index of a tag if known, -1 if not already known
+     */
+    public int getTagIndex(String tag) {
+        return tags.indexOf(tag);
+    }
 
-  public int numTags() {
-    return tags.getSize();
-  }
+    public int numTags() {
+        return tags.getSize();
+    }
 
-  public String getTag(int index) {
-    return tags.getTag(index);
-  }
+    public String getTag(int index) {
+        return tags.getTag(index);
+    }
 
-  public Set<String> tagSet() {
-    return tags.tagSet();
-  }
+    public Set<String> tagSet() {
+        return tags.tagSet();
+    }
 
-  private LambdaSolveTagger prob;
+    private LambdaSolveTagger prob;
 
-  // For each extractor index (List index), we have a Map from possible extracted
-  // feature values to an array which maps from tag number to feature weight index in the lambdas array.
-  List<Map<String, int[]>> fAssociations = Generics.newArrayList();
-  //PairsHolder pairs = new PairsHolder();
-  Extractors extractors;
-  Extractors extractorsRare;
-  AmbiguityClasses ambClasses;
-  final boolean alltags = false;
-  final Map<String, Set<String>> tagTokens = Generics.newHashMap();
+    // For each extractor index (List index), we have a Map from possible extracted
+    // feature values to an array which maps from tag number to feature weight index in the lambdas array.
+    List<Map<String, int[]>> fAssociations = Generics.newArrayList();
+    //PairsHolder pairs = new PairsHolder();
+    Extractors extractors;
+    Extractors extractorsRare;
+    AmbiguityClasses ambClasses;
+    final boolean alltags = false;
+    final Map<String, Set<String>> tagTokens = Generics.newHashMap();
 
-  private static final int RARE_WORD_THRESH = Integer.parseInt(TaggerConfig.RARE_WORD_THRESH);
-  private static final int MIN_FEATURE_THRESH = Integer.parseInt(TaggerConfig.MIN_FEATURE_THRESH);
-  private static final int CUR_WORD_MIN_FEATURE_THRESH = Integer.parseInt(TaggerConfig.CUR_WORD_MIN_FEATURE_THRESH);
-  private static final int RARE_WORD_MIN_FEATURE_THRESH = Integer.parseInt(TaggerConfig.RARE_WORD_MIN_FEATURE_THRESH);
-  private static final int VERY_COMMON_WORD_THRESH = Integer.parseInt(TaggerConfig.VERY_COMMON_WORD_THRESH);
+    private static final int RARE_WORD_THRESH = Integer.parseInt(TaggerConfig.RARE_WORD_THRESH);
+    private static final int MIN_FEATURE_THRESH = Integer.parseInt(TaggerConfig.MIN_FEATURE_THRESH);
+    private static final int CUR_WORD_MIN_FEATURE_THRESH = Integer.parseInt(TaggerConfig.CUR_WORD_MIN_FEATURE_THRESH);
+    private static final int RARE_WORD_MIN_FEATURE_THRESH = Integer.parseInt(TaggerConfig.RARE_WORD_MIN_FEATURE_THRESH);
+    private static final int VERY_COMMON_WORD_THRESH = Integer.parseInt(TaggerConfig.VERY_COMMON_WORD_THRESH);
 
-  private static final boolean OCCURRING_TAGS_ONLY = Boolean.parseBoolean(TaggerConfig.OCCURRING_TAGS_ONLY);
-  private static final boolean POSSIBLE_TAGS_ONLY = Boolean.parseBoolean(TaggerConfig.POSSIBLE_TAGS_ONLY);
+    private static final boolean OCCURRING_TAGS_ONLY = Boolean.parseBoolean(TaggerConfig.OCCURRING_TAGS_ONLY);
+    private static final boolean POSSIBLE_TAGS_ONLY = Boolean.parseBoolean(TaggerConfig.POSSIBLE_TAGS_ONLY);
 
-  private double defaultScore;
-  private double[] defaultScores; // = null;
+    private double defaultScore;
+    private double[] defaultScores; // = null;
 
-  int leftContext;
-  int rightContext;
+    int leftContext;
+    int rightContext;
 
-  TaggerConfig config;
+    TaggerConfig config;
 
-  /**
-   * Determines which words are considered rare.  All words with count
-   * in the training data strictly less than this number (standardly, &lt; 5) are
-   * considered rare.
-   */
-  private int rareWordThresh = RARE_WORD_THRESH;
+    /**
+     * Determines which words are considered rare.  All words with count
+     * in the training data strictly less than this number (standardly, &lt; 5) are
+     * considered rare.
+     */
+    private int rareWordThresh = RARE_WORD_THRESH;
 
-  /**
-   * Determines which features are included in the model.  The model
-   * includes features that occurred strictly more times than this number
-   * (standardly, &gt; 5) in the training data.  Here I look only at the
-   * history (not the tag), so the history appearing this often is enough.
-   */
-  int minFeatureThresh = MIN_FEATURE_THRESH;
+    /**
+     * Determines which features are included in the model.  The model
+     * includes features that occurred strictly more times than this number
+     * (standardly, &gt; 5) in the training data.  Here I look only at the
+     * history (not the tag), so the history appearing this often is enough.
+     */
+    int minFeatureThresh = MIN_FEATURE_THRESH;
 
-  /**
-   * This is a special threshold for the current word feature.
-   * Only words that have occurred strictly &gt; this number of times
-   * in total will generate word features with all of their occurring tags.
-   * The traditional default was 2.
-   */
-  int curWordMinFeatureThresh = CUR_WORD_MIN_FEATURE_THRESH;
+    /**
+     * This is a special threshold for the current word feature.
+     * Only words that have occurred strictly &gt; this number of times
+     * in total will generate word features with all of their occurring tags.
+     * The traditional default was 2.
+     */
+    int curWordMinFeatureThresh = CUR_WORD_MIN_FEATURE_THRESH;
 
-  /**
-   * Determines which rare word features are included in the model.
-   * The features for rare words have a strictly higher support than
-   * this number are included. Traditional default is 10.
-   */
-  int rareWordMinFeatureThresh = RARE_WORD_MIN_FEATURE_THRESH;
+    /**
+     * Determines which rare word features are included in the model.
+     * The features for rare words have a strictly higher support than
+     * this number are included. Traditional default is 10.
+     */
+    int rareWordMinFeatureThresh = RARE_WORD_MIN_FEATURE_THRESH;
 
-  /**
-   * If using tag equivalence classes on following words, words that occur
-   * strictly more than this number of times (in total with any tag)
-   * are sufficiently frequent to form an equivalence class
-   * by themselves. (Not used unless using equivalence classes.)
-   *
-   * There are places in the code (ExtractorAmbiguityClass.java, for one)
-   * that assume this value is constant over the life of a tagger.
-   */
-  int veryCommonWordThresh = VERY_COMMON_WORD_THRESH;
+    /**
+     * If using tag equivalence classes on following words, words that occur
+     * strictly more than this number of times (in total with any tag)
+     * are sufficiently frequent to form an equivalence class
+     * by themselves. (Not used unless using equivalence classes.)
+     * <p>
+     * There are places in the code (ExtractorAmbiguityClass.java, for one)
+     * that assume this value is constant over the life of a tagger.
+     */
+    int veryCommonWordThresh = VERY_COMMON_WORD_THRESH;
 
 
-  int xSize;
-  int ySize;
-  boolean occurringTagsOnly = OCCURRING_TAGS_ONLY;
-  boolean possibleTagsOnly = POSSIBLE_TAGS_ONLY;
+    int xSize;
+    int ySize;
+    boolean occurringTagsOnly = OCCURRING_TAGS_ONLY;
+    boolean possibleTagsOnly = POSSIBLE_TAGS_ONLY;
 
-  private boolean initted = false;
+    private boolean initted = false;
 
-  boolean VERBOSE = false;
+    boolean VERBOSE = false;
 
-  /**
-   * This is a function used to preprocess all text before applying
-   * the tagger to it.  For example, it could be a function to
-   * lowercase text, such as edu.stanford.nlp.util.LowercaseFunction
-   * (which makes the tagger case insensitive).  It is applied in
-   * ReadDataTagged, which loads in the training data, and in
-   * TestSentence, which processes sentences for new queries.  If any
-   * other classes are added or modified which use raw text, they must
-   * also use this function to keep results consistent.
-   * <br>
-   * An alternate design would have been to use the function at a
-   * lower level, such as at the extractor level.  That would have
-   * require more invasive changes to the tagger, though, because
-   * other data structures such as the Dictionary would then be using
-   * raw text as well.  This is also more efficient, in that the
-   * function is applied once at the start of the process.
-   */
-  Function<String, String> wordFunction;
+    /**
+     * This is a function used to preprocess all text before applying
+     * the tagger to it.  For example, it could be a function to
+     * lowercase text, such as edu.stanford.nlp.util.LowercaseFunction
+     * (which makes the tagger case insensitive).  It is applied in
+     * ReadDataTagged, which loads in the training data, and in
+     * TestSentence, which processes sentences for new queries.  If any
+     * other classes are added or modified which use raw text, they must
+     * also use this function to keep results consistent.
+     * <br>
+     * An alternate design would have been to use the function at a
+     * lower level, such as at the extractor level.  That would have
+     * require more invasive changes to the tagger, though, because
+     * other data structures such as the Dictionary would then be using
+     * raw text as well.  This is also more efficient, in that the
+     * function is applied once at the start of the process.
+     */
+    Function<String, String> wordFunction;
 
 
-  /* Package access - shouldn't be part of public API. */
-  LambdaSolve getLambdaSolve() {
-    return prob;
-  }
+    /* Package access - shouldn't be part of public API. */
+    LambdaSolve getLambdaSolve() {
+        return prob;
+    }
 
-  // TODO: make these constructors instead of init methods?
-  void init(TaggerConfig config) {
-    if (initted) return;  // TODO: why not reinit?
+    // TODO: make these constructors instead of updatePointers methods?
+    void init(TaggerConfig config) {
+        if (initted) return;  // TODO: why not reinit?
 
-    this.config = config;
+        this.config = config;
 
-    String lang, arch;
-    String[] openClassTags, closedClassTags;
+        String lang, arch;
+        String[] openClassTags, closedClassTags;
 
-    if (config == null) {
-      lang = "english";
-      arch = "left3words";
-      openClassTags = StringUtils.EMPTY_STRING_ARRAY;
-      closedClassTags = StringUtils.EMPTY_STRING_ARRAY;
-      wordFunction = null;
-    } else {
-      this.VERBOSE = config.getVerbose();
+        if (config == null) {
+            lang = "english";
+            arch = "left3words";
+            openClassTags = StringUtils.EMPTY_STRING_ARRAY;
+            closedClassTags = StringUtils.EMPTY_STRING_ARRAY;
+            wordFunction = null;
+        } else {
+            this.VERBOSE = config.getVerbose();
 
-      lang = config.getLang();
-      arch = config.getArch();
-      openClassTags = config.getOpenClassTags();
-      closedClassTags = config.getClosedClassTags();
-      if (!config.getWordFunction().equals("")) {
-        wordFunction =
-          ReflectionLoading.loadByReflection(config.getWordFunction());
-      }
+            lang = config.getLang();
+            arch = config.getArch();
+            openClassTags = config.getOpenClassTags();
+            closedClassTags = config.getClosedClassTags();
+            if (!config.getWordFunction().equals("")) {
+                wordFunction =
+                        ReflectionLoading.loadByReflection(config.getWordFunction());
+            }
 
-      if (((openClassTags.length > 0) && !lang.equals("")) || ((closedClassTags.length > 0) && !lang.equals("")) || ((closedClassTags.length > 0) && (openClassTags.length > 0))) {
-        throw new RuntimeException("At least two of lang (\"" + lang + "\"), openClassTags (length " + openClassTags.length + ": " + Arrays.toString(openClassTags) + ")," +
-            "and closedClassTags (length " + closedClassTags.length + ": " + Arrays.toString(closedClassTags) + ") specified---you must choose one!");
-      } else if ((openClassTags.length == 0) && lang.equals("") && (closedClassTags.length == 0) && ! config.getLearnClosedClassTags()) {
-        log.info("warning: no language set, no open-class tags specified, and no closed-class tags specified; assuming ALL tags are open class tags");
-      }
-    }
+            if (((openClassTags.length > 0) && !lang.equals("")) || ((closedClassTags.length > 0) && !lang.equals("")) || ((closedClassTags.length > 0) && (openClassTags.length > 0))) {
+                throw new RuntimeException("At least two of lang (\"" + lang + "\"), openClassTags (length " + openClassTags.length + ": " + Arrays.toString(openClassTags) + ")," +
+                        "and closedClassTags (length " + closedClassTags.length + ": " + Arrays.toString(closedClassTags) + ") specified---you must choose one!");
+            } else if ((openClassTags.length == 0) && lang.equals("") && (closedClassTags.length == 0) && !config.getLearnClosedClassTags()) {
+                log.info("warning: no language set, no open-class tags specified, and no closed-class tags specified; assuming ALL tags are open class tags");
+            }
+        }
 
-    if (openClassTags.length > 0) {
-      tags = new TTags();
-      tags.setOpenClassTags(openClassTags);
-    } else if (closedClassTags.length > 0) {
-      tags = new TTags();
-      tags.setClosedClassTags(closedClassTags);
-    } else {
-      tags = new TTags(lang);
-    }
+        if (openClassTags.length > 0) {
+            tags = new TTags(config);
+            tags.setOpenClassTags(openClassTags);
+        } else if (closedClassTags.length > 0) {
+            tags = new TTags(config);
+            tags.setClosedClassTags(closedClassTags);
+        } else {
+            tags = new TTags(config, lang);
+        }
 
-    defaultScore = lang.equals("english") ? 1.0 : 0.0;
+        defaultScore = lang.equals("english") ? 1.0 : 0.0;
 
-    if (config != null) {
-      rareWordThresh = config.getRareWordThresh();
-      minFeatureThresh = config.getMinFeatureThresh();
-      curWordMinFeatureThresh = config.getCurWordMinFeatureThresh();
-      rareWordMinFeatureThresh = config.getRareWordMinFeatureThresh();
-      veryCommonWordThresh = config.getVeryCommonWordThresh();
-      occurringTagsOnly = config.occurringTagsOnly();
-      possibleTagsOnly = config.possibleTagsOnly();
-      // log.info("occurringTagsOnly: "+occurringTagsOnly);
-      // log.info("possibleTagsOnly: "+possibleTagsOnly);
+        rareWordThresh = config.getRareWordThresh();
+        minFeatureThresh = config.getMinFeatureThresh();
+        curWordMinFeatureThresh = config.getCurWordMinFeatureThresh();
+        rareWordMinFeatureThresh = config.getRareWordMinFeatureThresh();
+        veryCommonWordThresh = config.getVeryCommonWordThresh();
+        occurringTagsOnly = config.occurringTagsOnly();
+        possibleTagsOnly = config.possibleTagsOnly();
+        // log.info("occurringTagsOnly: "+occurringTagsOnly);
+        // log.info("possibleTagsOnly: "+possibleTagsOnly);
 
-      defaultScore = config.getDefaultScore();
-    }
+        defaultScore = config.getDefaultScore();
 
-    // just in case, reset the defaultScores array so it will be
-    // recached later when needed.  can't initialize it now in case we
-    // don't know ysize yet
-    defaultScores = null;
+        // just in case, reset the defaultScores array so it will be
+        // recached later when needed.  can't initialize it now in case we
+        // don't know ysize yet
+        defaultScores = null;
 
-    if (config == null || config.getMode() == TaggerConfig.Mode.TRAIN) {
-      // initialize the extractors based on the arch variable
-      // you only need to do this when training; otherwise they will be
-      // restored from the serialized file
-      extractors = new Extractors(ExtractorFrames.getExtractorFrames(arch));
-      extractorsRare = new Extractors(ExtractorFramesRare.getExtractorFramesRare(arch, tags));
+        if (config.getMode() == TaggerConfig.Mode.TRAIN) {
+            // initialize the extractors based on the arch variable
+            // you only need to do this when training; otherwise they will be
+            // restored from the serialized file
+            extractors = new Extractors(ExtractorFrames.getExtractorFrames(arch));
+            extractorsRare = new Extractors(ExtractorFramesRare.getExtractorFramesRare(arch, tags));
 
-      setExtractorsGlobal();
-    }
+            setExtractorsGlobal();
+        }
 
-    ambClasses = new AmbiguityClasses(tags);
+        ambClasses = new AmbiguityClasses(tags);
 
-    initted = true;
-  }
+        initted = true;
+    }
 
 
-  private synchronized void initDefaultScores() {
-    if (defaultScores == null) {
-      defaultScores = new double[ySize + 1];
-      for (int i = 0; i < ySize + 1; ++i) {
-        defaultScores[i] = Math.log(i * defaultScore);
-      }
-    }
-  }
+    private synchronized void initDefaultScores() {
+        if (defaultScores == null) {
+            defaultScores = new double[ySize + 1];
+            for (int i = 0; i < ySize + 1; ++i) {
+                defaultScores[i] = Math.log(i * defaultScore);
+            }
+        }
+    }
 
-  /**
-   * Caches a math log operation to save a tiny bit of time
-   */
-  double getInactiveTagDefaultScore(int nDefault) {
-    if (defaultScores == null) {
-      initDefaultScores();
-    }
-    return defaultScores[nDefault];
-  }
-
-  boolean hasApproximateScoring() {
-    return defaultScore > 0.0;
-  }
-
-  /**
-   * Figures out what tokenizer factory might be described by the
-   * config.  If it's described by name in the config, uses reflection
-   * to get the factory (which may cause an exception, of course...)
-   */
-  protected TokenizerFactory<? extends HasWord> chooseTokenizerFactory() {
-    return chooseTokenizerFactory(config.getTokenize(),
-                                  config.getTokenizerFactory(),
-                                  config.getTokenizerOptions(),
-                                  config.getTokenizerInvertible());
-  }
+    /**
+     * Figures out what tokenizer factory might be described by the
+     * config.  If it's described by name in the config, uses reflection
+     * to get the factory (which may cause an exception, of course...)
+     */
+    private TokenizerFactory<? extends HasWord> chooseTokenizerFactory() {
+        return chooseTokenizerFactory(config.getTokenize(),
+                config.getTokenizerFactory(),
+                config.getTokenizerOptions(),
+                config.getTokenizerInvertible());
+    }
 
-  protected static TokenizerFactory<? extends HasWord>
+    private static TokenizerFactory<? extends HasWord>
     chooseTokenizerFactory(boolean tokenize, String tokenizerFactory,
                            String tokenizerOptions, boolean invertible) {
-    if (tokenize && tokenizerFactory.trim().length() != 0) {
-      //return (TokenizerFactory<? extends HasWord>) Class.forName(getTokenizerFactory()).newInstance();
-      try {
-        @SuppressWarnings({"unchecked"})
-        Class<TokenizerFactory<? extends HasWord>> clazz = (Class<TokenizerFactory<? extends HasWord>>) Class.forName(tokenizerFactory.trim());
-        Method factoryMethod = clazz.getMethod("newTokenizerFactory");
-        @SuppressWarnings({"unchecked"})
-        TokenizerFactory<? extends HasWord> factory = (TokenizerFactory<? extends HasWord>) factoryMethod.invoke(tokenizerOptions);
-        return factory;
-      } catch (Exception e) {
-        throw new RuntimeException("Could not load tokenizer factory", e);
-      }
-    } else if (tokenize) {
-      if (invertible) {
-        if (tokenizerOptions.equals("")) {
-          tokenizerOptions = "invertible=true";
-        } else if (!tokenizerOptions.matches("(^|.*,)invertible=true")) {
-          tokenizerOptions += ",invertible=true";
-        }
-        return PTBTokenizerFactory.newCoreLabelTokenizerFactory(tokenizerOptions);
-      } else {
-        return PTBTokenizerFactory.newWordTokenizerFactory(tokenizerOptions);
-      }
-    } else {
-      return WhitespaceTokenizer.factory();
-    }
-  }
+        if (tokenize && tokenizerFactory.trim().length() != 0) {
+            //return (TokenizerFactory<? extends HasWord>) Class.forName(getTokenizerFactory()).newInstance();
+            try {
+                @SuppressWarnings({"unchecked"})
+                Class<TokenizerFactory<? extends HasWord>> clazz = (Class<TokenizerFactory<? extends HasWord>>) Class.forName(tokenizerFactory.trim());
+                Method factoryMethod = clazz.getMethod("newTokenizerFactory");
+                @SuppressWarnings({"unchecked"})
+                TokenizerFactory<? extends HasWord> factory = (TokenizerFactory<? extends HasWord>) factoryMethod.invoke(tokenizerOptions);
+                return factory;
+            } catch (Exception e) {
+                throw new RuntimeException("Could not load tokenizer factory", e);
+            }
+        } else if (tokenize) {
+            if (invertible) {
+                if (tokenizerOptions.equals("")) {
+                    tokenizerOptions = "invertible=true";
+                } else if (!tokenizerOptions.matches("(^|.*,)invertible=true")) {
+                    tokenizerOptions += ",invertible=true";
+                }
+                return PTBTokenizerFactory.newCoreLabelTokenizerFactory(tokenizerOptions);
+            } else {
+                return PTBTokenizerFactory.newWordTokenizerFactory(tokenizerOptions);
+            }
+        } else {
+            return WhitespaceTokenizer.factory();
+        }
+    }
 
-  /** Serialize the ExtractorFrames and ExtractorFramesRare to os. */
-  private void saveExtractors(OutputStream os) throws IOException {
-    ObjectOutputStream out = new ObjectOutputStream(os);
-    out.writeObject(extractors);
-    out.writeObject(extractorsRare);
-    out.flush();
-  }
+    /**
+     * Serialize the ExtractorFrames and ExtractorFramesRare to os.
+     */
+    private void saveExtractors(OutputStream os) throws IOException {
+        ObjectOutputStream out = new ObjectOutputStream(os);
+        out.writeObject(extractors);
+        out.writeObject(extractorsRare);
+        out.flush();
+    }
 
-  /** Read the extractors from a stream. */
-  private void readExtractors(InputStream file) throws IOException, ClassNotFoundException {
-    ObjectInputStream in = new ObjectInputStream(file);
-    extractors = (Extractors) in.readObject();
-    extractorsRare = (Extractors) in.readObject();
-    extractors.initTypes();
-    extractorsRare.initTypes();
-    int left = extractors.leftContext();
-    int left_u = extractorsRare.leftContext();
-    if (left_u > left) {
-      left = left_u;
-    }
-    leftContext = left;
-    int right = extractors.rightContext();
-    int right_u = extractorsRare.rightContext();
-    if (right_u > right) {
-      right = right_u;
-    }
-    rightContext = right;
+    /**
+     * Read the extractors from a stream.
+     */
+    private void readExtractors(InputStream file) throws IOException, ClassNotFoundException {
+        ObjectInputStream in = new ObjectInputStream(file);
+        extractors = (Extractors) in.readObject();
+        extractorsRare = (Extractors) in.readObject();
+        extractors.initTypes();
+        extractorsRare.initTypes();
+        int left = extractors.leftContext();
+        int left_u = extractorsRare.leftContext();
+        if (left_u > left) {
+            left = left_u;
+        }
+        leftContext = left;
+        int right = extractors.rightContext();
+        int right_u = extractorsRare.rightContext();
+        if (right_u > right) {
+            right = right_u;
+        }
+        rightContext = right;
 
-    setExtractorsGlobal();
-  }
+        setExtractorsGlobal();
+    }
 
-  // Sometimes there is data associated with the tagger (such as a
-  // dictionary) that we don't want saved with each extractor.  This
-  // call lets those extractors get that information from the tagger
-  // after being loaded from a data file.
-  private void setExtractorsGlobal() {
-    extractors.setGlobalHolder(this);
-    extractorsRare.setGlobalHolder(this);
-  }
+    // Sometimes there is data associated with the tagger (such as a
+    // dictionary) that we don't want saved with each extractor.  This
+    // call lets those extractors get that information from the tagger
+    // after being loaded from a data file.
+    private void setExtractorsGlobal() {
+        extractors.setGlobalHolder(this);
+        extractorsRare.setGlobalHolder(this);
+    }
 
-  /** Removes features that never have a non-zero weight for any tag from
-   *  the fAssociations' appropriate Map.
-   */
-  private void removeDeadRules() {
-    for (Map<String, int[]> fAssociation : fAssociations) {
-      List<String> deadRules = Generics.newArrayList();
-      for (Map.Entry<String, int[]> entry : fAssociation.entrySet()) {
-        String value = entry.getKey();
-        int[] fAssociations = entry.getValue();
+    /**
+     * Removes features that never have a non-zero weight for any tag from
+     * the fAssociations' appropriate Map.
+     */
+    private void removeDeadRules() {
+        for (Map<String, int[]> fAssociation : fAssociations) {
+            List<String> deadRules = Generics.newArrayList();
+            for (Map.Entry<String, int[]> entry : fAssociation.entrySet()) {
+                String value = entry.getKey();
+                int[] fAssociations = entry.getValue();
 
-        boolean found = false;
-        for (int index = 0; index < ySize; ++index) {
-          int fNum = fAssociations[index];
-          if (fNum > -1) {
-            if (getLambdaSolve().lambda[fNum] != 0.0) {
-              found = true;
-              break;
-            }
-          }
-        }
-        if (!found) {
-          deadRules.add(value);
-        }
-      }
+                boolean found = false;
+                for (int index = 0; index < ySize; ++index) {
+                    int fNum = fAssociations[index];
+                    if (fNum > -1) {
+                        if (getLambdaSolve().lambda[fNum] != 0.0) {
+                            found = true;
+                            break;
+                        }
+                    }
+                }
+                if (!found) {
+                    deadRules.add(value);
+                }
+            }
 
-      for (String rule : deadRules) {
-        fAssociation.remove(rule);
-      }
-    }
-  }
+            for (String rule : deadRules) {
+                fAssociation.remove(rule);
+            }
+        }
+    }
 
-  /**
-   * Searching the lambda array for 0 entries, removes them.  This
-   * saves a large chunk of space in the tagger models which are built
-   * with L1 regularization.
-   * <br>
-   * After removing the zeroes, go through the feature arrays and
-   * reindex the pointers into the lambda array.  This saves some time
-   * later on at runtime.
-   */
-  private void simplifyLambda() {
-    double[] lambda = getLambdaSolve().lambda;
-    int[] map = new int[lambda.length];
-    int current = 0;
-    for (int index = 0; index < lambda.length; ++index) {
-      if (lambda[index] == 0.0) {
-        map[index] = -1;
-      } else {
-        map[index] = current;
-        current++;
-      }
-    }
+    /**
+     * Searching the lambda array for 0 entries, removes them.  This
+     * saves a large chunk of space in the tagger models which are built
+     * with L1 regularization.
+     * <br>
+     * After removing the zeroes, go through the feature arrays and
+     * reindex the pointers into the lambda array.  This saves some time
+     * later on at runtime.
+     */
+    private void simplifyLambda() {
+        double[] lambda = getLambdaSolve().lambda;
+        int[] map = new int[lambda.length];
+        int current = 0;
+        for (int index = 0; index < lambda.length; ++index) {
+            if (lambda[index] == 0.0) {
+                map[index] = -1;
+            } else {
+                map[index] = current;
+                current++;
+            }
+        }
 
-    double[] condensedLambda = new double[current];
-    for (int i = 0; i < lambda.length; ++i) {
-      if (map[i] != -1) {
-        condensedLambda[map[i]] = lambda[i];
-      }
-    }
+        double[] condensedLambda = new double[current];
+        for (int i = 0; i < lambda.length; ++i) {
+            if (map[i] != -1) {
+                condensedLambda[map[i]] = lambda[i];
+            }
+        }
 
-    for (Map<String, int[]> featureMap : fAssociations) {
-      for (Map.Entry<String, int[]> entry : featureMap.entrySet()) {
-        int[] fAssociations = entry.getValue();
-        for (int index = 0; index < ySize; ++index) {
-          if (fAssociations[index] >= 0) {
-            fAssociations[index] = map[fAssociations[index]];
-          }
-        }
-      }
-    }
+        for (Map<String, int[]> featureMap : fAssociations) {
+            for (Map.Entry<String, int[]> entry : featureMap.entrySet()) {
+                int[] fAssociations = entry.getValue();
+                for (int index = 0; index < ySize; ++index) {
+                    if (fAssociations[index] >= 0) {
+                        fAssociations[index] = map[fAssociations[index]];
+                    }
+                }
+            }
+        }
 
-    prob = new LambdaSolveTagger(condensedLambda);
-  }
+        prob = new LambdaSolveTagger(condensedLambda);
+    }
 
-  protected void saveModel(String filename) {
-    try {
-      DataOutputStream file = IOUtils.getDataOutputStream(filename);
-      saveModel(file);
-      file.close();
-    } catch (IOException ioe) {
-      log.info("Error saving tagger to file " + filename);
-      throw new RuntimeIOException(ioe);
-    }
-  }
+    protected void saveModel(String filename) {
+        try {
+            DataOutputStream file = IOUtils.getDataOutputStream(filename);
+            saveModel(file);
+            file.close();
+        } catch (IOException ioe) {
+            log.info("Error saving tagger to file " + filename);
+            throw new RuntimeIOException(ioe);
+        }
+    }
 
-  protected void saveModel(DataOutputStream file) throws IOException {
-      config.saveConfig(file);
-      file.writeInt(xSize);
-      file.writeInt(ySize);
-      dict.save(file);
-      tags.save(file, tagTokens);
+    private void saveModel(DataOutputStream file) throws IOException {
+        config.saveConfig(file);
+        file.writeInt(xSize);
+        file.writeInt(ySize);
+        dict.save(file);
+        tags.save(file, tagTokens);
 
-      saveExtractors(file);
+        saveExtractors(file);
 
-      int sizeAssoc = 0;
-      for (Map<String, int[]> fValueAssociations : fAssociations) {
-        for (int[] fTagAssociations : fValueAssociations.values()) {
-          for (int association : fTagAssociations) {
-            if (association >= 0) {
-              ++sizeAssoc;
-            }
-          }
-        }
-      }
-      file.writeInt(sizeAssoc);
-      for (int i = 0; i < fAssociations.size(); ++i) {
-        Map<String, int[]> fValueAssociations = fAssociations.get(i);
-        for (Map.Entry<String, int[]> item : fValueAssociations.entrySet()) {
-          String featureValue = item.getKey();
-          int[] fTagAssociations = item.getValue();
-          for (int j = 0; j < fTagAssociations.length; ++j) {
-            int association = fTagAssociations[j];
-            if (association >= 0) {
-              file.writeInt(association);
-              FeatureKey fk = new FeatureKey(i, featureValue, tags.getTag(j));
-              fk.save(file);
-            }
-          }
-        }
-      }
+        int sizeAssoc = 0;
+        for (Map<String, int[]> fValueAssociations : fAssociations) {
+            for (int[] fTagAssociations : fValueAssociations.values()) {
+                for (int association : fTagAssociations) {
+                    if (association >= 0) {
+                        ++sizeAssoc;
+                    }
+                }
+            }
+        }
+        file.writeInt(sizeAssoc);
+        for (int i = 0; i < fAssociations.size(); ++i) {
+            Map<String, int[]> fValueAssociations = fAssociations.get(i);
+            for (Map.Entry<String, int[]> item : fValueAssociations.entrySet()) {
+                String featureValue = item.getKey();
+                int[] fTagAssociations = item.getValue();
+                for (int j = 0; j < fTagAssociations.length; ++j) {
+                    int association = fTagAssociations[j];
+                    if (association >= 0) {
+                        file.writeInt(association);
+                        FeatureKey fk = new FeatureKey(i, featureValue, tags.getTag(j));
+                        fk.save(file);
+                    }
+                }
+            }
+        }
 
-      LambdaSolve.save_lambdas(file, prob.lambda);
-  }
+        LambdaSolve.save_lambdas(file, prob.lambda);
+    }
 
-  /** This reads the complete tagger from a single model stored in a file, at a URL,
-   *  or as a resource in a jar file, and initializes the tagger using a
-   *  combination of the properties passed in and parameters from the file.
-   *  <br>
-   *  <i>Note for the future:</i> This assumes that the TaggerConfig in the file
-   *  has already been read and used.  This work is done inside the
-   *  constructor of TaggerConfig.  It might be better to refactor
-   *  things so that is all done inside this method, but for the moment
-   *  it seemed better to leave working code alone [cdm 2008].
-   *
-   *  @param config The tagger config
-   *  @param modelFileOrUrl The name of the model file. This routine opens and closes it.
-   *  @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
-   *  @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  protected void readModelAndInit(Properties config, String modelFileOrUrl, boolean printLoading) {
-    try (InputStream is = IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(modelFileOrUrl)) {
-      readModelAndInit(config, is, printLoading);
-    } catch (IOException e) {
-      throw new RuntimeIOException("Error while loading a tagger model (probably missing model file)", e);
-    }
-  }
+    /**
+     * This reads the complete tagger from a single model stored in a file, at a URL,
+     * or as a resource in a jar file, and initializes the tagger using a
+     * combination of the properties passed in and parameters from the file.
+     * <br>
+     * <i>Note for the future:</i> This assumes that the TaggerConfig in the file
+     * has already been read and used.  This work is done inside the
+     * constructor of TaggerConfig.  It might be better to refactor
+     * things so that is all done inside this method, but for the moment
+     * it seemed better to leave working code alone [cdm 2008].
+     *
+     * @param config         The tagger config
+     * @param modelFileOrUrl The name of the model file. This routine opens and closes it.
+     * @param printLoading   Whether to print a message saying what model file is being loaded and how long it took when finished.
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    private void readModelAndInit(Properties config, String modelFileOrUrl, boolean printLoading) {
+        try (InputStream is = IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(modelFileOrUrl)) {
+            readModelAndInit(config, is, printLoading);
+        } catch (IOException e) {
+            throw new RuntimeIOException("Error while loading a tagger model (probably missing model file)", e);
+        }
+    }
 
-  /** This reads the complete tagger from a single model provided as an InputStream,
-   *  and initializes the tagger using a
-   *  combination of the properties passed in and parameters from the file.
-   *  <br>
-   *  <i>Note for the future:</i> This assumes that the TaggerConfig in the file
-   *  has already been read and used.  This work is done inside the
-   *  constructor of TaggerConfig.  It might be better to refactor
-   *  things so that is all done inside this method, but for the moment
-   *  it seemed better to leave working code alone [cdm 2008].
-   *
-   *  @param config The tagger config
-   *  @param modelStream The model provided as an InputStream
-   *  @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
-   *  @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  protected void readModelAndInit(Properties config, InputStream modelStream, boolean printLoading) {
-    try {
-      // first check can open file ... or else leave with exception
-      DataInputStream rf = new DataInputStream(modelStream);
+    /**
+     * This reads the complete tagger from a single model provided as an InputStream,
+     * and initializes the tagger using a
+     * combination of the properties passed in and parameters from the file.
+     * <br>
+     * <i>Note for the future:</i> This assumes that the TaggerConfig in the file
+     * has already been read and used.  This work is done inside the
+     * constructor of TaggerConfig.  It might be better to refactor
+     * things so that is all done inside this method, but for the moment
+     * it seemed better to leave working code alone [cdm 2008].
+     *
+     * @param config       The tagger config
+     * @param modelStream  The model provided as an InputStream
+     * @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    private void readModelAndInit(Properties config, InputStream modelStream, boolean printLoading) {
+        try {
+            // first check can open file ... or else leave with exception
+            DataInputStream rf = new DataInputStream(modelStream);
 
-      readModelAndInit(config, rf, printLoading);
-      rf.close();
-    } catch (IOException e) {
-      throw new RuntimeIOException("Error while loading a tagger model (probably missing model file)", e);
-    }
-  }
+            readModelAndInit(config, rf, printLoading);
+            rf.close();
+        } catch (IOException e) {
+            throw new RuntimeIOException("Error while loading a tagger model (probably missing model file)", e);
+        }
+    }
 
 
-  /** This reads the complete tagger from a single model file, and inits
-   *  the tagger using a combination of the properties passed in and
-   *  parameters from the file.
-   *  <br>
-   *  <i>Note for the future: This assumes that the TaggerConfig in the file
-   *  has already been read and used.  It might be better to refactor
-   *  things so that is all done inside this method, but for the moment
-   *  it seemed better to leave working code alone [cdm 2008].</i>
-   *
-   *  @param config The tagger config
-   *  @param rf DataInputStream to read from.  It's the caller's job to open and close this stream.
-   *  @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
-   *  @throws RuntimeIOException if I/O errors or serialization errors
-   */
-  protected void readModelAndInit(Properties config, DataInputStream rf, boolean printLoading) {
-    try {
-      Timing t = new Timing();
-      String source = null;
-      if (printLoading) {
-        if (config != null) {
-          // TODO: "model"
-          source = config.getProperty("model");
-        }
-        if (source == null) {
-          source = "data stream";
-        }
-      }
-      TaggerConfig taggerConfig = TaggerConfig.readConfig(rf);
-      if (config != null) {
-        taggerConfig.setProperties(config);
-      }
-      // then init tagger
-      init(taggerConfig);
+    /**
+     * This reads the complete tagger from a single model file, and inits
+     * the tagger using a combination of the properties passed in and
+     * parameters from the file.
+     * <br>
+     * <i>Note for the future: This assumes that the TaggerConfig in the file
+     * has already been read and used.  It might be better to refactor
+     * things so that is all done inside this method, but for the moment
+     * it seemed better to leave working code alone [cdm 2008].</i>
+     *
+     * @param config       The tagger config
+     * @param rf           DataInputStream to read from.  It's the caller's job to open and close this stream.
+     * @param printLoading Whether to print a message saying what model file is being loaded and how long it took when finished.
+     * @throws RuntimeIOException if I/O errors or serialization errors
+     */
+    private void readModelAndInit(Properties config, DataInputStream rf, boolean printLoading) {
+        try {
+            Timing t = new Timing();
+            String source = null;
+            if (printLoading) {
+                if (config != null) {
+                    // TODO: "model"
+                    source = config.getProperty("model");
+                }
+                if (source == null) {
+                    source = "data stream";
+                }
+            }
+            TaggerConfig taggerConfig = TaggerConfig.readConfig(rf);
+            if (config != null) {
+                taggerConfig.setProperties(config);
+            }
+            // then updatePointers tagger
+            init(taggerConfig);
 
-      xSize = rf.readInt();
-      ySize = rf.readInt();
-      // dict = new Dictionary();  // this method is called in constructor, and it's initialized as empty already
-      dict.read(rf);
+            xSize = rf.readInt();
+            ySize = rf.readInt();
+            // dict = new Dictionary();  // this method is called in constructor, and it's initialized as empty already
+            dict.read(rf);
 
-      if (VERBOSE) {
-        log.info("Tagger dictionary read.");
-      }
-      tags.read(rf);
-      readExtractors(rf);
-      dict.setAmbClasses(ambClasses, veryCommonWordThresh, tags);
+            if (VERBOSE) {
+                log.info("Tagger dictionary read.");
+            }
+            tags.read(rf);
+            readExtractors(rf);
+            dict.setAmbClasses(ambClasses, veryCommonWordThresh, tags);
 
-      int[] numFA = new int[extractors.size() + extractorsRare.size()];
-      int sizeAssoc = rf.readInt();
-      fAssociations = Generics.newArrayList();
-      for (int i = 0; i < extractors.size() + extractorsRare.size(); ++i) {
-        fAssociations.add(Generics.newHashMap());
-      }
-      if (VERBOSE) log.infof("Reading %d feature keys...%n", sizeAssoc);
-      PrintFile pfVP = null;
-      if (VERBOSE) {
-        pfVP = new PrintFile("pairs.txt");
-      }
-      FeatureKey fK = new FeatureKey(); // reused in for loop but not stored. just a temp variable
-      for (int i = 0; i < sizeAssoc; i++) {
-        int numF = rf.readInt();
-        fK.read(rf);
-        numFA[fK.num]++;
-        if (VERBOSE) {
-          String eName = (fK.num < extractors.size() ? extractors.get(fK.num): extractorsRare.get(fK.num - extractors.size())).toString();
-          Map<String, int[]> valFeats = fAssociations.get(fK.num);
-          pfVP.print(eName);
-          pfVP.print(' ');
-          pfVP.print(fK);
-          pfVP.print(' ');
-          if (valFeats != null) {
-            pfVP.print(valFeats.keySet());
-          }
-          pfVP.println();
-        }
+            int[] numFA = new int[extractors.size() + extractorsRare.size()];
+            int sizeAssoc = rf.readInt();
+            fAssociations = Generics.newArrayList();
+            for (int i = 0; i < extractors.size() + extractorsRare.size(); ++i) {
+                fAssociations.add(Generics.newHashMap());
+            }
+            if (VERBOSE) log.infof("Reading %d feature keys...%n", sizeAssoc);
+            PrintFile pfVP = null;
+            if (VERBOSE) {
+                pfVP = new PrintFile("pairs.txt");
+            }
+            FeatureKey fK = new FeatureKey(); // reused in for loop but not stored. just a temp variable
+            for (int i = 0; i < sizeAssoc; i++) {
+                int numF = rf.readInt();
+                fK.read(rf);
+                numFA[fK.num]++;
+                if (VERBOSE) {
+                    String eName = (fK.num < extractors.size() ? extractors.get(fK.num) : extractorsRare.get(fK.num - extractors.size())).toString();
+                    Map<String, int[]> valFeats = fAssociations.get(fK.num);
+                    pfVP.print(eName);
+                    pfVP.print(' ');
+                    pfVP.print(fK);
+                    pfVP.print(' ');
+                    if (valFeats != null) {
+                        pfVP.print(valFeats.keySet());
+                    }
+                    pfVP.println();
+                }
 
-        // TODO: rewrite the writing / reading code to store fAssociations in a cleaner manner?
-        // Only do this when rebuilding all the tagger models anyway.  When we do that, we can get rid of FeatureKey
-        Map<String, int[]> fValueAssociations = fAssociations.get(fK.num);
-        int[] fTagAssociations = fValueAssociations.get(fK.val);
-        if (fTagAssociations == null) {
-          fTagAssociations = new int[ySize];
-          for (int j = 0; j < ySize; ++j) {
-            fTagAssociations[j] = -1;
-          }
-          fValueAssociations.put(fK.val, fTagAssociations);
-        }
-        fTagAssociations[tags.getIndex(fK.tag)] = numF;
-      }
-      if (VERBOSE) {
-        IOUtils.closeIgnoringExceptions(pfVP);
-      }
-      if (VERBOSE) {
-        for (int k = 0; k < numFA.length; k++) {
-          log.info("Number of features of kind " + k + ' ' + (k < extractors.size() ? extractors.get(k): extractorsRare.get(k - extractors.size())) +": " + numFA[k]);
-        }
-      }
-      prob = new LambdaSolveTagger(rf);
-      if (VERBOSE) {
-        log.info("prob read ");
-      }
-      if (printLoading) {
-        t.done(log, "Loading POS tagger from " + source);
-      }
-    } catch (IOException | ClassNotFoundException e) {
-      throw new RuntimeIOException("Error while loading a tagger model (probably missing model file)", e);
-    }
-  }
+                // TODO: rewrite the writing / reading code to store fAssociations in a cleaner manner?
+                // Only do this when rebuilding all the tagger models anyway.  When we do that, we can get rid of FeatureKey
+                Map<String, int[]> fValueAssociations = fAssociations.get(fK.num);
+                int[] fTagAssociations = fValueAssociations.get(fK.val);
+                if (fTagAssociations == null) {
+                    fTagAssociations = new int[ySize];
+                    for (int j = 0; j < ySize; ++j) {
+                        fTagAssociations[j] = -1;
+                    }
+                    fValueAssociations.put(fK.val, fTagAssociations);
+                }
+                fTagAssociations[tags.indexOf(fK.tag)] = numF;
+            }
+            if (VERBOSE) {
+                IOUtils.closeIgnoringExceptions(pfVP);
+            }
+            if (VERBOSE) {
+                for (int k = 0; k < numFA.length; k++) {
+                    log.info("Number of features of kind " + k + ' ' + (k < extractors.size() ? extractors.get(k) : extractorsRare.get(k - extractors.size())) + ": " + numFA[k]);
+                }
+            }
+            prob = new LambdaSolveTagger(rf);
+            if (VERBOSE) {
+                log.info("prob read ");
+            }
+            if (printLoading) {
+                t.done(log, "Loading POS tagger from " + source);
+            }
+        } catch (IOException | ClassNotFoundException e) {
+            throw new RuntimeIOException("Error while loading a tagger model (probably missing model file)", e);
+        }
+    }
 
 
-  private void dumpModel(PrintStream out) {
-    out.println("Features: template featureValue tag: lambda");
-    NumberFormat nf = new DecimalFormat(" 0.000000;-0.000000");
-    for (int i = 0; i < fAssociations.size(); ++i) {
-      Map<String, int[]> fValueAssociations = fAssociations.get(i);
-      List<String> features = Generics.newArrayList();
-      Collections.sort(features);
-      for (String featureValue : features) {
-        int[] fTagAssociations = fValueAssociations.get(featureValue);
-        for (int j = 0; j < fTagAssociations.length; ++j) {
-          int association = fTagAssociations[j];
-          if (association >= 0) {
-            FeatureKey fk = new FeatureKey(i, featureValue, tags.getTag(j));
-            out.println((fk.num < extractors.size() ? extractors.get(fk.num) : extractorsRare.get(fk.num - extractors.size()))
-                    + " " + fk.val + ' ' + fk.tag + ": " + nf.format(getLambdaSolve().lambda[association]));
-          }
-        }
-      }
-    }
-  }
+    private void dumpModel(PrintStream out) {
+        out.println("Features: template featureValue tag: lambda");
+        NumberFormat nf = new DecimalFormat(" 0.000000;-0.000000");
+        for (int i = 0; i < fAssociations.size(); ++i) {
+            Map<String, int[]> fValueAssociations = fAssociations.get(i);
+            List<String> features = Generics.newArrayList();
+            Collections.sort(features);
+            for (String featureValue : features) {
+                int[] fTagAssociations = fValueAssociations.get(featureValue);
+                for (int j = 0; j < fTagAssociations.length; ++j) {
+                    int association = fTagAssociations[j];
+                    if (association >= 0) {
+                        FeatureKey fk = new FeatureKey(i, featureValue, tags.getTag(j));
+                        out.println((fk.num < extractors.size() ? extractors.get(fk.num) : extractorsRare.get(fk.num - extractors.size()))
+                                + " " + fk.val + ' ' + fk.tag + ": " + nf.format(getLambdaSolve().lambda[association]));
+                    }
+                }
+            }
+        }
+    }
 
 
-  /* Package access so it doesn't appear in public API. */
-  boolean isRare(String word) {
-    return dict.sum(word) < rareWordThresh;
-  }
+    /* Package access so it doesn't appear in public API. */
+    boolean isRare(String word) {
+        return dict.sum(word) < rareWordThresh;
+    }
 
-  /**
-   * Tags the tokenized input string and returns the tagged version.
-   * This method requires the input to already be tokenized.
-   * The tagger wants input that is whitespace separated tokens, tokenized
-   * according to the conventions of the training data. (For instance,
-   * for the Penn Treebank, punctuation marks and possessive "'s" should
-   * be separated from words.)
-   *
-   * @param toTag The untagged input String
-   * @return The same string with tags inserted in the form word/tag
-   */
-  public String tagTokenizedString(String toTag) {
-    List<Word> sent = SentenceUtils.toUntaggedList(Arrays.asList(toTag.split("\\s+")));
-    TestSentence testSentence = new TestSentence(this);
-    testSentence.tagSentence(sent, false);
-    return testSentence.getTaggedNice();
-  }
-
 
-  /**
-   * Tags the input string and returns the tagged version.
-   * This method tokenizes the input into words in perhaps multiple sentences
-   * and then tags those sentences.  The default (PTB English)
-   * tokenizer is used.
-   *
-   * @param toTag The untagged input String
-   * @return A String of sentences with tags inserted in the form word/tag
-   */
-  public String tagString(String toTag) {
-    TaggerWrapper tw = new TaggerWrapper(this);
-    return tw.apply(toTag);
-  }
+    /**
+     * Tags the input string and returns the tagged version.
+     * This method tokenizes the input into words in perhaps multiple sentences
+     * and then tags those sentences.  The default (PTB English)
+     * tokenizer is used.
+     *
+     * @param toTag The untagged input String
+     * @return A String of sentences with tags inserted in the form word/tag
+     */
+    public String tagString(String toTag) {
+        TaggerWrapper tw = new TaggerWrapper(this);
+        return tw.apply(toTag);
+    }
 
-  /**
-   * Expects a sentence and returns a tagged sentence.
-   *
-   * @param in This needs to be a sentence (List of words)
-   * @return A sentence of TaggedWord
-   */
-  @Override
-  public List<TaggedWord> apply(List<? extends HasWord> in) {
-    TestSentence testSentence = new TestSentence(this);
-    return testSentence.tagSentence(in, false);
-  }
+    /**
+     * Expects a sentence and returns a tagged sentence.
+     *
+     * @param in This needs to be a sentence (List of words)
+     * @return A sentence of TaggedWord
+     */
+    @Override
+    public List<TaggedWord> apply(List<? extends HasWord> in) {
+        BaseTagger baseTagger = new BaseTagger(this);
+        return baseTagger.tagSentence(in, false);
+    }
 
 
-  /**
-   * Tags the Words in each Sentence in the given List with their
-   * grammatical part-of-speech. The returned List contains Sentences
-   * consisting of TaggedWords.
-   * <br><b>NOTE: </b>The input document must contain sentences as its elements,
-   * not words. To turn a Document of words into a Document of sentences, run
-   * it through {@link edu.stanford.nlp.process.WordToSentenceProcessor}.
-   *
-   * @param sentences A List of Sentence
-   * @return A List of Sentence of TaggedWord
-   */
-  @Override
-  public List<List<TaggedWord>> process(List<? extends List<? extends HasWord>> sentences) {
-    List<List<TaggedWord>> taggedSentences = Generics.newArrayList();
+    /**
+     * Tags the Words in each Sentence in the given List with their
+     * grammatical part-of-speech. The returned List contains Sentences
+     * consisting of TaggedWords.
+     * <br><b>NOTE: </b>The input document must contain sentences as its elements,
+     * not words. To turn a Document of words into a Document of sentences, run
+     * it through {@link edu.stanford.nlp.process.WordToSentenceProcessor}.
+     *
+     * @param sentences A List of Sentence
+     * @return A List of Sentence of TaggedWord
+     */
+    @Override
+    public List<List<TaggedWord>> process(List<? extends List<? extends HasWord>> sentences) {
+        List<List<TaggedWord>> taggedSentences = Generics.newArrayList();
 
-    TestSentence testSentence = new TestSentence(this);
-    for (List<? extends HasWord> sentence : sentences) {
-      taggedSentences.add(testSentence.tagSentence(sentence, false));
-    }
-    return taggedSentences;
-  }
+        BaseTagger baseTagger = new BaseTagger(this);
+        for (List<? extends HasWord> sentence : sentences) {
+            taggedSentences.add(baseTagger.tagSentence(sentence, false));
+        }
+        return taggedSentences;
+    }
 
 
-  /**
-   * Returns a new Sentence that is a copy of the given sentence with all the
-   * words tagged with their part-of-speech. Convenience method when you only
-   * want to tag a single List instead of a Document of sentences.
-   * @param sentence sentence to tag
-   * @return tagged sentence
-   */
-  public List<TaggedWord> tagSentence(List<? extends HasWord> sentence) {
-    TestSentence testSentence = new TestSentence(this);
-    return testSentence.tagSentence(sentence, false);
-  }
+    /**
+     * Returns a new Sentence that is a copy of the given sentence with all the
+     * words tagged with their part-of-speech. Convenience method when you only
+     * want to tag a single List instead of a Document of sentences.
+     *
+     * @param sentence sentence to tag
+     * @return tagged sentence
+     */
+    public List<TaggedWord> tagSentence(List<? extends HasWord> sentence) {
+        BaseTagger baseTagger = new BaseTagger(this);
+        return baseTagger.tagSentence(sentence, false);
+    }
 
-  /**
-   * Returns a new Sentence that is a copy of the given sentence with all the
-   * words tagged with their part-of-speech. Convenience method when you only
-   * want to tag a single List instead of a List of Lists.  If you
-   * supply tagSentence with a List of HasTag, and set reuseTags to
-   * true, the tagger will reuse the supplied tags.
-   *
-   * @param sentence sentence to tag
-   * @param reuseTags whether or not to reuse the given tag
-   * @return tagged sentence
-   */
-  public List<TaggedWord> tagSentence(List<? extends HasWord> sentence,
-                                           boolean reuseTags) {
-    TestSentence testSentence = new TestSentence(this);
-    return testSentence.tagSentence(sentence, reuseTags);
-  }
+    /**
+     * Returns a new Sentence that is a copy of the given sentence with all the
+     * words tagged with their part-of-speech. Convenience method when you only
+     * want to tag a single List instead of a List of Lists.  If you
+     * supply tagSentence with a List of HasTag, and set reuseTags to
+     * true, the tagger will reuse the supplied tags.
+     *
+     * @param sentence  sentence to tag
+     * @param reuseTags whether or not to reuse the given tag
+     * @return tagged sentence
+     */
+    public List<TaggedWord> tagSentence(List<? extends HasWord> sentence,
+                                        boolean reuseTags) {
+        BaseTagger baseTagger = new BaseTagger(this);
+        return baseTagger.tagSentence(sentence, reuseTags);
+    }
 
-  /**
-   * Takes a sentence composed of CoreLabels and add the tags to the
-   * CoreLabels, modifying the input sentence.
-   */
-  public void tagCoreLabels(List<CoreLabel> sentence) {
-    tagCoreLabels(sentence, false);
-  }
+    /**
+     * Takes a sentence composed of CoreLabels and add the tags to the
+     * CoreLabels, modifying the input sentence.
+     */
+    public void tagCoreLabels(List<CoreLabel> sentence) {
+        tagCoreLabels(sentence, false);
+    }
 
-  /**
-   * Takes a sentence composed of CoreLabels and add the tags to the
-   * CoreLabels, modifying the input sentence.  If reuseTags is set to
-   * true, any tags supplied with the CoreLabels are taken as correct.
-   */
-  public void tagCoreLabels(List<CoreLabel> sentence,
-                            boolean reuseTags) {
-    List<TaggedWord> taggedWords = tagSentence(sentence, reuseTags);
-    if (taggedWords.size() != sentence.size())
-      throw new AssertionError("Tagged word list not the same length " +
-                               "as the original sentence");
-    for (int i = 0, size = sentence.size(); i < size; ++i) {
-      sentence.get(i).setTag(taggedWords.get(i).tag());
-    }
-  }
+    /**
+     * Takes a sentence composed of CoreLabels and add the tags to the
+     * CoreLabels, modifying the input sentence.  If reuseTags is set to
+     * true, any tags supplied with the CoreLabels are taken as correct.
+     */
+    public void tagCoreLabels(List<CoreLabel> sentence,
+                              boolean reuseTags) {
+        List<TaggedWord> taggedWords = tagSentence(sentence, reuseTags);
+        if (taggedWords.size() != sentence.size())
+            throw new AssertionError("Tagged word list not the same length " +
+                    "as the original sentence");
+        for (int i = 0, size = sentence.size(); i < size; ++i) {
+            sentence.get(i).setTag(taggedWords.get(i).tag());
+        }
+    }
 
-  /**
-   * Adds lemmas to the given list of CoreLabels, using the given
-   * Morphology object.  The input list must already have tags set.
-   */
-  public static void lemmatize(List<CoreLabel> sentence,
-                               Morphology morpha) {
-    for (CoreLabel label : sentence) {
-      morpha.stem(label);
-    }
-  }
+    /**
+     * Adds lemmas to the given list of CoreLabels, using the given
+     * Morphology object.  The input list must already have tags set.
+     */
+    public static void lemmatize(List<CoreLabel> sentence,
+                                 Morphology morpha) {
+        for (CoreLabel label : sentence) {
+            morpha.stem(label);
+        }
+    }
 
-  /**
-   * Casts a list of HasWords, which we secretly know to be
-   * CoreLabels, to a list of CoreLabels.  Barfs if you didn't
-   * actually give it CoreLabels.
-   */
-  private static List<CoreLabel> castCoreLabels(List<? extends HasWord> sent) {
-    List<CoreLabel> coreLabels = Generics.newArrayList();
-    for (HasWord word : sent) {
-      if (!(word instanceof CoreLabel)) {
-        throw new ClassCastException("Expected CoreLabels");
-      }
-      coreLabels.add((CoreLabel) word);
-    }
-    return coreLabels;
-  }
+    /**
+     * Casts a list of HasWords, which we secretly know to be
+     * CoreLabels, to a list of CoreLabels.  Barfs if you didn't
+     * actually give it CoreLabels.
+     */
+    private static List<CoreLabel> castCoreLabels(List<? extends HasWord> sent) {
+        List<CoreLabel> coreLabels = Generics.newArrayList();
+        for (HasWord word : sent) {
+            if (!(word instanceof CoreLabel)) {
+                throw new ClassCastException("Expected CoreLabels");
+            }
+            coreLabels.add((CoreLabel) word);
+        }
+        return coreLabels;
+    }
 
-  /**
-   * Reads data from r, tokenizes it with the default (Penn Treebank)
-   * tokenizer, and returns a List of Sentence objects, which can
-   * then be fed into tagSentence.
-   *
-   * @param r Reader where untokenized text is read
-   * @return List of tokenized sentences
-   */
-  public static List<List<HasWord>> tokenizeText(Reader r) {
-    return tokenizeText(r, null);
-  }
+    /**
+     * Reads data from r, tokenizes it with the default (Penn Treebank)
+     * tokenizer, and returns a List of Sentence objects, which can
+     * then be fed into tagSentence.
+     *
+     * @param r Reader where untokenized text is read
+     * @return List of tokenized sentences
+     */
+    public static List<List<HasWord>> tokenizeText(Reader r) {
+        return tokenizeText(r, null);
+    }
 
 
-  /**
-   * Reads data from r, tokenizes it with the given tokenizer, and
-   * returns a List of Lists of (extends) HasWord objects, which can then be
-   * fed into tagSentence.
-   *
-   * @param r Reader where untokenized text is read
-   * @param tokenizerFactory Tokenizer.  This can be {@code null} in which case
-   *     the default English tokenizer (PTBTokenizerFactory) is used.
-   * @return List of tokenized sentences
-   */
-  public static List<List<HasWord>> tokenizeText(Reader r,
-                 TokenizerFactory<? extends HasWord> tokenizerFactory) {
-    DocumentPreprocessor documentPreprocessor = new DocumentPreprocessor(r);
-    if (tokenizerFactory != null) {
-      documentPreprocessor.setTokenizerFactory(tokenizerFactory);
-    }
-    List<List<HasWord>> out = Generics.newArrayList();
-    for (List<HasWord> item : documentPreprocessor) {
-      out.add(item);
-    }
-    return out;
-  }
+    /**
+     * Reads data from r, tokenizes it with the given tokenizer, and
+     * returns a List of Lists of (extends) HasWord objects, which can then be
+     * fed into tagSentence.
+     *
+     * @param r                Reader where untokenized text is read
+     * @param tokenizerFactory Tokenizer.  This can be {@code null} in which case
+     *                         the default English tokenizer (PTBTokenizerFactory) is used.
+     * @return List of tokenized sentences
+     */
+    public static List<List<HasWord>> tokenizeText(Reader r,
+                                                   TokenizerFactory<? extends HasWord> tokenizerFactory) {
+        DocumentPreprocessor documentPreprocessor = new DocumentPreprocessor(r);
+        if (tokenizerFactory != null) {
+            documentPreprocessor.setTokenizerFactory(tokenizerFactory);
+        }
+        List<List<HasWord>> out = Generics.newArrayList();
+        for (List<HasWord> item : documentPreprocessor) {
+            out.add(item);
+        }
+        return out;
+    }
 
 
-  private static void dumpModel(TaggerConfig config) {
-    try {
-      MaxentTagger tagger = new MaxentTagger(config.getModel(), config, false);
-      System.out.println("Serialized tagger built with config:");
-      tagger.config.dump(System.out);
-      tagger.dumpModel(System.out);
-    } catch (Exception e) {
-      log.err(e);
-    }
-  }
+    private static void dumpModel(TaggerConfig config) {
+        try {
+            MaxentTagger tagger = new MaxentTagger(config.getModel(), config, false);
+            System.out.println("Serialized tagger built with config:");
+            tagger.config.dump(System.out);
+            tagger.dumpModel(System.out);
+        } catch (Exception e) {
+            log.err(e);
+        }
+    }
 
 
-  /**
-   * Tests a tagger on data with gold tags available.  This is TEST mode.
-   *
-   * @param config Properties giving parameters for the testing run
-   */
-  private static void runTest(TaggerConfig config) {
-    if (config.getVerbose()) {
-      log.info("Tagger testing invoked at " + new Date() + " with arguments:");
-      config.dump();
-    }
+    /**
+     * Tests a tagger on data with gold tags available.  This is TEST mode.
+     *
+     * @param config Properties giving parameters for the testing run
+     */
+    private static void runTest(TaggerConfig config) {
+        if (config.getVerbose()) {
+            log.info("Tagger testing invoked at " + new Date() + " with arguments:");
+            config.dump();
+        }
 
-    try {
-      MaxentTagger tagger = new MaxentTagger(config.getModel(), config);
+        try {
+            MaxentTagger tagger = new MaxentTagger(config.getModel(), config);
 
-      Timing t = new Timing();
-      TestClassifier testClassifier = new TestClassifier(tagger);
-      long millis = t.stop();
-      printErrWordsPerSec(millis, testClassifier.getNumWords());
-      testClassifier.printModelAndAccuracy(tagger);
-    } catch (Exception e) {
-      log.warn("An error occurred while testing the tagger.", e);
-    }
-  }
+            Timing t = new Timing();
+            TestClassifier testClassifier = new TestClassifier(tagger);
+            testClassifier.test();
+            long millis = t.stop();
+            printErrWordsPerSec(millis, testClassifier.getNumWords());
+            testClassifier.printModelAndAccuracy(tagger);
+        } catch (Exception e) {
+            log.warn("An error occurred while testing the tagger.", e);
+        }
+    }
 
 
-  /**
-   * Reads in the training corpus from a filename and trains the tagger
-   *
-   * @param config Configuration parameters for training a model (filename, etc.
-   * @throws IOException If IO problem
-   */
-  private static void trainAndSaveModel(TaggerConfig config) throws IOException {
+    /**
+     * Reads in the training corpus from a filename and trains the tagger
+     *
+     * @param config Configuration parameters for training a model (filename, etc.
+     * @throws IOException If IO problem
+     */
+    private static void trainAndSaveModel(TaggerConfig config) throws IOException {
 
-    String modelName = config.getModel();
-    MaxentTagger maxentTagger = new MaxentTagger();
-    maxentTagger.init(config);
+        String modelName = config.getModel();
+        MaxentTagger maxentTagger = new MaxentTagger();
+        maxentTagger.init(config);
 
-    // Allow clobbering.  You want it all the time when running experiments.
+        // Allow clobbering.  You want it all the time when running experiments.
 
-    TaggerExperiments samples = new TaggerExperiments(config, maxentTagger);
-    TaggerFeatures feats = samples.getTaggerFeatures();
-    byte[][] fnumArr = samples.getFnumArr();
-    log.info("Samples from " + config.getFile());
-    log.info("Number of features: " + feats.size());
-    log.info("Tag set: " + maxentTagger.tags.tagSet());
-    Problem p = new Problem(samples, feats);
-    LambdaSolveTagger prob = new LambdaSolveTagger(p, 0.0001, fnumArr);
-    maxentTagger.prob = prob;
+        TaggerExperiments samples = new TaggerExperiments(config, maxentTagger);
+        TaggerFeatures feats = samples.getTaggerFeatures();
+        byte[][] fnumArr = samples.getFnumArr();
+        log.info("Samples from " + config.getFile());
+        log.info("Number of features: " + feats.size());
+        log.info("Tag set: " + maxentTagger.tags.tagSet());
+        Problem p = new Problem(samples, feats);
+        LambdaSolveTagger prob = new LambdaSolveTagger(p, 0.0001, fnumArr);
+        maxentTagger.prob = prob;
 
-    if (config.getSearch().equals("owlqn")) {
-      CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
-      runner.solveL1(config.getRegL1());
-    } else if (config.getSearch().equals("owlqn2")) {
-      CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
-      runner.solveOWLQN2(config.getRegL1());
-    } else if (config.getSearch().equals("cg")) {
-      CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
-      runner.solveCG();
-    } else if (config.getSearch().equals("qn")) {
-      CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
-      runner.solveQN();
-    } else {
-      prob.improvedIterative(config.getIterations());
-    }
+        if (config.getSearch().equals("owlqn")) {
+            CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
+            runner.solveL1(config.getRegL1());
+        } else if (config.getSearch().equals("owlqn2")) {
+            CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
+            runner.solveOWLQN2(config.getRegL1());
+        } else if (config.getSearch().equals("cg")) {
+            CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
+            runner.solveCG();
+        } else if (config.getSearch().equals("qn")) {
+            CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());
+            runner.solveQN();
+        } else {
+            prob.improvedIterative(config.getIterations());
+        }
 
-    if (prob.checkCorrectness()) {
-      log.info("Model is correct [empirical expec = model expec]");
-    } else {
-      log.info("Model is not correct");
-    }
+        if (prob.checkCorrectness()) {
+            log.info("Model is correct [empirical expec = model expec]");
+        } else {
+            log.info("Model is not correct");
+        }
 
-    // Some of the rules may have been optimized so they don't have
-    // any effect on the final scores.  Eliminating those rules
-    // entirely saves space and runtime
-    maxentTagger.removeDeadRules();
+        // Some of the rules may have been optimized so they don't have
+        // any effect on the final scores.  Eliminating those rules
+        // entirely saves space and runtime
+        maxentTagger.removeDeadRules();
 
-    // If any of the features have been optimized to 0, we can remove
-    // them from the LambdaSolve.  This will save quite a bit of space
-    // depending on the optimization used
-    maxentTagger.simplifyLambda();
+        // If any of the features have been optimized to 0, we can remove
+        // them from the LambdaSolve.  This will save quite a bit of space
+        // depending on the optimization used
+        maxentTagger.simplifyLambda();
 
-    maxentTagger.saveModel(modelName);
-    log.info("Extractors list:");
-    log.info(maxentTagger.extractors.toString() + "\nrare" + maxentTagger.extractorsRare.toString());
-  }
+        maxentTagger.saveModel(modelName);
+        log.info("Extractors list:");
+        log.info(maxentTagger.extractors.toString() + "\nrare" + maxentTagger.extractorsRare.toString());
+    }
 
 
-  /**
-   * Trains a tagger model.
-   *
-   * @param config Properties giving parameters for the training run
-   */
-  private static void runTraining(TaggerConfig config)
-    throws IOException
-  {
-    Date now = new Date();
+    /**
+     * Trains a tagger model.
+     *
+     * @param config Properties giving parameters for the training run
+     */
+    private static void runTraining(TaggerConfig config)
+            throws IOException {
+        Date now = new Date();
 
-    log.info("## tagger training invoked at " + now + " with arguments:");
-    config.dump();
-    Timing tim = new Timing();
+        log.info("## tagger training invoked at " + now + " with arguments:");
+        config.dump();
+        Timing tim = new Timing();
 
-    PrintFile log = new PrintFile(config.getModel() + ".props");
-    log.println("## tagger training invoked at " + now + " with arguments:");
-    config.dump(log);
-    log.close();
+        PrintFile log = new PrintFile(config.getModel() + ".props");
+        log.println("## tagger training invoked at " + now + " with arguments:");
+        config.dump(log);
+        log.close();
 
-    trainAndSaveModel(config);
-    tim.done("Training POS tagger");
-  }
+        trainAndSaveModel(config);
+        tim.done("Training POS tagger");
+    }
 
 
-  private static void printErrWordsPerSec(long milliSec, int numWords) {
-    double wordsPerSec = numWords / (((double) milliSec) / 1000);
-    NumberFormat nf = new DecimalFormat("0.00");
-    log.info("Tagged " + numWords + " words at " +
-        nf.format(wordsPerSec) + " words per second.");
-  }
+    private static void printErrWordsPerSec(long milliSec, int numWords) {
+        double wordsPerSec = numWords / (((double) milliSec) / 1000);
+        NumberFormat nf = new DecimalFormat("0.00");
+        log.info("Tagged " + numWords + " words at " +
+                nf.format(wordsPerSec) + " words per second.");
+    }
 
 
-  // not so much a wrapper as a class with some various functionality
-  // extending the MaxentTagger...
-  // TODO: can we get rid of this? [cdm: sure. I'm not quite sure why Anna added it.  It seems like it could just be inside MaxentTagger]
-  static class TaggerWrapper implements Function<String, String> {
+    // not so much a wrapper as a class with some various functionality
+    // extending the MaxentTagger...
+    // TODO: can we get rid of this? [cdm: sure. I'm not quite sure why Anna added it.  It seems like it could just be inside MaxentTagger]
+    static class TaggerWrapper implements Function<String, String> {
 
-    private final TaggerConfig config;
-    private final MaxentTagger tagger;
-    private TokenizerFactory<? extends HasWord> tokenizerFactory;
-    private int sentNum; // = 0;
+        private final TaggerConfig config;
+        private final MaxentTagger tagger;
+        private TokenizerFactory<? extends HasWord> tokenizerFactory;
+        private int sentNum; // = 0;
 
-    private final boolean tokenize;
-    private final boolean outputVerbosity, outputLemmas;
-    private final OutputStyle outputStyle;
-    // private final String tagSeparator;
-    private final Morphology morpha;
+        private final boolean tokenize;
+        private final boolean outputVerbosity, outputLemmas;
+        private final OutputStyle outputStyle;
+        // private final String tagSeparator;
+        private final Morphology morpha;
 
-    protected TaggerWrapper(MaxentTagger tagger) {
-      this.tagger = tagger;
-      this.config = tagger.config;
+        protected TaggerWrapper(MaxentTagger tagger) {
+            this.tagger = tagger;
+            this.config = tagger.config;
 
-      try {
-        tokenizerFactory =
-          chooseTokenizerFactory(config.getTokenize(),
-                                 config.getTokenizerFactory(),
-                                 config.getTokenizerOptions(),
-                                 config.getTokenizerInvertible());
-      } catch (Exception e) {
-        log.info("Error in tokenizer factory instantiation for class: " + config.getTokenizerFactory());
-        e.printStackTrace();
-        tokenizerFactory = PTBTokenizerFactory.newWordTokenizerFactory(config.getTokenizerOptions());
-      }
+            try {
+                tokenizerFactory =
+                        chooseTokenizerFactory(config.getTokenize(),
+                                config.getTokenizerFactory(),
+                                config.getTokenizerOptions(),
+                                config.getTokenizerInvertible());
+            } catch (Exception e) {
+                log.info("Error in tokenizer factory instantiation for class: " + config.getTokenizerFactory());
+                e.printStackTrace();
+                tokenizerFactory = PTBTokenizerFactory.newWordTokenizerFactory(config.getTokenizerOptions());
+            }
 
-      outputStyle = OutputStyle.fromShortName(config.getOutputFormat());
-      outputVerbosity = config.getOutputVerbosity();
-      outputLemmas = config.getOutputLemmas();
-      morpha = (outputLemmas) ? new Morphology() : null;
-      tokenize = config.getTokenize();
-      // tagSeparator = config.getTagSeparator();
-    }
+            outputStyle = OutputStyle.fromShortName(config.getOutputFormat());
+            outputVerbosity = config.getOutputVerbosity();
+            outputLemmas = config.getOutputLemmas();
+            morpha = (outputLemmas) ? new Morphology() : null;
+            tokenize = config.getTokenize();
+            // tagSeparator = config.getTagSeparator();
+        }
 
-    @Override
-    public String apply(String o) {
-      StringWriter taggedResults = new StringWriter();
+        @Override
+        public String apply(String o) {
+            StringWriter taggedResults = new StringWriter();
 
-      List<List<HasWord>> sentences;
-      if (tokenize) {
-        sentences = tokenizeText(new StringReader(o), tokenizerFactory);
-      } else {
-        sentences = Generics.newArrayList();
-        sentences.add(SentenceUtils.toWordList(o.split("\\s+")));
-      }
+            List<List<HasWord>> sentences;
+            if (tokenize) {
+                sentences = tokenizeText(new StringReader(o), tokenizerFactory);
+            } else {
+                sentences = Generics.newArrayList();
+                sentences.add(SentenceUtils.toWordList(o.split("\\s+")));
+            }
 
-      // TODO: there is another almost identical block of code elsewhere.  Refactor
-      if (config.getNThreads() != 1) {
-        MulticoreWrapper<List<? extends HasWord>, List<? extends HasWord>> wrapper = new MulticoreWrapper<>(config.getNThreads(), new SentenceTaggingProcessor(tagger, outputLemmas));
-        for (List<? extends HasWord> sentence : sentences) {
-          wrapper.put(sentence);
-          while (wrapper.peek()) {
-            List<? extends HasWord> taggedSentence = wrapper.poll();
-            tagger.outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, sentNum++, " ", taggedResults);
-          }
-        }
-        wrapper.join();
-        while (wrapper.peek()) {
-          List<? extends HasWord> taggedSentence = wrapper.poll();
-          tagger.outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, sentNum++, " ", taggedResults);
-        }
-      } else {
-        // there is only one thread
-        for (List<? extends HasWord> sent : sentences) {
-          // Morphology morpha = (outputLemmas) ? new Morphology() : null;
-          sent = tagger.tagCoreLabelsOrHasWords(sent, morpha, outputLemmas);
-          tagger.outputTaggedSentence(sent, outputLemmas, outputStyle, outputVerbosity, sentNum++, " ", taggedResults);
-        }
-      }
-      return taggedResults.toString();
-    }
+            // TODO: there is another almost identical block of code elsewhere.  Refactor
+            if (config.getNThreads() != 1) {
+                MulticoreWrapper<List<? extends HasWord>, List<? extends HasWord>> wrapper = new MulticoreWrapper<>(config.getNThreads(), new SentenceTaggingProcessor(tagger, outputLemmas));
+                for (List<? extends HasWord> sentence : sentences) {
+                    wrapper.put(sentence);
+                    while (wrapper.peek()) {
+                        List<? extends HasWord> taggedSentence = wrapper.poll();
+                        tagger.outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, sentNum++, " ", taggedResults);
+                    }
+                }
+                wrapper.join();
+                while (wrapper.peek()) {
+                    List<? extends HasWord> taggedSentence = wrapper.poll();
+                    tagger.outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, sentNum++, " ", taggedResults);
+                }
+            } else {
+                // there is only one thread
+                for (List<? extends HasWord> sent : sentences) {
+                    // Morphology morpha = (outputLemmas) ? new Morphology() : null;
+                    sent = tagger.tagCoreLabelsOrHasWords(sent, morpha, outputLemmas);
+                    tagger.outputTaggedSentence(sent, outputLemmas, outputStyle, outputVerbosity, sentNum++, " ", taggedResults);
+                }
+            }
+            return taggedResults.toString();
+        }
 
-  } // end class TaggerWrapper
+    } // end class TaggerWrapper
 
-  private static String getXMLWords(List<? extends HasWord> sentence,
-                                    int sentNum, boolean outputLemmas) {
-    boolean hasCoreLabels = (sentence != null &&
-                             sentence.size() > 0 &&
-                             sentence.get(0) instanceof CoreLabel);
-    StringBuilder sb = new StringBuilder();
-    sb.append("<sentence id=\"").append(sentNum).append("\">\n");
-    int wordIndex = 0;
-    for (HasWord hw : sentence) {
-      String word = hw.word();
-      if (!(hw instanceof HasTag)) {
-        throw new IllegalArgumentException("Expected HasTags, got " +
-                                           hw.getClass());
-      }
-      String tag = ((HasTag) hw).tag();
-      sb.append("  <word wid=\"").append(wordIndex).append("\" pos=\"").append(XMLUtils.escapeAttributeXML(tag)).append("\"");
-      if (outputLemmas && hasCoreLabels) {
-        if (!(hw instanceof CoreLabel)) {
-          throw new IllegalArgumentException("You mixed CoreLabels with " +
-                                             hw.getClass() + "?  " +
-                                             "Why would you do that?");
-        }
-        CoreLabel label = (CoreLabel) hw;
-        String lemma = label.lemma();
-        if (lemma != null) {
-          sb.append(" lemma=\"").append(XMLUtils.escapeElementXML(lemma)).append('\"');
-        }
-      }
-      sb.append(">").append(XMLUtils.escapeElementXML(word)).append("</word>\n");
-      ++wordIndex;
-    }
-    sb.append("</sentence>\n");
-    return sb.toString();
-  }
+    private static String getXMLWords(List<? extends HasWord> sentence,
+                                      int sentNum, boolean outputLemmas) {
+        boolean hasCoreLabels = (sentence != null &&
+                sentence.size() > 0 &&
+                sentence.get(0) instanceof CoreLabel);
+        StringBuilder sb = new StringBuilder();
+        sb.append("<sentence id=\"").append(sentNum).append("\">\n");
+        int wordIndex = 0;
+        for (HasWord hw : sentence) {
+            String word = hw.word();
+            if (!(hw instanceof HasTag)) {
+                throw new IllegalArgumentException("Expected HasTags, got " +
+                        hw.getClass());
+            }
+            String tag = ((HasTag) hw).tag();
+            sb.append("  <word wid=\"").append(wordIndex).append("\" pos=\"").append(XMLUtils.escapeAttributeXML(tag)).append("\"");
+            if (outputLemmas && hasCoreLabels) {
+                if (!(hw instanceof CoreLabel)) {
+                    throw new IllegalArgumentException("You mixed CoreLabels with " +
+                            hw.getClass() + "?  " +
+                            "Why would you do that?");
+                }
+                CoreLabel label = (CoreLabel) hw;
+                String lemma = label.lemma();
+                if (lemma != null) {
+                    sb.append(" lemma=\"").append(XMLUtils.escapeElementXML(lemma)).append('\"');
+                }
+            }
+            sb.append(">").append(XMLUtils.escapeElementXML(word)).append("</word>\n");
+            ++wordIndex;
+        }
+        sb.append("</sentence>\n");
+        return sb.toString();
+    }
 
-  private static String getTsvWords(boolean verbose, boolean outputLemmas,
-                                    List<? extends HasWord> sentence) {
-    StringBuilder sb = new StringBuilder();
-    if (verbose && sentence.size() > 0 &&
-        sentence.get(0) instanceof CoreLabel) {
-      for (HasWord hw : sentence) {
-        if (!(hw instanceof CoreLabel)) {
-          throw new IllegalArgumentException("You mixed CoreLabels with " +
-                                             hw.getClass() + "?  " +
-                                             "Why would you do that?");
-        }
-        CoreLabel label = (CoreLabel) hw;
-        sb.append(label.word());
-        sb.append("\t");
-        sb.append(label.originalText());
-        sb.append("\t");
-        if (outputLemmas) {
-          sb.append(label.lemma());
-          sb.append("\t");
-        }
-        sb.append(label.tag());
-        sb.append("\t");
-        sb.append(label.beginPosition());
-        sb.append("\t");
-        sb.append(label.endPosition());
-        sb.append("\n");
-      }
-      sb.append('\n');
-      return sb.toString();
-    } // otherwise, fall through
+    private static String getTsvWords(boolean verbose, boolean outputLemmas,
+                                      List<? extends HasWord> sentence) {
+        StringBuilder sb = new StringBuilder();
+        if (verbose && sentence.size() > 0 &&
+                sentence.get(0) instanceof CoreLabel) {
+            for (HasWord hw : sentence) {
+                if (!(hw instanceof CoreLabel)) {
+                    throw new IllegalArgumentException("You mixed CoreLabels with " +
+                            hw.getClass() + "?  " +
+                            "Why would you do that?");
+                }
+                CoreLabel label = (CoreLabel) hw;
+                sb.append(label.word());
+                sb.append("\t");
+                sb.append(label.originalText());
+                sb.append("\t");
+                if (outputLemmas) {
+                    sb.append(label.lemma());
+                    sb.append("\t");
+                }
+                sb.append(label.tag());
+                sb.append("\t");
+                sb.append(label.beginPosition());
+                sb.append("\t");
+                sb.append(label.endPosition());
+                sb.append("\n");
+            }
+            sb.append('\n');
+            return sb.toString();
+        } // otherwise, fall through
 
-    // either not verbose, or not CoreLabels
-    for (HasWord hw : sentence) {
-      String word = hw.word();
-      if (!(hw instanceof HasTag)) {
-        throw new IllegalArgumentException("Expected HasTags, got " +
-                                           hw.getClass());
-      }
-      String tag = ((HasTag) hw).tag();
-      sb.append(word).append('\t').append(tag).append('\n');
-    }
-    sb.append('\n');
-    return sb.toString();
-  }
+        // either not verbose, or not CoreLabels
+        for (HasWord hw : sentence) {
+            String word = hw.word();
+            if (!(hw instanceof HasTag)) {
+                throw new IllegalArgumentException("Expected HasTags, got " +
+                        hw.getClass());
+            }
+            String tag = ((HasTag) hw).tag();
+            sb.append(word).append('\t').append(tag).append('\n');
+        }
+        sb.append('\n');
+        return sb.toString();
+    }
 
-  /**
-   * Takes a tagged sentence and writes out the xml version.
-   *
-   * @param w Where to write the output to
-   * @param sent A tagged sentence
-   * @param sentNum The sentence index for XML printout
-   * @param outputLemmas Whether to write the lemmas of words
-   */
-  private static void writeXMLSentence(Writer w, List<? extends HasWord> sent,
-                                       int sentNum, boolean outputLemmas) {
-    try {
-      w.write(getXMLWords(sent, sentNum, outputLemmas));
-    } catch (IOException e) {
-      log.info("Error writing sentence " + sentNum + ": " +
-                         SentenceUtils.listToString(sent));
-      throw new RuntimeIOException(e);
-    }
-  }
+    /**
+     * Takes a tagged sentence and writes out the xml version.
+     *
+     * @param w            Where to write the output to
+     * @param sent         A tagged sentence
+     * @param sentNum      The sentence index for XML printout
+     * @param outputLemmas Whether to write the lemmas of words
+     */
+    private static void writeXMLSentence(Writer w, List<? extends HasWord> sent,
+                                         int sentNum, boolean outputLemmas) {
+        try {
+            w.write(getXMLWords(sent, sentNum, outputLemmas));
+        } catch (IOException e) {
+            log.info("Error writing sentence " + sentNum + ": " +
+                    SentenceUtils.listToString(sent));
+            throw new RuntimeIOException(e);
+        }
+    }
 
-  /**
-   * Uses an XML transformer to turn an input stream into a bunch of
-   * output.  Tags all of the text between xmlTags.
-   *
-   * The difference between using this and using runTagger in XML mode
-   * is that this preserves the XML structure outside of the list of
-   * elements to tag, whereas the runTagger method throws away all of
-   * the surrounding structure and returns tagged plain text.
-   */
-  public void tagFromXML(InputStream input, Writer writer, String... xmlTags) {
-    OutputStyle outputStyle =
-      OutputStyle.fromShortName(config.getOutputFormat());
+
+    private void tagFromXML(Reader input, Writer writer, String... xmlTags) {
+        OutputStyle outputStyle =
+                OutputStyle.fromShortName(config.getOutputFormat());
 
-    TransformXML<String> txml = new TransformXML<>();
-    switch(outputStyle) {
-    case XML:
-    case INLINE_XML:
-      txml.transformXML(xmlTags, new TaggerWrapper(this),
+        TransformXML<String> txml = new TransformXML<>();
+        switch (outputStyle) {
+            case XML:
+            case INLINE_XML:
+                txml.transformXML(xmlTags, new TaggerWrapper(this),
                         input, writer,
                         new TransformXML.NoEscapingSAXInterface<>());
-      break;
-    case SLASH_TAGS:
-    case TSV:
-      txml.transformXML(xmlTags, new TaggerWrapper(this),
+                break;
+            case SLASH_TAGS:
+            case TSV:
+                txml.transformXML(xmlTags, new TaggerWrapper(this),
                         input, writer,
                         new TransformXML.SAXInterface<>());
-      break;
-    default:
-      throw new RuntimeException("Unexpected format " + outputStyle);
-    }
-  }
+                break;
+            default:
+                throw new RuntimeException("Unexpected format " + outputStyle);
+        }
+    }
 
-  public void tagFromXML(Reader input, Writer writer, String... xmlTags) {
-    OutputStyle outputStyle =
-      OutputStyle.fromShortName(config.getOutputFormat());
-
-    TransformXML<String> txml = new TransformXML<>();
-    switch(outputStyle) {
-    case XML:
-    case INLINE_XML:
-      txml.transformXML(xmlTags, new TaggerWrapper(this),
-                        input, writer,
-                        new TransformXML.NoEscapingSAXInterface<>());
-      break;
-    case SLASH_TAGS:
-    case TSV:
-      txml.transformXML(xmlTags, new TaggerWrapper(this),
-                        input, writer,
-                        new TransformXML.SAXInterface<>());
-      break;
-    default:
-      throw new RuntimeException("Unexpected format " + outputStyle);
-    }
-  }
-
-  private void tagFromXML() {
-    Reader reader = null;
-    Writer w = null;
-    try {
-      // todo [cdm dec 13]: change to use the IOUtils read-from-anywhere routines
-      reader = new BufferedReader(new InputStreamReader(new FileInputStream(config.getFile()), config.getEncoding()));
+    private void tagFromXML() {
+        Reader reader = null;
+        Writer w = null;
+        try {
+            // todo [cdm dec 13]: change to use the IOUtils read-from-anywhere routines
+            reader = new BufferedReader(new InputStreamReader(new FileInputStream(config.getFile()), config.getEncoding()));
 
-      String outFile = config.getOutputFile();
-      if (outFile.length() > 0) {
-        w = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(outFile),
-                                                      config.getEncoding()));
-      } else {
-        w = new BufferedWriter(new OutputStreamWriter(System.out, config.getEncoding()));
-      }
-      w.write("<?xml version=\"1.0\" encoding=\"" +
-              config.getEncoding() + "\"?>\n");
-      tagFromXML(reader, w, config.getXMLInput());
-    } catch (FileNotFoundException e) {
-      log.info("Input file not found: " + config.getFile());
-      e.printStackTrace();
-    } catch (IOException ioe) {
-      log.info("tagFromXML: mysterious IO Exception");
-      ioe.printStackTrace();
-    } finally {
-      IOUtils.closeIgnoringExceptions(reader);
-      IOUtils.closeIgnoringExceptions(w);
-    }
-  }
+            String outFile = config.getOutputFile();
+            if (outFile.length() > 0) {
+                w = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(outFile),
+                        config.getEncoding()));
+            } else {
+                w = new BufferedWriter(new OutputStreamWriter(System.out, config.getEncoding()));
+            }
+            w.write("<?xml version=\"1.0\" encoding=\"" +
+                    config.getEncoding() + "\"?>\n");
+            tagFromXML(reader, w, config.getXMLInput());
+        } catch (FileNotFoundException e) {
+            log.info("Input file not found: " + config.getFile());
+            e.printStackTrace();
+        } catch (IOException ioe) {
+            log.info("tagFromXML: mysterious IO Exception");
+            ioe.printStackTrace();
+        } finally {
+            IOUtils.closeIgnoringExceptions(reader);
+            IOUtils.closeIgnoringExceptions(w);
+        }
+    }
 
-  /**
-   * Loads the tagger from a config file and then runs it in TAG mode.
-   *
-   * @param config The configuration parameters for the run.
-   */
-  private static void runTagger(TaggerConfig config)
-    throws IOException, ClassNotFoundException,
-           NoSuchMethodException, IllegalAccessException,
-           java.lang.reflect.InvocationTargetException
-  {
-    if (config.getVerbose()) {
-      Date now = new Date();
-      log.info("## tagger invoked at " + now + " with arguments:");
-      config.dump();
-    }
-    MaxentTagger tagger = new MaxentTagger(config.getModel(), config);
-    tagger.runTagger();
-  }
+    /**
+     * Loads the tagger from a config file and then runs it in TAG mode.
+     *
+     * @param config The configuration parameters for the run.
+     */
+    private static void runTagger(TaggerConfig config)
+            throws IOException, ClassNotFoundException,
+            NoSuchMethodException, IllegalAccessException,
+            java.lang.reflect.InvocationTargetException {
+        if (config.getVerbose()) {
+            Date now = new Date();
+            log.info("## tagger invoked at " + now + " with arguments:");
+            config.dump();
+        }
+        MaxentTagger tagger = new MaxentTagger(config.getModel(), config);
+        tagger.runTagger();
+    }
 
-  private static final Pattern formatPattern = Pattern.compile("format=[a-zA-Z]+,");
+    private static final Pattern formatPattern = Pattern.compile("format=[a-zA-Z]+,");
 
-  /**
-   * Runs the tagger when we're in TAG mode.
-   * In this mode, the config contains either the name of the file to
-   * tag or stdin.  That file or input is then tagged.
-   */
-  private void runTagger()
-    throws IOException, ClassNotFoundException,
-           NoSuchMethodException, IllegalAccessException,
-           java.lang.reflect.InvocationTargetException
-  {
-    String[] xmlInput = config.getXMLInput();
-    if (xmlInput.length > 0) {
-      if(xmlInput.length > 1 || !xmlInput[0].equals("null")) {
-        tagFromXML();
-        return;
-      }
-    }
+    /**
+     * Runs the tagger when we're in TAG mode.
+     * In this mode, the config contains either the name of the file to
+     * tag or stdin.  That file or input is then tagged.
+     */
+    private void runTagger()
+            throws IOException, ClassNotFoundException,
+            NoSuchMethodException, IllegalAccessException,
+            java.lang.reflect.InvocationTargetException {
+        String[] xmlInput = config.getXMLInput();
+        if (xmlInput.length > 0) {
+            if (xmlInput.length > 1 || !xmlInput[0].equals("null")) {
+                tagFromXML();
+                return;
+            }
+        }
 
-    BufferedWriter writer = null;
-    BufferedReader br = null;
-    try {
-      String outFile = config.getOutputFile();
-      if (outFile.length() > 0) {
-        writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(outFile), config.getEncoding()));
-      } else {
-        writer = new BufferedWriter(new OutputStreamWriter(System.out, config.getEncoding()));
-      }
+        BufferedWriter writer = null;
+        BufferedReader br = null;
+        try {
+            String outFile = config.getOutputFile();
+            if (outFile.length() > 0) {
+                writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(outFile), config.getEncoding()));
+            } else {
+                writer = new BufferedWriter(new OutputStreamWriter(System.out, config.getEncoding()));
+            }
 
-      //Now determine if we're tagging from stdin or from a file,
-      //construct a reader accordingly
-      boolean stdin = config.useStdin();
-      OutputStyle outputStyle = OutputStyle.fromShortName(config.getOutputFormat());
-      if (!stdin) {
-        String filename = config.getFile();
-        if (formatPattern.matcher(filename).find()) {
-          TaggedFileRecord record = TaggedFileRecord.createRecord(config, filename);
-          runTagger(record.reader(), writer, outputStyle);
-        } else {
-          br = IOUtils.readerFromString(config.getFile(), config.getEncoding());
-          runTagger(br, writer, config.getTagInside(), outputStyle);
-        }
-      } else {
-        log.info("Type some text to tag, then EOF.");
-        log.info("  (For EOF, use Return, Ctrl-D on Unix; Enter, Ctrl-Z, Enter on Windows.)");
-        br = new BufferedReader(new InputStreamReader(System.in));
+            //Now determine if we're tagging from stdin or from a file,
+            //construct a reader accordingly
+            boolean stdin = config.useStdin();
+            OutputStyle outputStyle = OutputStyle.fromShortName(config.getOutputFormat());
+            if (!stdin) {
+                String filename = config.getFile();
+                if (formatPattern.matcher(filename).find()) {
+                    TaggedFileRecord record = TaggedFileRecord.createRecord(config, filename);
+                    runTagger(record.reader(), writer, outputStyle);
+                } else {
+                    br = IOUtils.readerFromString(config.getFile(), config.getEncoding());
+                    runTagger(br, writer, config.getTagInside(), outputStyle);
+                }
+            } else {
+                log.info("Type some text to tag, then EOF.");
+                log.info("  (For EOF, use Return, Ctrl-D on Unix; Enter, Ctrl-Z, Enter on Windows.)");
+                br = new BufferedReader(new InputStreamReader(System.in));
 
-        runTaggerStdin(br, writer, outputStyle);
-      }
-    } finally {
-      IOUtils.closeIgnoringExceptions(br);
-      IOUtils.closeIgnoringExceptions(writer);
-    }
-  }
+                runTaggerStdin(br, writer, outputStyle);
+            }
+        } finally {
+            IOUtils.closeIgnoringExceptions(br);
+            IOUtils.closeIgnoringExceptions(writer);
+        }
+    }
 
-  public void runTaggerStdin(BufferedReader reader, BufferedWriter writer, OutputStyle outputStyle)
-    throws IOException
-  {
-    final TokenizerFactory<? extends HasWord> tokenizerFactory = chooseTokenizerFactory();
+    private void runTaggerStdin(BufferedReader reader, BufferedWriter writer, OutputStyle outputStyle)
+            throws IOException {
+        final TokenizerFactory<? extends HasWord> tokenizerFactory = chooseTokenizerFactory();
 
-    //Counts
-    long totalMillis = 0;
-    int numWords = 0;
-    int numSentences = 0;
+        //Counts
+        long totalMillis = 0;
+        int numWords = 0;
+        int numSentences = 0;
 
-    boolean outputVerbosity = config.getOutputVerbosity();
-    boolean outputLemmas = config.getOutputLemmas();
-    Morphology morpha = (outputLemmas) ? new Morphology() : null;
+        boolean outputVerbosity = config.getOutputVerbosity();
+        boolean outputLemmas = config.getOutputLemmas();
+        Morphology morpha = (outputLemmas) ? new Morphology() : null;
 
-    if (outputStyle == OutputStyle.XML ||
-        outputStyle == OutputStyle.INLINE_XML) {
-      writer.write("<?xml version=\"1.0\" encoding=\"" +
-                   config.getEncoding() + "\"?>\n");
-      writer.write("<pos>\n");
-    }
+        if (outputStyle == OutputStyle.XML ||
+                outputStyle == OutputStyle.INLINE_XML) {
+            writer.write("<?xml version=\"1.0\" encoding=\"" +
+                    config.getEncoding() + "\"?>\n");
+            writer.write("<pos>\n");
+        }
 
-    String sentenceDelimiter = config.getSentenceDelimiter();
-    if (sentenceDelimiter != null && sentenceDelimiter.equals("newline")) {
-      sentenceDelimiter = "\n";
-    }
+        String sentenceDelimiter = config.getSentenceDelimiter();
+        if (sentenceDelimiter != null && sentenceDelimiter.equals("newline")) {
+            sentenceDelimiter = "\n";
+        }
 
-    while (true) {
-      //Now we do everything through the doc preprocessor
-      final DocumentPreprocessor docProcessor;
-      String line = reader.readLine();
-      // this happens when we reach end of file
-      if (line == null)
-        break;
-      docProcessor = new DocumentPreprocessor(new StringReader(line));
-      docProcessor.setTokenizerFactory(tokenizerFactory);
-      docProcessor.setSentenceDelimiter(sentenceDelimiter);
-      if (config.keepEmptySentences()) {
-        docProcessor.setKeepEmptySentences(true);
-      }
+        while (true) {
+            //Now we do everything through the doc preprocessor
+            final DocumentPreprocessor docProcessor;
+            String line = reader.readLine();
+            // this happens when we reach end of file
+            if (line == null)
+                break;
+            docProcessor = new DocumentPreprocessor(new StringReader(line));
+            docProcessor.setTokenizerFactory(tokenizerFactory);
+            docProcessor.setSentenceDelimiter(sentenceDelimiter);
+            if (config.keepEmptySentences()) {
+                docProcessor.setKeepEmptySentences(true);
+            }
 
-      for (List<HasWord> sentence : docProcessor) {
-        numWords += sentence.size();
+            for (List<HasWord> sentence : docProcessor) {
+                numWords += sentence.size();
 
-        Timing t = new Timing();
-        tagAndOutputSentence(sentence, outputLemmas, morpha, outputStyle,
-                             outputVerbosity, numSentences, "", writer);
+                Timing t = new Timing();
+                tagAndOutputSentence(sentence, outputLemmas, morpha, outputStyle,
+                        outputVerbosity, numSentences, "", writer);
 
-        totalMillis += t.stop();
-        writer.newLine();
-        writer.flush();
-        numSentences++;
-      }
-    }
+                totalMillis += t.stop();
+                writer.newLine();
+                writer.flush();
+                numSentences++;
+            }
+        }
 
-    if (outputStyle == OutputStyle.XML ||
-        outputStyle == OutputStyle.INLINE_XML) {
-      writer.write("</pos>\n");
-    }
+        if (outputStyle == OutputStyle.XML ||
+                outputStyle == OutputStyle.INLINE_XML) {
+            writer.write("</pos>\n");
+        }
 
-    writer.flush();
-    printErrWordsPerSec(totalMillis, numWords);
-  }
+        writer.flush();
+        printErrWordsPerSec(totalMillis, numWords);
+    }
 
-  public void runTaggerSGML(BufferedReader reader, BufferedWriter writer, OutputStyle outputStyle)
-    throws IOException
-  {
-    Timing t = new Timing();
-
-    //Counts
-    int numWords = 0;
-    int numSentences = 0;
-
-    if (outputStyle == OutputStyle.XML ||
-        outputStyle == OutputStyle.INLINE_XML) {
-      writer.write("<?xml version=\"1.0\" encoding=\"" +
-                   config.getEncoding() + "\"?>\n");
-      writer.write("<pos>\n");
-    }
-
-    // this uses NER codebase technology to read/write SGML-ish files
-    PlainTextDocumentReaderAndWriter<CoreLabel> readerAndWriter = new PlainTextDocumentReaderAndWriter<>();
-    ObjectBank<List<CoreLabel>> ob = new ObjectBank<>(new ReaderIteratorFactory(reader), readerAndWriter);
-    PrintWriter pw = new PrintWriter(writer);
-    for (List<CoreLabel> sentence : ob) {
-      List<CoreLabel> s = Generics.newArrayList();
-      numWords += s.size();
-      List<TaggedWord> taggedSentence = tagSentence(s, false);
-      Iterator<CoreLabel> origIter = sentence.iterator();
-      for (TaggedWord tw : taggedSentence) {
-        CoreLabel cl = origIter.next();
-        cl.set(CoreAnnotations.AnswerAnnotation.class, tw.tag());
-      }
-      readerAndWriter.printAnswers(sentence, pw, outputStyle, true);
-      ++numSentences;
-    }
-
-    if (outputStyle == OutputStyle.XML ||
-        outputStyle == OutputStyle.INLINE_XML) {
-      writer.write("</pos>\n");
-    }
-
-    writer.flush();
-    long millis = t.stop();
-    printErrWordsPerSec(millis, numWords);
-  }
 
-  public <X extends HasWord> void runTagger(Iterable<List<X>> document,
-                                            BufferedWriter writer,
-                                            OutputStyle outputStyle)
-    throws IOException
-  {
-    Timing t = new Timing();
+    public <X extends HasWord> void runTagger(Iterable<List<X>> document,
+                                              BufferedWriter writer,
+                                              OutputStyle outputStyle)
+            throws IOException {
+        Timing t = new Timing();
 
-    //Counts
-    int numWords = 0;
-    int numSentences = 0;
+        //Counts
+        int numWords = 0;
+        int numSentences = 0;
 
-    boolean outputVerbosity = config.getOutputVerbosity();
-    boolean outputLemmas = config.getOutputLemmas();
+        boolean outputVerbosity = config.getOutputVerbosity();
+        boolean outputLemmas = config.getOutputLemmas();
 
-    if (outputStyle == OutputStyle.XML ||
-        outputStyle == OutputStyle.INLINE_XML) {
-      writer.write("<?xml version=\"1.0\" encoding=\"" +
-                   config.getEncoding() + "\"?>\n");
-      writer.write("<pos>\n");
-    }
+        if (outputStyle == OutputStyle.XML ||
+                outputStyle == OutputStyle.INLINE_XML) {
+            writer.write("<?xml version=\"1.0\" encoding=\"" +
+                    config.getEncoding() + "\"?>\n");
+            writer.write("<pos>\n");
+        }
 
 
-    if (config.getNThreads() != 1) {
-      MulticoreWrapper<List<? extends HasWord>, List<? extends HasWord>> wrapper = new MulticoreWrapper<>(config.getNThreads(), new SentenceTaggingProcessor(this, outputLemmas));
-      for (List<X> sentence : document) {
-        wrapper.put(sentence);
-        while (wrapper.peek()) {
-          List<? extends HasWord> taggedSentence = wrapper.poll();
-          numWords += taggedSentence.size();
-          outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, numSentences, "\n", writer);
-          numSentences++;
-        }
-      }
-      wrapper.join();
-      while (wrapper.peek()) {
-        List<? extends HasWord> taggedSentence = wrapper.poll();
-        numWords += taggedSentence.size();
-        outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, numSentences, "\n", writer);
-        numSentences++;
-      }
-    } else {
-      Morphology morpha = (outputLemmas) ? new Morphology() : null;
-      for (List<X> sentence : document) {
-        numWords += sentence.size();
+        if (config.getNThreads() != 1) {
+            MulticoreWrapper<List<? extends HasWord>, List<? extends HasWord>> wrapper = new MulticoreWrapper<>(config.getNThreads(), new SentenceTaggingProcessor(this, outputLemmas));
+            for (List<X> sentence : document) {
+                wrapper.put(sentence);
+                while (wrapper.peek()) {
+                    List<? extends HasWord> taggedSentence = wrapper.poll();
+                    numWords += taggedSentence.size();
+                    outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, numSentences, "\n", writer);
+                    numSentences++;
+                }
+            }
+            wrapper.join();
+            while (wrapper.peek()) {
+                List<? extends HasWord> taggedSentence = wrapper.poll();
+                numWords += taggedSentence.size();
+                outputTaggedSentence(taggedSentence, outputLemmas, outputStyle, outputVerbosity, numSentences, "\n", writer);
+                numSentences++;
+            }
+        } else {
+            Morphology morpha = (outputLemmas) ? new Morphology() : null;
+            for (List<X> sentence : document) {
+                numWords += sentence.size();
 
-        tagAndOutputSentence(sentence, outputLemmas, morpha, outputStyle,
-                             outputVerbosity, numSentences, "\n", writer);
+                tagAndOutputSentence(sentence, outputLemmas, morpha, outputStyle,
+                        outputVerbosity, numSentences, "\n", writer);
 
-        numSentences++;
-      }
-    }
+                numSentences++;
+            }
+        }
 
-    if (outputStyle == OutputStyle.XML ||
-        outputStyle == OutputStyle.INLINE_XML) {
-      writer.write("</pos>\n");
-    }
+        if (outputStyle == OutputStyle.XML ||
+                outputStyle == OutputStyle.INLINE_XML) {
+            writer.write("</pos>\n");
+        }
 
-    writer.flush();
-    long millis = t.stop();
-    printErrWordsPerSec(millis, numWords);
-  }
+        writer.flush();
+        long millis = t.stop();
+        printErrWordsPerSec(millis, numWords);
+    }
 
 
-  /**
-   * This method runs the tagger on the provided reader and writer.
-   *
-   * It takes input from the given {@code reader}, applies the
-   * tagger to it one sentence at a time (determined using
-   * documentPreprocessor), and writes the output to the given
-   * {@code writer}.
-   *
-   * The document is broken into sentences using the sentence
-   * processor determined in the tagger's TaggerConfig.
-   *
-   * {@code tagInside} makes the tagger run in XML mode.... If set
-   * to non-empty, instead of processing the document as one large
-   * text blob, it considers each region in between the given tag to
-   * be a separate text blob.
-   */
-  public void runTagger(BufferedReader reader, BufferedWriter writer,
-                        String tagInside, OutputStyle outputStyle)
-    throws IOException
-  {
-    String sentenceDelimiter = config.getSentenceDelimiter();
-    if (sentenceDelimiter != null && sentenceDelimiter.equals("newline")) {
-      sentenceDelimiter = "\n";
-    }
-    final TokenizerFactory<? extends HasWord> tokenizerFactory = chooseTokenizerFactory();
+    /**
+     * This method runs the tagger on the provided reader and writer.
+     * <p>
+     * It takes input from the given {@code reader}, applies the
+     * tagger to it one sentence at a time (determined using
+     * documentPreprocessor), and writes the output to the given
+     * {@code writer}.
+     * <p>
+     * The document is broken into sentences using the sentence
+     * processor determined in the tagger's TaggerConfig.
+     * <p>
+     * {@code tagInside} makes the tagger run in XML mode.... If set
+     * to non-empty, instead of processing the document as one large
+     * text blob, it considers each region in between the given tag to
+     * be a separate text blob.
+     */
+    public void runTagger(BufferedReader reader, BufferedWriter writer,
+                          String tagInside, OutputStyle outputStyle)
+            throws IOException {
+        String sentenceDelimiter = config.getSentenceDelimiter();
+        if (sentenceDelimiter != null && sentenceDelimiter.equals("newline")) {
+            sentenceDelimiter = "\n";
+        }
+        final TokenizerFactory<? extends HasWord> tokenizerFactory = chooseTokenizerFactory();
 
-    //Now we do everything through the doc preprocessor
-    final DocumentPreprocessor docProcessor;
-    if (tagInside.length() > 0) {
-      docProcessor = new DocumentPreprocessor(reader, DocumentPreprocessor.DocType.XML);
-      docProcessor.setElementDelimiter(tagInside);
-      if (config.keepEmptySentences()) {
-        docProcessor.setKeepEmptySentences(true);
-      }
-    } else {
-      docProcessor = new DocumentPreprocessor(reader);
-      docProcessor.setSentenceDelimiter(sentenceDelimiter);
-      if (config.keepEmptySentences()) {
-        docProcessor.setKeepEmptySentences(true);
-      }
-    }
-    docProcessor.setTokenizerFactory(tokenizerFactory);
+        //Now we do everything through the doc preprocessor
+        final DocumentPreprocessor docProcessor;
+        if (tagInside.length() > 0) {
+            docProcessor = new DocumentPreprocessor(reader, DocumentPreprocessor.DocType.XML);
+            docProcessor.setElementDelimiter(tagInside);
+            if (config.keepEmptySentences()) {
+                docProcessor.setKeepEmptySentences(true);
+            }
+        } else {
+            docProcessor = new DocumentPreprocessor(reader);
+            docProcessor.setSentenceDelimiter(sentenceDelimiter);
+            if (config.keepEmptySentences()) {
+                docProcessor.setKeepEmptySentences(true);
+            }
+        }
+        docProcessor.setTokenizerFactory(tokenizerFactory);
 
-    runTagger(docProcessor, writer, outputStyle);
-  }
+        runTagger(docProcessor, writer, outputStyle);
+    }
 
-  public List<? extends HasWord> tagCoreLabelsOrHasWords(List<? extends HasWord> sentence, Morphology morpha, boolean outputLemmas) {
-    if (sentence.size() > 0 && sentence.get(0) instanceof CoreLabel) {
-      List<CoreLabel> coreLabels = castCoreLabels(sentence);
-      tagCoreLabels(coreLabels);
-      if (outputLemmas) {
-        // We may want to lemmatize things without using an existing
-        // Morphology object, as Morphology objects are not
-        // thread-safe, so we would make a new one here
-        if (morpha == null) {
-          morpha = new Morphology();
-        }
-        lemmatize(coreLabels, morpha);
-      }
-      return coreLabels;
-    } else {
-      List<TaggedWord> taggedSentence = tagSentence(sentence, false);
-      return taggedSentence;
-    }
-  }
+    public List<? extends HasWord> tagCoreLabelsOrHasWords(List<? extends HasWord> sentence, Morphology morpha, boolean outputLemmas) {
+        if (sentence.size() > 0 && sentence.get(0) instanceof CoreLabel) {
+            List<CoreLabel> coreLabels = castCoreLabels(sentence);
+            tagCoreLabels(coreLabels);
+            if (outputLemmas) {
+                // We may want to lemmatize things without using an existing
+                // Morphology object, as Morphology objects are not
+                // thread-safe, so we would make a new one here
+                if (morpha == null) {
+                    morpha = new Morphology();
+                }
+                lemmatize(coreLabels, morpha);
+            }
+            return coreLabels;
+        } else {
+            List<TaggedWord> taggedSentence = tagSentence(sentence, false);
+            return taggedSentence;
+        }
+    }
 
-  public void tagAndOutputSentence(List<? extends HasWord> sentence,
-                                   boolean outputLemmas, Morphology morpha,
-                                   OutputStyle outputStyle,
-                                   boolean outputVerbosity, int numSentences,
-                                   String separator, Writer writer) {
-    sentence = tagCoreLabelsOrHasWords(sentence, morpha, outputLemmas);
-    outputTaggedSentence(sentence, outputLemmas, outputStyle, outputVerbosity, numSentences, separator, writer);
-  }
+    public void tagAndOutputSentence(List<? extends HasWord> sentence,
+                                     boolean outputLemmas, Morphology morpha,
+                                     OutputStyle outputStyle,
+                                     boolean outputVerbosity, int numSentences,
+                                     String separator, Writer writer) {
+        sentence = tagCoreLabelsOrHasWords(sentence, morpha, outputLemmas);
+        outputTaggedSentence(sentence, outputLemmas, outputStyle, outputVerbosity, numSentences, separator, writer);
+    }
 
-  public void outputTaggedSentence(List<? extends HasWord> sentence,
-                                   boolean outputLemmas, OutputStyle outputStyle,
-                                   boolean outputVerbosity, int numSentences,
-                                   String separator, Writer writer) {
-    try {
-      switch (outputStyle) {
-      case TSV:
-        writer.write(getTsvWords(outputVerbosity, outputLemmas, sentence));
-        break;
-      case XML:
-      case INLINE_XML:
-        writeXMLSentence(writer, sentence, numSentences, outputLemmas);
-        break;
-      case SLASH_TAGS:
-        writer.write(SentenceUtils.listToString(sentence, false, config.getTagSeparator()));
-        writer.write(separator);
-        break;
-      default:
-        throw new IllegalArgumentException("Unsupported output style " + outputStyle);
-      }
-    } catch (IOException e) {
-      throw new RuntimeIOException(e);
-    }
-  }
+    public void outputTaggedSentence(List<? extends HasWord> sentence,
+                                     boolean outputLemmas, OutputStyle outputStyle,
+                                     boolean outputVerbosity, int numSentences,
+                                     String separator, Writer writer) {
+        try {
+            switch (outputStyle) {
+                case TSV:
+                    writer.write(getTsvWords(outputVerbosity, outputLemmas, sentence));
+                    break;
+                case XML:
+                case INLINE_XML:
+                    writeXMLSentence(writer, sentence, numSentences, outputLemmas);
+                    break;
+                case SLASH_TAGS:
+                    writer.write(SentenceUtils.listToString(sentence, false, config.getTagSeparator()));
+                    writer.write(separator);
+                    break;
+                default:
+                    throw new IllegalArgumentException("Unsupported output style " + outputStyle);
+            }
+        } catch (IOException e) {
+            throw new RuntimeIOException(e);
+        }
+    }
 
-  /**
-   * Command-line tagger interface.
-   * Can be used to train or test taggers, or to tag text, taking input from
-   * stdin or a file.
-   * See class documentation for usage.
-   *
-   * @param args Command-line arguments
-   * @throws IOException If any file problems
-   */
-  public static void main(String[] args) throws Exception {
-    TaggerConfig config = new TaggerConfig(args);
+    /**
+     * Command-line tagger interface.
+     * Can be used to train or test taggers, or to tag text, taking input from
+     * stdin or a file.
+     * See class documentation for usage.
+     *
+     * @param args Command-line arguments
+     * @throws IOException If any file problems
+     */
+    public static void main(String[] args) throws Exception {
+        TaggerConfig config = new TaggerConfig(args);
 
-    if (config.getMode() == TaggerConfig.Mode.TRAIN) {
-      runTraining(config);
-    } else if (config.getMode() == TaggerConfig.Mode.TAG) {
-      runTagger(config);
-    } else if (config.getMode() == TaggerConfig.Mode.TEST) {
-      runTest(config);
-    } else if (config.getMode() == TaggerConfig.Mode.DUMP) {
-      dumpModel(config);
-    } else {
-      log.info("Impossible: nothing to do. None of train, tag, test, or dump was specified.");
-    }
-  } // end main()
+        if (config.getMode() == TaggerConfig.Mode.TRAIN) {
+            runTraining(config);
+        } else if (config.getMode() == TaggerConfig.Mode.TAG) {
+            runTagger(config);
+        } else if (config.getMode() == TaggerConfig.Mode.TEST) {
+            runTest(config);
+        } else if (config.getMode() == TaggerConfig.Mode.DUMP) {
+            dumpModel(config);
+        } else {
+            log.info("Impossible: nothing to do. None of train, tag, test, or dump was specified.");
+        }
+    } // end main()
 
 
-  static class SentenceTaggingProcessor implements ThreadsafeProcessor<List<? extends HasWord>, List<? extends HasWord>> {
-    MaxentTagger maxentTagger;
-    boolean outputLemmas;
+    static class SentenceTaggingProcessor implements ThreadsafeProcessor<List<? extends HasWord>, List<? extends HasWord>> {
+        MaxentTagger maxentTagger;
+        boolean outputLemmas;
 
-    SentenceTaggingProcessor(MaxentTagger maxentTagger, boolean outputLemmas) {
-      this.maxentTagger = maxentTagger;
-      this.outputLemmas = outputLemmas;
-    }
+        SentenceTaggingProcessor(MaxentTagger maxentTagger, boolean outputLemmas) {
+            this.maxentTagger = maxentTagger;
+            this.outputLemmas = outputLemmas;
+        }
 
-    @Override
-    public List<? extends HasWord> process(List<? extends HasWord> sentence) {
-      return maxentTagger.tagCoreLabelsOrHasWords(sentence, null, outputLemmas);
-    }
+        @Override
+        public List<? extends HasWord> process(List<? extends HasWord> sentence) {
+            return maxentTagger.tagCoreLabelsOrHasWords(sentence, null, outputLemmas);
+        }
 
-    @Override
-    public ThreadsafeProcessor<List<? extends HasWord>, List<? extends HasWord>> newInstance() {
-      // MaxentTagger is threadsafe
-      return this;
-    }
-  }
+        @Override
+        public ThreadsafeProcessor<List<? extends HasWord>, List<? extends HasWord>> newInstance() {
+            // MaxentTagger is threadsafe
+            return this;
+        }
+    }
 
-  private static final long serialVersionUID = 2;
+    private static final long serialVersionUID = 2;
 
 }
Index: src/edu/stanford/nlp/tagger/maxent/TaggerExperiments.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/TaggerExperiments.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/TaggerExperiments.java	(revision f3bf217addd8a9e5a5e4216a488f94870df00bec)
@@ -176,7 +176,7 @@
           continue;
         }
         int numEvidence = 0;
-        int y = maxentTagger.tags.getIndex(fK.tag);
+        int y = maxentTagger.tags.indexOf(fK.tag);
         for (int xValue : xValues) {
 
           if (maxentTagger.occurringTagsOnly) {
@@ -267,7 +267,7 @@
             }
             fValueAssociations.put(fK.val, fTagAssociations);
           }
-          fTagAssociations[maxentTagger.tags.getIndex(fK.tag)] = numFeats;
+          fTagAssociations[maxentTagger.tags.indexOf(fK.tag)] = numFeats;
 
           numFeats++;
         }
@@ -470,7 +470,7 @@
   public static void main(String[] args) {
     int[] hPos = {0, 1, 2, -1, -2};
     boolean[] isTag = {false, false, false, true, true};
-    maxentTagger.init();
+    maxentTagger.updatePointers();
     TaggerExperiments gophers = new TaggerExperiments("trainhuge.txt", null);
     //gophers.ptilde();
   }
Index: src/edu/stanford/nlp/tagger/maxent/TemplateHash.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/TemplateHash.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/TemplateHash.java	(revision 108c365276c71c4be9040ccfff65d298300ae639)
@@ -37,8 +37,8 @@
     return tempHash.get(p).getPositions();
   }
 
-  //public void init() {
-//    cdm 2008: stringNums isn't used anywhere, so we now don't do any init.
+  //public void updatePointers() {
+//    cdm 2008: stringNums isn't used anywhere, so we now don't do any updatePointers.
 //    int num = maxentTagger.extractors.getSize() + maxentTagger.extractorsRare.getSize();
 //    //log.info("A total of "+num+" features in TemplateHash");
 //    stringNums = new String[num];
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .gitignore	(revision 108c365276c71c4be9040ccfff65d298300ae639)
+++ .gitignore	(revision 3f3ac8938eedd2e6ad4722848b70ff6bcbf4ff68)
@@ -6,3 +6,9 @@
 *.so
 .DS_Store
 .settings/**
+
+.idea/
+*.iml
+/target/
+/classes/
+/POS-test/
Index: src/edu/stanford/nlp/sequences/ExactBestSequenceFinder.kt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/sequences/ExactBestSequenceFinder.kt	(revision c8c4456601d5c057e1de2272ffa2dfc1ea81e8b8)
+++ src/edu/stanford/nlp/sequences/ExactBestSequenceFinder.kt	(revision c8c4456601d5c057e1de2272ffa2dfc1ea81e8b8)
@@ -0,0 +1,126 @@
+package edu.stanford.nlp.sequences
+
+
+import java.lang.Double.NEGATIVE_INFINITY
+
+
+/**
+ * A class capable of computing the best sequence given a SequenceModel.
+ * Uses the Viterbi algorithm.
+ *
+ * @author Dan Klein
+ * @author Teg Grenager (grenager@stanford.edu)
+ */
+class ExactBestSequenceFinder : BestSequenceFinder {
+
+    /**
+     * Runs the Viterbi algorithm on the sequence model given by the TagScorer
+     * in order to find the best sequence.
+     *
+     * @param ts The SequenceModel to be used for scoring
+     * @return An array containing the int tags of the best sequence
+     */
+    override fun bestSequence(ts: SequenceModel): IntArray {
+        // Set up tag options
+        val length = ts.length()
+        val leftWindow = ts.leftWindow()
+        val rightWindow = ts.rightWindow()
+        val padLength = length + leftWindow + rightWindow
+
+        // constraint to: only observed closed tags for the word IF word known ELSE all open tags
+        // contains integer code representing a specific tag
+        val tags = Array<IntArray>(padLength) { ts.getPossibleValues(it) }
+        val tagNum = IntArray(padLength) { tags[it].size }
+
+        // Set up product space sizes
+        val productSizes = IntArray(length)
+        var curProduct = tagNum.take(leftWindow + rightWindow).reduce { x, y -> x * y }
+        for (pos in 0 until length) {
+            curProduct *= tagNum[pos + leftWindow + rightWindow] // shift on
+            productSizes[pos] = curProduct
+            curProduct /= tagNum[pos] // shift off
+        }
+
+        // Score all of each window's options
+        val currentTagSequence = IntArray(padLength)
+        val windowScore = Array(length) { DoubleArray(productSizes[it]) }
+        for (pos in leftWindow until leftWindow + length) {
+            currentTagSequence.fill(tags[0][0])
+
+            for (product in 0 until productSizes[pos - leftWindow]) {
+                var p = product
+                var shift = 1
+                for (curPos in pos + rightWindow downTo pos - leftWindow) {
+                    currentTagSequence[curPos] = tags[curPos][p % tagNum[curPos]]
+                    p /= tagNum[curPos]
+                    if (curPos > pos) {
+                        shift *= tagNum[curPos]
+                    }
+                }
+
+                if (currentTagSequence[pos] == tags[pos][0]) {
+                    // get the scores of all tags considered for the current position with respect to the whole depending tagsequence
+                    // a subset of the posterior as log-probabilities
+                    val scores = ts.scoresOf(currentTagSequence, pos)
+                    // fill in the relevant windowScores
+                    for (t in 0 until tagNum[pos]) {
+                        windowScore[pos - leftWindow][product + t * shift] = scores[t]
+                    }
+                }
+            }
+        }
+
+        // Set up score and backtrace arrays
+        val score = Array(length) { DoubleArray(productSizes[it]) { NEGATIVE_INFINITY } }
+        val trace = Array(length - 1) { IntArray(productSizes[it + 1]) }
+
+
+        // ############################
+        // DO FORWARD VITERBI ALGORITHM
+        // ############################
+
+        // check for initial spot
+        if (productSizes[0] >= 0) System.arraycopy(windowScore[0], 0, score[0], 0, productSizes[0])
+
+        // loop over the classification spot (positions in sentence)
+        for (pos in 1 until length) {
+            // loop over view windows
+            for (product in 0 until productSizes[pos]) {
+                val factor = productSizes[pos] / tagNum[pos + leftWindow + rightWindow]
+                val sharedProduct = product / tagNum[pos + leftWindow + rightWindow]
+
+                // calculate maximum
+                val (s, t) = (0 until tagNum[pos - 1]).map {
+                    val predProduct = it * factor + sharedProduct
+                    // this is actually the probability of the the state.
+                    // there is no transition probability considered, as there is no notion as transitions from one tag to another
+                    val predScore = score[pos - 1][predProduct] + windowScore[pos][product]
+                    Pair(predScore, predProduct)
+                }.maxBy(Pair<Double, Int>::first)!!
+
+                score[pos][product] = s
+                // this is the backpointer to the current state, therefore its correct if we incorporate the emission probability
+                trace[pos - 1][product] = t
+            }
+        }
+
+        // Project the actual tag sequence
+
+        // select the best window with respect to its score
+        var bestCurrentProduct = (0 until productSizes[length - 1]).maxBy { score[length - 1][it] }!!
+
+        var lastProduct = bestCurrentProduct
+        var last = padLength - 1
+        while (last >= length - 1 && last >= 0) {
+            currentTagSequence[last] = tags[last][lastProduct % tagNum[last]]
+            lastProduct /= tagNum[last]
+            last--
+        }
+        for (pos in length - 2 downTo 0) {
+            val bestNextProduct = bestCurrentProduct
+            bestCurrentProduct = trace[pos][bestNextProduct]
+            currentTagSequence[pos] = tags[pos][bestCurrentProduct / (productSizes[pos] / tagNum[pos])]
+        }
+        return currentTagSequence.copyOfRange(leftWindow, leftWindow + length)
+    }
+}
Index: src/edu/stanford/nlp/tagger/maxent/PairsHolder.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/PairsHolder.java	(revision bd1987a8b533a60e998e44a2cd5c5f91398f6a1b)
+++ src/edu/stanford/nlp/tagger/maxent/PairsHolder.java	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
@@ -11,129 +11,78 @@
 
 import java.util.*;
 
-/** A simple class that maintains a list of WordTag pairs which are interned
- *  as they are added.  This stores a tagged corpus.
- *  It is also used to represent partial histories at tagging time.
- *  It may not simply represent a sentence, and a History is used to overlay a sentence on a PairsHolder.
+/**
+ * A simple class that maintains a list of WordTag pairs which are interned
+ * as they are added.  This stores a tagged corpus.
+ * It is also used to represent partial histories at tagging time.
+ * It may not simply represent a sentence, and a History is used to overlay a sentence on a PairsHolder.
  *
- *  @author Kristina Toutanova
- *  @version 1.0
+ * @author Kristina Toutanova
+ * @version 1.0
  */
 public class PairsHolder {
 
-  // todo: In Java 5+, just make this class an ArrayList<WordTag> and be done with it?? Or actually, probably a PaddedList. Or need a WindowedList?
+    // todo: In Java 5+, just make this class an ArrayList<WordTag> and be done with it?? Or actually, probably a PaddedList. Or need a WindowedList?
 
-  private final ArrayList<WordTag> arr = new ArrayList<>();
+    private final ArrayList<WordTag> arr = new ArrayList<>();
 
-  public PairsHolder() {}
+    public PairsHolder() {
+    }
 
-  // todo: This method seems crazy.  Can't we either just do nothing or using ensureCapacity()?
-  public void setSize(int s) {
-    while (arr.size() < s) {
-      arr.add(new WordTag(null,"NN"));  // todo: remove NN.  NA okay?
-    }
-  }
+    // todo: This method seems crazy.  Can't we either just do nothing or using ensureCapacity()?
+    public void setSize(int s) {
+        while (arr.size() < s) {
+            arr.add(new WordTag(null, Defaults.naTag));
+        }
+    }
 
-  public int size() {
-    return arr.size();
-  }
+    public int size() {
+        return arr.size();
+    }
 
-  void clear() {
-    arr.clear();
-  }
+    void clear() {
+        arr.clear();
+    }
 
-  /* -----------------
-     CDM May 2008.  This method was unused.  But it also has a bug in it
-     in that the equals() test can never succeed (Integer vs WordTag).
-     So I'm commenting it out for now....
-  public int[] getIndexes(Object wordtag) {
-    ArrayList<Integer> arr1 = new ArrayList<Integer>();
-    int l = wordtag.hashCode();
-    Integer lO = Integer.valueOf(l);
-    for (int i = 0; i < arrNum.size(); i++) {
-      if (arrNum.get(i).equals(lO)) {
-        arr1.add(Integer.valueOf(i));
-      }
-    }
-    int[] ret = new int[arr1.size()];
-    for (int i = 0; i < arr1.size(); i++) {
-      ret[i] = arr1.get(i).intValue();
-    }
-    return ret;
-  }
-   */
 
-  void add(WordTag wordtag) {
-    arr.add(wordtag);
-  }
+    void add(WordTag wordtag) {
+        arr.add(wordtag);
+    }
 
-  void setWord(int pos, String word) {
-    arr.get(pos).setWord(word);
-  }
+    void setWord(int pos, String word) {
+        arr.get(pos).setWord(word);
+    }
 
-  void setTag(int pos, String tag) {
-    arr.get(pos).setTag(tag);
-  }
+    void setTag(int pos, String tag) {
+        arr.get(pos).setTag(tag);
+    }
 
-  /* Methods unused. Commented for now:
-  public void save(String filename) {
-    try {
-      DataOutputStream rf = IOUtils.getDataOutputStream(filename);
-      int sz = arr.size();
-      rf.writeInt(sz);
-      for (int i = 0; i < sz; i++) {
-        //save the wordtag in the file
-        WordTag wT = arr.get(i);
-        rf.writeUTF(wT.word());
-        rf.writeUTF(wT.tag());
-      }
-      rf.close();
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  public void read(String filename) {
-    try {
-      InDataStreamFile rf = new InDataStreamFile(filename);
-      int len = rf.readInt();
-      for (int i = 0; i < len; i++) {
-        WordTag wT = new WordTag();
-        wT.setWord(rf.readUTF());
-        wT.setTag(rf.readUTF());
-        add(wT);
-
-      }
-      rf.close();
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-  */
-
-  String getTag(int position) {
-    return arr.get(position).tag();
-  }
+    String getTag(int position) {
+        return arr.get(position).tag();
+    }
 
-  /** This gets a word at an absolute position. */
-  String getWord(int position) {
-    return arr.get(position).word();
-  }
+    /**
+     * This gets a word at an absolute position.
+     */
+    String getWord(int position) {
+        return arr.get(position).word();
+    }
 
-  /** This gets a word at a position relative to the "current" position in the history. */
-  String getWord(History h, int position) {
-    final int p = h.current + position;
-    return (p >= h.start && p <= h.end) ? arr.get(p).word() : "NA";
-  }
+    /**
+     * This gets a word at a position relative to the "current" position in the history.
+     */
+    String getWord(History h, int position) {
+        final int p = h.current + position;
+        return (p >= h.start && p <= h.end) ? arr.get(p).word() : Defaults.naTag;
+    }
 
-  String getTag(History h, int position) {
-    final int p = h.current + position;
-    return (p >= h.start && p <= h.end) ? arr.get(p).tag() : "NA";
-  }
+    String getTag(History h, int position) {
+        final int p = h.current + position;
+        return (p >= h.start && p <= h.end) ? arr.get(p).tag() : Defaults.naTag;
+    }
 
-  @Override
-  public String toString() {
-    return arr.toString();
-  }
-
+    @Override
+    public String toString() {
+        return arr.toString();
+    }
 }
Index: src/edu/stanford/nlp/tagger/maxent/BaseSetTagger.kt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/BaseSetTagger.kt	(revision b2d1a9a42e572c987c6031e6ec99113741579220)
+++ src/edu/stanford/nlp/tagger/maxent/BaseSetTagger.kt	(revision b2d1a9a42e572c987c6031e6ec99113741579220)
@@ -0,0 +1,70 @@
+package edu.stanford.nlp.tagger.maxent
+
+import edu.stanford.nlp.io.PrintFile
+
+class BaseSetTagger(maxentTagger: MaxentTagger?) : BaseTagger(maxentTagger) {
+    companion object {
+        private var n_sent = 1
+    }
+
+
+    internal override fun writeTagsAndErrors(pf: PrintFile?, verboseResults: Boolean) {
+        super.writeTagsAndErrors(null, verboseResults)
+        // call ubop for the whole sequence
+        //val finalTagSets = deriveTagSets(::genSingletons, false)
+        //println(finalTagSets.joinToString())
+
+        if (pf == null)
+            return
+
+        //write stuff to csv
+        val sequence = (List(leftWindow()) { naTag } + finalTags + List(rightWindow()) { naTag }).map { maxentTagger.tags.indexOf(it) }.toIntArray()
+
+        // skip end of sentence tag
+        for (pos in 0 until size - 1) {
+            //word; sentenceID; isunknown; truelabel; label posterior; constrained tags;
+            val word = sent[pos]
+            val data = arrayOf(
+                    word,
+                    n_sent.toString(),
+                    maxentTagger.dict.isUnknown(word).toString(),
+                    correctTags[pos],
+                    finalTags[pos],
+                    scoresOf(sequence, pos + leftWindow(), false).joinToString(prefix = "[", postfix = "]"),
+                    getPossibleTagsAsString(pos + leftWindow()).joinToString(prefix = "[", postfix = "]")
+            )
+            pf.println(data.joinToString(separator = ";", transform = { "\"$it\"" }))
+            //if (!getPossibleTagsAsString(pos + leftWindow()).contains(correctTags[pos]))
+            //    println("forced misclassification: ${finalTags[pos]}; ${getPossibleTagsAsString(pos + leftWindow()).joinToString(prefix = "[", postfix = "]")}")
+        }
+
+        n_sent++
+    }
+
+    private fun deriveTagSets(setpredictor: (scores: DoubleArray, tags: Array<String>) -> Set<String> = ::genSingletons,
+                              constraintTags: Boolean = true): List<Set<String>> {
+        // fill left and right window with NA tags and convert tags to tagindices
+        val sequence = (List(leftWindow()) { naTag } + finalTags + List(rightWindow()) { naTag }).map { maxentTagger.tags.indexOf(it) }.toIntArray()
+
+
+        // skip end of sentence tag
+        val result = (0 until size - 1).map { pos ->
+            // in each position we call the set-valued predictor
+            if (constraintTags) {
+                val scores = scoresOf(sequence, pos + leftWindow())
+                val tags = getPossibleTagsAsString(pos + leftWindow())
+                setpredictor(scores, tags)
+            } else {
+                // similar to above, but we do not constraint the scores.
+                val scores = scoresOf(sequence, pos + leftWindow(), false)
+                val tags = maxentTagger.tags.tagSet().sortedBy { maxentTagger.tags.indexOf(it) }.toTypedArray()
+                setpredictor(scores, tags)
+            }
+        }
+
+        return result
+    }
+
+    private fun genSingletons(scores: DoubleArray, filteredTags: Array<String>) =
+            setOf("${scores.size}:" + scores.indices.maxBy { scores[it] }?.let { filteredTags[it] })
+}
\ No newline at end of file
Index: src/edu/stanford/nlp/tagger/maxent/TestSentence.java
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/TestSentence.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/BaseTagger.java	(revision b2d1a9a42e572c987c6031e6ec99113741579220)
@@ -28,24 +28,21 @@
 
 import edu.stanford.nlp.io.PrintFile;
 import edu.stanford.nlp.ling.*;
-import edu.stanford.nlp.ling.SentenceUtils;
 import edu.stanford.nlp.math.ArrayMath;
-import edu.stanford.nlp.math.SloppyMath;
 import edu.stanford.nlp.sequences.BestSequenceFinder;
 import edu.stanford.nlp.sequences.ExactBestSequenceFinder;
 import edu.stanford.nlp.sequences.SequenceModel;
 import edu.stanford.nlp.tagger.common.Tagger;
 import edu.stanford.nlp.util.*;
 import edu.stanford.nlp.util.logging.Redwood;
+import org.jetbrains.annotations.Contract;
 
 import java.io.OutputStreamWriter;
 import java.io.PrintWriter;
 import java.io.StringWriter;
 import java.io.UnsupportedEncodingException;
 import java.util.*;
-import java.text.NumberFormat;
-import java.text.DecimalFormat;
-import java.util.stream.IntStream;
+import java.util.stream.Stream;
 
 
 /**
@@ -53,793 +50,413 @@
  * @author Michel Galley
  * @version 1.0
  */
-public class TestSentence implements SequenceModel  {
+public class BaseTagger implements SequenceModel {
 
-  /** A logger for this class */
-  private static final Redwood.RedwoodChannels log = Redwood.channels(TestSentence.class);
+    /**
+     * A logger for this class
+     */
+    protected static final Redwood.RedwoodChannels log = Redwood.channels(BaseTagger.class);
 
-  protected final boolean VERBOSE;
-  protected static final String naTag = "NA";
-  private static final String[] naTagArr = { naTag };
-  protected static final boolean DBG = false;
-  protected static final int kBestSize = 1;
+    protected static final String naTag = Defaults.naTag;
+    private static final String[] naTagArr = {naTag};
+    protected static final boolean DBG = false;
+    protected static final boolean doConstraintTagSet = true;
 
-  protected final String tagSeparator;
-  protected final String encoding;
-  protected final PairsHolder pairs = new PairsHolder();
-  protected List<String> sent;
-  private List<String> originalTags;
-  // origWords is only set when run with a list of HasWords; when run
-  // with a list of strings, this will be null
-  protected List<HasWord> origWords;
-  protected int size; // TODO this always has the value of sent.size(). Remove it? [cdm 2008]
-  // protected double[][][] probabilities;
-  private String[] correctTags;
-  protected String[] finalTags;
-  int numRight;
-  int numWrong;
-  int numUnknown;
-  int numWrongUnknown;
-  private int endSizePairs; // = 0;
+    protected final String tagSeparator;
+    protected final String encoding;
+    protected final PairsHolder pairs = new PairsHolder();
+    protected List<String> sent;
+    // is only set inside "tagSentence" and only if the "reuseTags" flag is set true.
+    private List<String> originalTags;
+    // origWords is only set when run with a list of HasWords; when run
+    // with a list of strings, this will be null
+    private List<HasWord> origWords;
+    protected int size; // TODO this always has the value of sent.size(). Remove it? [cdm 2008]
+    protected String[] correctTags;
+    String[] finalTags;
+    int numRight;
+    int numWrong;
+    int numUnknown;
+    int numWrongUnknown;
 
-  private volatile History history;
-  private volatile Map<String,double[]> localScores = Generics.newHashMap();
-  private volatile double[][] localContextScores;
+    protected volatile History history;
+    private volatile Map<String, double[]> localScores = Generics.newHashMap();
+    private volatile double[][] localContextScores;
 
-  protected final MaxentTagger maxentTagger;
+    protected final MaxentTagger maxentTagger;
 
-  public TestSentence(MaxentTagger maxentTagger) {
-    assert(maxentTagger != null);
-    assert(maxentTagger.getLambdaSolve() != null);
-    this.maxentTagger = maxentTagger;
-    if (maxentTagger.config != null) {
-      tagSeparator = maxentTagger.config.getTagSeparator();
-      encoding = maxentTagger.config.getEncoding();
-      VERBOSE = maxentTagger.config.getVerbose();
-    } else {
-      tagSeparator = TaggerConfig.getDefaultTagSeparator();
-      encoding = "utf-8";
-      VERBOSE = false;
-    }
-    history = new History(pairs, maxentTagger.extractors);
-  }
+    public BaseTagger(MaxentTagger maxentTagger) {
+        assert (maxentTagger != null);
+        assert (maxentTagger.getLambdaSolve() != null);
+        this.maxentTagger = maxentTagger;
+        if (maxentTagger.config != null) {
+            tagSeparator = maxentTagger.config.getTagSeparator();
+            encoding = maxentTagger.config.getEncoding();
+        } else {
+            tagSeparator = TaggerConfig.getDefaultTagSeparator();
+            encoding = "utf-8";
+        }
+        history = new History(pairs, maxentTagger.extractors);
+    }
 
-  public void setCorrectTags(List<? extends HasTag> sentence) {
-    int len = sentence.size();
-    correctTags = new String[len];
-    for (int i = 0; i < len; i++) {
-      correctTags[i] = sentence.get(i).tag();
+    void setCorrectTags(List<? extends HasTag> sentence) {
+        correctTags = sentence.stream().map(HasTag::tag).toArray(String[]::new);
     }
-  }
 
-  /**
-   * Tags the sentence s by running maxent model.  Returns a sentence (List) of
-   * TaggedWord objects.
-   *
-   * @param s Input sentence (List).  This isn't changed.
-   * @return Tagged sentence
-   */
-  public ArrayList<TaggedWord> tagSentence(List<? extends HasWord> s,
-                                           boolean reuseTags) {
-    this.origWords = new ArrayList<>(s);
-    int sz = s.size();
-    this.sent = new ArrayList<>(sz + 1);
-    for (HasWord value1 : s) {
-      if (maxentTagger.wordFunction != null) {
-        sent.add(maxentTagger.wordFunction.apply(value1.word()));
-      } else {
-        sent.add(value1.word());
-      }
-    }
-    sent.add(Tagger.EOS_WORD);
-    if (reuseTags) {
-      this.originalTags = new ArrayList<>(sz + 1);
-      for (HasWord value : s) {
-        if (value instanceof HasTag) {
-          originalTags.add(((HasTag) value).tag());
-        } else {
-          originalTags.add(null);
-        }
-      }
-      originalTags.add(Tagger.EOS_TAG);
-    }
-    size = sz + 1;
-    if (VERBOSE) {
-      log.info("Sentence: " + SentenceUtils.listToString(sent, false, tagSeparator));
-    }
-    init();
-    ArrayList<TaggedWord> result = testTagInference();
-    if (maxentTagger.wordFunction != null) {
-      for (int j = 0; j < sz; ++j) {
-        result.get(j).setWord(s.get(j).word());
-      }
-    }
-    return result;
-  }
+    /**
+     * Tags the sentence s by running maxent model.  Returns a sentence (List) of
+     * TaggedWord objects.
+     *
+     * @param s Input sentence (List).  This isn't changed.
+     * @return Tagged sentence
+     */
+    public ArrayList<TaggedWord> tagSentence(List<? extends HasWord> s,
+                                             boolean reuseTags) {
+        this.origWords = new ArrayList<>(s);
+        int sz = s.size();
+        this.sent = new ArrayList<>(sz + 1);
+        for (HasWord value1 : s) {
+            if (maxentTagger.wordFunction != null) {
+                sent.add(maxentTagger.wordFunction.apply(value1.word()));
+            } else {
+                sent.add(value1.word());
+            }
+        }
+        sent.add(Tagger.EOS_WORD);
+        if (reuseTags) {
+            this.originalTags = new ArrayList<>(sz + 1);
+            for (HasWord value : s) {
+                if (value instanceof HasTag) {
+                    originalTags.add(((HasTag) value).tag());
+                } else {
+                    originalTags.add(null);
+                }
+            }
+            originalTags.add(Tagger.EOS_TAG);
+        }
+        size = sz + 1;
+        init();
+        ArrayList<TaggedWord> result = testTagInference();
+        if (maxentTagger.wordFunction != null) {
+            for (int j = 0; j < sz; ++j) {
+                result.get(j).setWord(s.get(j).word());
+            }
+        }
+        return result;
+    }
 
-
-  protected void revert(int prevSize) {
-    endSizePairs = prevSize;
-  }
-
-  protected void init() {
-    //the eos are assumed already there
-    localContextScores = new double[size][];
-    for (int i = 0; i < size - 1; i++) {
-      if (maxentTagger.dict.isUnknown(sent.get(i))) {
-        numUnknown++;
-      }
+    protected void init() {
+        //the eos are assumed already there
+        localContextScores = new double[size][];
+        numUnknown += sent.stream().filter(maxentTagger.dict::isUnknown).count();
     }
-  }
-
-  /**
-   * Returns a string representation of the sentence.
-   * @return tagged sentence
-   */
-  String getTaggedNice() {
-    StringBuilder sb = new StringBuilder();
-    // size - 1 means to exclude the EOS (end of string) symbol
-    for (int i = 0; i < size - 1; i++) {
-      sb.append(toNice(sent.get(i))).append(tagSeparator).append(toNice(finalTags[i]));
-      sb.append(' ');
-    }
-    return sb.toString();
-  }
 
 
-  private ArrayList<TaggedWord> getTaggedSentence() {
-    final boolean hasOffset;
-    hasOffset = origWords != null && ! origWords.isEmpty() && (origWords.get(0) instanceof HasOffset);
-    ArrayList<TaggedWord> taggedSentence = new ArrayList<>();
-    for (int j = 0; j < size - 1; j++) {
-      String tag = finalTags[j];
-      TaggedWord w = new TaggedWord(sent.get(j), tag);
-      if (hasOffset) {
-        HasOffset offset = (HasOffset) origWords.get(j);
-        w.setBeginPosition(offset.beginPosition());
-        w.setEndPosition(offset.endPosition());
-      }
-      taggedSentence.add(w);
-    }
-    return taggedSentence;
-  }
+    private ArrayList<TaggedWord> getTaggedSentence() {
+        final boolean hasOffset;
+        hasOffset = origWords != null && !origWords.isEmpty() && (origWords.get(0) instanceof HasOffset);
+        ArrayList<TaggedWord> taggedSentence = new ArrayList<>();
+        for (int j = 0; j < size - 1; j++) {
+            String tag = finalTags[j];
+            TaggedWord w = new TaggedWord(sent.get(j), tag);
+            if (hasOffset) {
+                HasOffset offset = (HasOffset) origWords.get(j);
+                w.setBeginPosition(offset.beginPosition());
+                w.setEndPosition(offset.endPosition());
+            }
+            taggedSentence.add(w);
+        }
+        return taggedSentence;
+    }
 
-  static String toNice(String s) {
-    if (s == null) {
-      return naTag;
-    } else {
-      return s;
+    @Contract(value = "!null -> !null", pure = true)
+    static String toNice(String s) {
+        return Objects.requireNonNullElse(s, naTag);
     }
-  }
 
-  /** calculateProbs puts log probs of taggings in the probabilities array.
-   *
-   *  @param probabilities Array with indices sent size, k best size, numTags
-   */
-  protected void calculateProbs(double[][][] probabilities) {
-    ArrayUtils.fill(probabilities, Double.NEGATIVE_INFINITY);
-    for (int hyp = 0; hyp < kBestSize; hyp++) {
-      // put the whole thing in pairs, give its beginning and end
-      pairs.setSize(size);
-      for (int i = 0; i < size; i++) {
-        pairs.setWord(i,sent.get(i));
-        pairs.setTag(i,finalTags[i]);
-        //pairs.add(new WordTag(sent.get(i),finalTags[i]));
-        // TODO: if kBestSize > 1, use KBestSequenceFinder and save
-        // k-best hypotheses into finalTags:
-        //pairs.setTag(i,finalTags[i]);
-      }
-      int start = endSizePairs;
-      int end = endSizePairs + size - 1;
-      endSizePairs = endSizePairs + size;
-      // iterate over the sentence
-      for (int current = 0; current < size; current++) {
-        History h = new History(start, end, current + start, pairs, maxentTagger.extractors);
-        String[] tags = stringTagsAt(h.current - h.start + leftWindow());
-        double[] probs = getHistories(tags, h);
-        ArrayMath.logNormalize(probs);
 
-        for (int j = 0; j < tags.length; j++) {
-          // score the j-th tag
-          String tag = tags[j];
-          boolean approximate = maxentTagger.hasApproximateScoring();
-          int tagindex = approximate ? maxentTagger.tags.getIndex(tag) : j;
-          // log.info("Mapped from j="+ j + " " + tag + " to " + tagindex);
-          probabilities[current][hyp][tagindex] = probs[j];
-        }
-      } // for current
-    } // for hyp
-    // clean up the stuff in PairsHolder (added by cdm in Aug 2008)
-    revert(0);
-  } // end calculateProbs()
-
-
-  /** Write the tagging and note any errors (if pf != null) and accumulate
-   *  global statistics.
-   *
-   *  @param finalTags Chosen tags for sentence
-   *  @param pf File to write tagged output to (can be null, then no output;
-   *               at present it is non-null iff the debug property is set)
-   */
-  protected void writeTagsAndErrors(String[] finalTags, PrintFile pf, boolean verboseResults) {
-    StringWriter sw = new StringWriter(200);
-    for (int i = 0; i < correctTags.length; i++) {
-      sw.write(toNice(sent.get(i)));
-      sw.write(tagSeparator);
-      sw.write(finalTags[i]);
-      sw.write(' ');
-      if (pf != null) {
-        pf.print(toNice(sent.get(i)));
-        pf.print(tagSeparator);
-        pf.print(finalTags[i]);
-      }
-      if ((correctTags[i]).equals(finalTags[i])) {
-        numRight++;
-      } else {
-        numWrong++;
-        if (pf != null) pf.print('|' + correctTags[i]);
-        if (verboseResults) {
-          log.info((maxentTagger.dict.isUnknown(sent.get(i)) ? "Unk" : "") + "Word: " + sent.get(i) + "; correct: " + correctTags[i] + "; guessed: " + finalTags[i]);
-        }
+    /**
+     * Write the tagging and note any errors (if pf != null) and accumulate
+     * global statistics.
+     *
+     * @param pf File to write tagged output to (can be null, then no output;
+     *           at present it is non-null iff the debug property is set)
+     */
+    void writeTagsAndErrors(PrintFile pf, boolean verboseResults) {
+        StringWriter sw = new StringWriter(200);
+        for (int i = 0; i < correctTags.length; i++) {
+            sw.write(toNice(sent.get(i)));
+            sw.write(tagSeparator);
+            sw.write(finalTags[i]);
+            sw.write(' ');
+            if (pf != null) {
+                pf.print(toNice(sent.get(i)));
+                pf.print(tagSeparator);
+                pf.print(finalTags[i]);
+            }
+            if ((correctTags[i]).equals(finalTags[i])) {
+                numRight++;
+            } else {
+                numWrong++;
+                if (pf != null) pf.print('|' + correctTags[i]);
+                if (verboseResults) {
+                    log.info((maxentTagger.dict.isUnknown(sent.get(i)) ? "Unk" : "") + "Word: " + sent.get(i) + "; correct: " + correctTags[i] + "; guessed: " + finalTags[i]);
+                }
 
-        if (maxentTagger.dict.isUnknown(sent.get(i))) {
-          numWrongUnknown++;
-          if (pf != null) pf.print("*");
-        }// if
-      }// else
-      if (pf != null) pf.print(' ');
-    }// for
-    if (pf != null) pf.println();
+                if (maxentTagger.dict.isUnknown(sent.get(i))) {
+                    numWrongUnknown++;
+                    if (pf != null) pf.print("*");
+                }// if
+            }// else
+            if (pf != null) pf.print(' ');
+        }// for
+        if (pf != null) pf.println();
 
-    if (verboseResults) {
-      PrintWriter pw;
-      try {
-        pw = new PrintWriter(new OutputStreamWriter(System.out, encoding), true);
-      } catch (UnsupportedEncodingException uee) {
-        pw = new PrintWriter(new OutputStreamWriter(System.out), true);
-      }
-      pw.println(sw);
-    }
-  }
+        if (verboseResults) {
+            PrintWriter pw;
+            try {
+                pw = new PrintWriter(new OutputStreamWriter(System.out, encoding), true);
+            } catch (UnsupportedEncodingException uee) {
+                pw = new PrintWriter(new OutputStreamWriter(System.out), true);
+            }
+            pw.println(sw);
+        }
+    }
 
-  /**
-   * Update a confusion matrix with the errors from this sentence.
-   *
-   * @param finalTags Chosen tags for sentence
-   * @param confusionMatrix Confusion matrix to write to
-   */
-  protected void updateConfusionMatrix(String[] finalTags,
-                                       ConfusionMatrix<String> confusionMatrix) {
-    for (int i = 0; i < correctTags.length; i++)
-      confusionMatrix.add(finalTags[i], correctTags[i]);
-  }
+    /**
+     * Update a confusion matrix with the errors from this sentence.
+     *
+     * @param confusionMatrix Confusion matrix to write to
+     */
+    void updateConfusionMatrix(ConfusionMatrix<String> confusionMatrix) {
+        for (int i = 0; i < correctTags.length; i++)
+            confusionMatrix.add(finalTags[i], correctTags[i]);
+    }
 
 
-  /**
-   * Test using (exact Viterbi) TagInference.
-   *
-   * @return The tagged sentence
-   */
-  private ArrayList<TaggedWord> testTagInference() {
-    runTagInference();
-    return getTaggedSentence();
-  }
+    /**
+     * Test using (exact Viterbi) TagInference.
+     *
+     * @return The tagged sentence
+     */
+    private ArrayList<TaggedWord> testTagInference() {
+        runTagInference();
+        return getTaggedSentence();
+    }
 
-  private void runTagInference() {
-    this.initializeScorer();
-    if (Thread.interrupted()) {  // Allow interrupting
-      throw new RuntimeInterruptedException();
-    }
-
-    BestSequenceFinder ti = new ExactBestSequenceFinder();
-      //new BeamBestSequenceFinder(50);
-      //new KBestSequenceFinder()
-    int[] bestTags = ti.bestSequence(this);
-    finalTags = new String[bestTags.length];
-    for (int j = 0; j < size; j++) {
-      finalTags[j] = maxentTagger.tags.getTag(bestTags[j + leftWindow()]);
-    }
-
-    if (Thread.interrupted()) {  // Allow interrupting
-      throw new RuntimeInterruptedException();
+    private void runTagInference() {
+        initializeScorer();
+        BestSequenceFinder ti = new ExactBestSequenceFinder();
+        finalTags = Arrays.stream(ti.bestSequence(this)).boxed()
+                .map(i -> maxentTagger.tags.getTag(i)).toArray(String[]::new);
     }
-    cleanUpScorer();
-  }
-
 
-  // This is used for Dan's tag inference methods.
-  // current is the actual word number + leftW
-  private void setHistory(int current, History h, int[] tags) {
-    //writes over the tags in the last thing in pairs
-
-    int left = leftWindow();
-    int right = rightWindow();
+    // This is used for Dan's tag inference methods.
+    // current is the actual word number + leftW
+    protected void setHistory(int current, int[] tags) {
+        //writes over the tags in the last thing in pairs
+        int left = leftWindow();
+        int right = rightWindow();
 
-    for (int j = current - left; j <= current + right; j++) {
-      if (j < left) {
-        continue;
-      } //but shouldn't happen
-      if (j >= size + left) {
-        break;
-      } //but shouldn't happen
-      h.setTag(j - left, maxentTagger.tags.getTag(tags[j]));
-    }
-  }
+        for (int j = current - left; j <= current + right; j++) {
+            if (j < left) {
+                continue;
+            } //but shouldn't happen
+            if (j >= size + left) {
+                break;
+            } //but shouldn't happen
+            history.setTag(j - left, maxentTagger.tags.getTag(tags[j]));
+        }
+    }
 
-  // do initializations for the TagScorer interface
-  private void initializeScorer() {
-    pairs.setSize(size);
-    for (int i = 0; i < size; i++)
-      pairs.setWord(i,sent.get(i));
-    endSizePairs += size;
-  }
-
-
-  /**
-   * clean-up after the scorer
-   */
-  private void cleanUpScorer() {
-    revert(0);
-  }
+    // do initializations for the TagScorer interface
+    private void initializeScorer() {
+        pairs.setSize(size);
+        for (int i = 0; i < size; i++)
+            pairs.setWord(i, sent.get(i));
+    }
 
-  // This scores the current assignment in PairsHolder at
-  // current position h.current (returns normalized scores)
-  private double[] getScores(History h) {
-    if (maxentTagger.hasApproximateScoring()) {
-      if (DBG) { System.err.println("Tagger has approx scoring; "); }
-      return getApproximateScores(h);
+    // This scores the current assignment in PairsHolder at
+    // current position h.current (returns normalized scores)
+    private double[] getScores() {
+        String[] tags = getPossibleTagsAsString(history.current - history.start + leftWindow());
+        double[] histories = getAllScores();
+        // now we pick out the single values for the specific tags.
+        return Stream.of(tags).map(tag -> histories[maxentTagger.tags.indexOf(tag)])
+                .mapToDouble(d -> d).toArray();
     }
-    if (DBG) { System.err.println("Tagger has exact scoring"); }
-    return getExactScores(h);
-  }
 
-  private double[] getExactScores(History h) {
-    String[] tags = stringTagsAt(h.current - h.start + leftWindow());
-    double[] histories = getHistories(tags, h); // log score for each tag
-    ArrayMath.logNormalize(histories);
-    double[] scores = new double[tags.length];
-    for (int j = 0; j < tags.length; j++) {
-      // score the j-th tag
-      String tag = tags[j];
-      int tagindex = maxentTagger.tags.getIndex(tag);
-      scores[j] = histories[tagindex];
+    private double[] getAllScores() {
+        double[] histories = getHistories(); // log score for each tag
+        // tags is only used if we calculate approximate histories
+        ArrayMath.logNormalize(histories);
+        // assert Arrays.stream(histories).map(x-> Math.exp(x)).sum() == 1
+        return histories;
     }
-    return scores;
-  }
-
-  // In this method, each tag that is incompatible with the current word
-  // (e.g., apple_CC) gets a default (constant) score instead of its exact score.
-  // The scores of all other tags are computed exactly.
-  private double[] getApproximateScores(History h) {
-    String[] tags = stringTagsAt(h.current - h.start + leftWindow());
-    double[] scores = getHistories(tags, h); // log score for each active tag, unnormalized
-
-    // Number of tags that get assigned a default score:
-    int nDefault = maxentTagger.ySize - tags.length;
-    double logScore = ArrayMath.logSum(scores);
-    double logScoreInactiveTags = maxentTagger.getInactiveTagDefaultScore(nDefault);
-    double logTotal = SloppyMath.logAdd(logScore, logScoreInactiveTags);
-    ArrayMath.addInPlace(scores, -logTotal);
-
-    return scores;
-  }
 
-  // this is for the VERBOSE debugging code
-  private double[][] fullScores; // = null;
-
-  /** This computes scores of tags at a position in a sentence (the so called "History").
-   *  Usually, it precomputes scores of local features (localScores).
-   *  This is turned off if VERBOSE is set to make printing feature weights simpler....
-   *
-   * @param tags
-   * @param h
-   * @return
-   */
-  protected double[] getHistories(String[] tags, History h) {
-    boolean rare = maxentTagger.isRare(ExtractorFrames.cWord.extract(h));
-    Extractors ex = maxentTagger.extractors;
-    Extractors exR = maxentTagger.extractorsRare;
-    String w = pairs.getWord(h.current);
-    // if (DBG) { System.err.printf("%s: loc %s lc %s dy %s; rloc %s rlc %s rdy %s%n", w, ex.local, ex.localContext, ex.dynamic, exR.local, exR.localContext, exR.dynamic); }
-
-    if (VERBOSE) {
-      // Good options to print out what is calculated here are: -debug -verbose -verboseResults false -approximate false
-      int extractorsSize = rare ? ex.size() + exR.size() : ex.size();
-      fullScores = new double[extractorsSize][maxentTagger.ySize];
-
-      List<Pair<Integer, Extractor>> allEx = new ArrayList<>(ex.local);
-      allEx.addAll(ex.localContext);
-      allEx.addAll(ex.dynamic);
-      List<Pair<Integer, Extractor>> allExR = new ArrayList<>();
-      if (rare) {
-        allExR.addAll(exR.local);
-        allExR.addAll(exR.localContext);
-        allExR.addAll(exR.dynamic);
-      }
-
-      // = null;
-      ArrayList<String> extractorVals = new ArrayList<>();
-      for (int i = 0; i < extractorsSize; i++) {
-        extractorVals.add("foo");
-      }
-      for (Pair<Integer, Extractor> pair : allEx) {
-        int kf = pair.first();
-        Extractor e = pair.second();
-        String val = e.extract(h);
-        extractorVals.set(kf, e + " " + val);
-      }
-      for (Pair<Integer, Extractor> pair : allExR) {
-        int kf = pair.first();
-        Extractor e = pair.second();
-        String val = e.extract(h);
-        extractorVals.set(kf + ex.size(), e + " " + val);
-      }
-
-      double[] totalS = getHistories(tags, h, allEx, rare ? allExR : null);
-
-      NumberFormat nf = new DecimalFormat("0.00");
-      Object[] colNames = IntStream.range(0, maxentTagger.ySize).mapToObj(k -> maxentTagger.tags.getTag(k)).toArray();
-      System.err.println(ArrayMath.toString(fullScores, 6, extractorVals.toArray(), colNames,
-              48, nf, false, true, w));
-      return totalS;
-    } // end if (VERBOSE) case
+    /**
+     * This computes scores of tags at a position in a sentence (the so called "History").
+     */
+    private double[] getHistories() {
+        String[] tags = new String[]{};
+        boolean rare = maxentTagger.isRare(ExtractorFrames.cWord.extract(history));
+        Extractors ex = maxentTagger.extractors;
+        Extractors exR = maxentTagger.extractorsRare;
+        String w = pairs.getWord(history.current);
 
-    double[] lS = localScores.get(w);
-    if (lS == null) {
-      lS = getHistories(tags, h, ex.local, rare ? exR.local : null);
-      localScores.put(w, lS);
-    } else if (lS.length != tags.length) {
-      // This case can occur when a word was given a specific forced
-      // tag, and then later it shows up without the forced tag.
-      // TODO: if a word is given a forced tag, we should always get
-      // its features rather than use the cache, just in case the tag
-      // given is not the same tag as before
-      lS = getHistories(tags, h, ex.local, rare ? exR.local : null);
-      if (tags.length > 1) {
-        localScores.put(w, lS);
-      }
-    }
-    double[] lcS = localContextScores[h.current];
-    if (lcS == null) {
-      lcS = getHistories(tags, h, ex.localContext, rare ? exR.localContext : null);
-      localContextScores[h.current] = lcS;
-      ArrayMath.pairwiseAddInPlace(lcS, lS);
-    }
-    double[] totalS = getHistories(tags, h, ex.dynamic, rare ? exR.dynamic : null);
-    ArrayMath.pairwiseAddInPlace(totalS, lcS);
-    return totalS;
-  }
+        double[] lS = localScores.get(w);
+        if (lS == null) {
+            lS = getHistories(ex.local, rare ? exR.local : null);
+            localScores.put(w, lS);
+        } else if (lS.length != tags.length) {
+            // This case can occur when a word was given a specific forced
+            // tag, and then later it shows up without the forced tag.
+            // TODO: if a word is given a forced tag, we should always get
+            // its features rather than use the cache, just in case the tag
+            // given is not the same tag as before
+            lS = getHistories(ex.local, rare ? exR.local : null);
+        }
+        double[] lcS = localContextScores[history.current];
+        if (lcS == null) {
+            lcS = getHistories(ex.localContext, rare ? exR.localContext : null);
+            localContextScores[history.current] = lcS;
+            ArrayMath.pairwiseAddInPlace(lcS, lS);
+        }
+        double[] totalS = getHistories(ex.dynamic, rare ? exR.dynamic : null);
+        ArrayMath.pairwiseAddInPlace(totalS, lcS);
+        return totalS;
+    }
 
-  private double[] getHistories(String[] tags, History h, List<Pair<Integer,Extractor>> extractors, List<Pair<Integer,Extractor>> extractorsRare) {
-    if (maxentTagger.hasApproximateScoring()) {
-      return getApproximateHistories(tags, h, extractors, extractorsRare);
-    }
-    return getExactHistories(h, extractors, extractorsRare);
-  }
-
-  private double[] getExactHistories(History h, List<Pair<Integer,Extractor>> extractors, List<Pair<Integer,Extractor>> extractorsRare) {
-    double[] scores = new double[maxentTagger.ySize];
-    for (Pair<Integer,Extractor> e : extractors) {
-      int kf = e.first();
-      Extractor ex = e.second();
-      String val = ex.extract(h);
-      int[] fAssociations = maxentTagger.fAssociations.get(kf).get(val);
-      if (fAssociations != null) {
-        for (int j = 0; j < maxentTagger.ySize; j++) {
-          int fNum = fAssociations[j];
-          if (fNum > -1) {
-            double score = maxentTagger.getLambdaSolve().lambda[fNum];
-            if (VERBOSE) {
-              fullScores[kf][j] = score;
-            }
-            scores[j] += score;
-          }
+    /**
+     * @param extractors
+     * @param extractorsRare
+     * @return
+     */
+    private double[] getHistories(List<Pair<Integer, Extractor>> extractors, List<Pair<Integer, Extractor>> extractorsRare) {
+        double[] scores = new double[maxentTagger.ySize];
+        for (Pair<Integer, Extractor> e : extractors) {
+            addScoresForExtractor(scores, 0, e.first(), e.second());
         }
-      }
-    }
-    if (extractorsRare != null) {
-      int szCommon = maxentTagger.extractors.size();  // needs to be full size list of extractors not subset of some type
-      for (Pair<Integer,Extractor> e : extractorsRare) {
-        int kf = e.first();
-        Extractor ex = e.second();
-        String val = ex.extract(h);
+        if (extractorsRare != null) {
+            int szCommon = maxentTagger.extractors.size();  // needs to be full size list of extractors not subset of some type
+            for (Pair<Integer, Extractor> e : extractorsRare) {
+                addScoresForExtractor(scores, szCommon, e.first(), e.second());
+            }
+        }
+        return scores;
+    }
 
-        int[] fAssociations = maxentTagger.fAssociations.get(kf+szCommon).get(val);
-//        String word = h.pairs.getWord(h, 0);
-//        if (kf + szCommon == 30) { // ExtractorWordPref(len5,w0)
-//          System.err.print(ex + ": word=" + word + ", val=" + val);
-//          if (fAssociations == null) {
-//            System.err.println(", fAssociations is NULL");
-//          } else {
-//            System.err.println(", fAssociations = " + Arrays.toString(fAssociations));
-//          }
-//        }
+    private void addScoresForExtractor(double[] scores, int szCommon, int kf, Extractor ex) {
+        String val = ex.extract(history);
+        int[] fAssociations = maxentTagger.fAssociations.get(kf + szCommon).get(val);
+
         if (fAssociations != null) {
-          for (int j = 0; j < maxentTagger.ySize; j++) {
-            int fNum = fAssociations[j];
-            if (fNum > -1) {
-              double score = maxentTagger.getLambdaSolve().lambda[fNum];
-              if (VERBOSE) {
-                fullScores[kf+szCommon][j] = score;
-              }
-              scores[j] += score;
-            }
-          }
-        }
-      }
-    }
-    return scores;
-  }
-
-  // todo [cdm 2016]: Could this be sped up a bit by caching lambda array, extracting method for shared code?
-  // todo [cdm 2016]: Also it's allocating java.util.ArrayList$Itr for for loop - why can't it just random access array?
-  /** Returns an unnormalized score (in log space) for each tag. */
-  private double[] getApproximateHistories(String[] tags, History h, List<Pair<Integer,Extractor>> extractors, List<Pair<Integer,Extractor>> extractorsRare) {
-
-    double[] scores = new double[tags.length];
-    int szCommon = maxentTagger.extractors.size();
-    if (VERBOSE) { System.err.println("Calling approx histories"); }
-
-    for (Pair<Integer,Extractor> e : extractors) {
-      int kf = e.first();
-      Extractor ex = e.second();
-      String val = ex.extract(h);
-      int[] fAssociations = maxentTagger.fAssociations.get(kf).get(val);
-      if (fAssociations != null) {
-        for (int j = 0; j < tags.length; j++) {
-          String tag = tags[j];
-          int tagIndex = maxentTagger.tags.getIndex(tag);
-          int fNum = fAssociations[tagIndex];
-          if (fNum > -1) {
-            double score = maxentTagger.getLambdaSolve().lambda[fNum];
-            if (VERBOSE) {
-              fullScores[kf][j] = score;
-            }
-            scores[j] += score;
-          }
-        }
-      }
+            for (int j = 0; j < maxentTagger.ySize; j++) {
+                int fNum = fAssociations[j];
+                if (fNum > -1) {
+                    double score = maxentTagger.getLambdaSolve().lambda[fNum];
+                    scores[j] += score;
+                }
+            }
+        }
     }
-    if (extractorsRare != null) {
-      for (Pair<Integer,Extractor> e : extractorsRare) {
-        int kf = e.first();
-        Extractor ex = e.second();
-        String val = ex.extract(h);
-        int[] fAssociations = maxentTagger.fAssociations.get(szCommon+kf).get(val);
-        if (fAssociations != null) {
-          for (int j = 0; j < tags.length; j++) {
-            String tag = tags[j];
-            int tagIndex = maxentTagger.tags.getIndex(tag);
-            int fNum = fAssociations[tagIndex];
-            if (fNum > -1) {
-              double score = maxentTagger.getLambdaSolve().lambda[fNum];
-              if (VERBOSE) {
-                fullScores[kf+szCommon][j] = score;
-              }
-              scores[j] += score;
-            }
-          }
-        }
-      }
-    }
-    return scores;
-  }
-
-
-  /**
-   * This method should be called after the sentence has been tagged.
-   * For every unknown word, this method prints the 3 most probable tags
-   * to the file pfu.
-   *
-   * @param numSent The sentence number
-   * @param pfu The file to print the probable tags to
-   */
-  void printUnknown(int numSent, PrintFile pfu) {
-    NumberFormat nf = new DecimalFormat("0.0000");
-    int numTags = maxentTagger.numTags();
-    double[][][] probabilities = new double[size][kBestSize][numTags];
-    calculateProbs(probabilities);
-    for (int current = 0; current < size; current++) {
-      if (maxentTagger.dict.isUnknown(sent.get(current))) {
-        pfu.print(sent.get(current));
-        pfu.print(':');
-        pfu.print(numSent);
-        double[] probs = new double[3];
-        String[] tag3 = new String[3];
-        getTop3(probabilities, current, probs, tag3);
-        for (int i = 0; i < 3; i++) {
-          if (probs[i] > Double.NEGATIVE_INFINITY) {
-            pfu.print('\t');
-            pfu.print(tag3[i]);
-            pfu.print(' ');
-            pfu.print(nf.format(Math.exp(probs[i])));
-          }
-        }
-        int rank;
-        String correctTag = toNice(this.correctTags[current]);
-        for (rank = 0; rank < 3; rank++) {
-          if (correctTag.equals(tag3[rank])) {
-            break;
-          } //if
-        }
-        pfu.print('\t');
-        switch (rank) {
-          case 0:
-            pfu.print("Correct");
-            break;
-          case 1:
-            pfu.print("2nd");
-            break;
-          case 2:
-            pfu.print("3rd");
-            break;
-          default:
-            pfu.print("Not top 3");
-        }
-        pfu.println();
-      }// if
-    }// for
-  }
-
-  // This method should be called after a sentence has been tagged.
-  // For every word token, this method prints the 3 most probable tags
-  // to the file pfu except for
-  void printTop(PrintFile pfu) {
-    NumberFormat nf = new DecimalFormat("0.0000");
-    int numTags = maxentTagger.numTags();
-    double[][][] probabilities = new double[size][kBestSize][numTags];
-    calculateProbs(probabilities);
 
-    for (int current = 0; current < correctTags.length; current++) {
-      pfu.print(sent.get(current));
-      double[] probs = new double[3];
-      String[] tag3 = new String[3];
-      getTop3(probabilities, current, probs, tag3);
-      for (int i = 0; i < 3; i++) {
-        if (probs[i] > Double.NEGATIVE_INFINITY) {
-          pfu.print('\t');
-          pfu.print(tag3[i]);
-          pfu.print(' ');
-          pfu.print(nf.format(Math.exp(probs[i])));
-        }
-      }
-      int rank;
-      String correctTag = toNice(this.correctTags[current]);
-      for (rank = 0; rank < 3; rank++) {
-        if (correctTag.equals(tag3[rank])) {
-          break;
-        } //if
-      }
-      pfu.print('\t');
-      switch (rank) {
-      case 0:
-        pfu.print("Correct");
-        break;
-      case 1:
-        pfu.print("2nd");
-        break;
-      case 2:
-        pfu.print("3rd");
-        break;
-      default:
-        pfu.print("Not top 3");
-      }
-      pfu.println();
-    } // for
-  }
-
-  /** probs and tags should be passed in as arrays of size 3!
-   *  If probs[i] == Double.NEGATIVE_INFINITY, then the entry should be ignored.
-   */
-  private void getTop3(double[][][] probabilities, int current, double[] probs, String[] tags) {
-    int[] topIds = new int[3];
-    double[] probTags = probabilities[current][0];
-    Arrays.fill(probs, Double.NEGATIVE_INFINITY);
-    for (int i = 0; i < probTags.length; i++) {
-      if (probTags[i] > probs[0]) {
-        probs[2] = probs[1];
-        probs[1] = probs[0];
-        probs[0] = probTags[i];
-        topIds[2] = topIds[1];
-        topIds[1] = topIds[0];
-        topIds[0] = i;
-      } else if (probTags[i] > probs[1]) {
-        probs[2] = probs[1];
-        probs[1] = probTags[i];
-        topIds[2] = topIds[1];
-        topIds[1] = i;
-      } else if (probTags[i] > probs[2]) {
-        probs[2] = probTags[i];
-        topIds[2] = i;
-      }
-    }
-    for (int j = 0; j < 3; j++) {
-      tags[j] = toNice(maxentTagger.tags.getTag(topIds[j]));
-    }
-  }
 
-  /*
-   * Implementation of the TagScorer interface follows
-   */
+    /*
+     * Implementation of the TagScorer interface follows
+     */
 
-  @Override
-  public int length() {
-    return sent.size();
-  }
+    @Override
+    public int length() {
+        return sent.size();
+    }
 
-  @Override
-  public int leftWindow() {
-    return maxentTagger.leftContext; //hard-code for now
-  }
+    @Override
+    public int leftWindow() {
+        return maxentTagger.leftContext; //hard-code for now
+    }
 
-  @Override
-  public int rightWindow() {
-    return maxentTagger.rightContext; //hard code for now
-  }
+    @Override
+    public int rightWindow() {
+        return maxentTagger.rightContext; //hard code for now
+    }
 
 
-  @Override
-  public int[] getPossibleValues(int pos) {
-    String[] arr1 = stringTagsAt(pos);
-    int[] arr = new int[arr1.length];
-    for (int i = 0; i < arr.length; i++) {
-      arr[i] = maxentTagger.tags.getIndex(arr1[i]);
+    @Override
+    public int[] getPossibleValues(int pos) {
+        return Stream.of(getPossibleTagsAsString(pos))
+                .map(tag -> maxentTagger.tags.indexOf(tag))
+                .mapToInt(x -> x).toArray();
     }
+
+    // todo [cdm 2013]: Tagging could be sped up quite a bit here if we cached int arrays of tags by index, not Strings
+    public String[] getPossibleTagsAsString(int pos) {
+        pos -= leftWindow();
+        // if word in padding part, return NA tag array
+        if (!(0 <= pos && pos < size)) {
+            return naTagArr;
+        }
+
+        if (!doConstraintTagSet) {
+            return maxentTagger.tags.tagSet().toArray(new String[0]);
+        }
+
+        // reuse supplied tags. this means each word contains only one tag, which is the supplied one.
+        if (originalTags != null && originalTags.get(pos) != null) {
+            return new String[]{originalTags.get(pos - leftWindow())};
+        }
+
+        String[] arr1;
+        String word = sent.get(pos);
+        if (maxentTagger.dict.isUnknown(word)) {
+            // if word is unknown we assume all open tags
+            // todo: really want array of String or int here
+            arr1 = maxentTagger.tags.getOpenTags().toArray(StringUtils.EMPTY_STRING_ARRAY);
+        } else {
+            // if the word is known we assume it can only take tags that we seen it with during training
+            arr1 = maxentTagger.dict.getTags(word);
+        }
+        // we expand the tags
+        String[] tags = maxentTagger.tags.deterministicallyExpandTags(arr1);
 
-    return arr;
-  }
+        // filter for tags that are known during training! otherwise we run in out of bounds exceptions
+        // ideally the filtering does not have an effect.
+        return Stream.of(tags).filter(tag -> -1 < maxentTagger.tags.indexOf(tag)).toArray(String[]::new);
+    }
 
-  @Override
-  public double scoreOf(int[] tags, int pos) {
-    double[] scores = scoresOf(tags, pos);
-    double score = Double.NEGATIVE_INFINITY;
-    int[] pv = getPossibleValues(pos);
-    for (int i = 0; i < scores.length; i++) {
-      if (pv[i] == tags[pos]) {
-        score = scores[i];
-      }
-    }
-    return score;
-  }
+    @Override
+    public double scoreOf(int[] tags, int pos) {
+        double[] scores = scoresOf(tags, pos);
+        double score = Double.NEGATIVE_INFINITY;
+        int[] pv = getPossibleValues(pos);
+        for (int i = 0; i < scores.length; i++) {
+            if (pv[i] == tags[pos]) {
+                score = scores[i];
+            }
+        }
+        return score;
+    }
 
-  @Override
-  public double scoreOf(int[] sequence) {
-    throw new UnsupportedOperationException();
-  }
+    @Override
+    public double scoreOf(int[] sequence) {
+        throw new UnsupportedOperationException();
+    }
 
-  @Override
-  public double[] scoresOf(int[] tags, int pos) {
-    if (DBG) {
-      log.info("scoresOf(): length of tags is " + tags.length + "; position is " + pos + "; endSizePairs = " + endSizePairs + "; size is " + size + "; leftWindow is " + leftWindow());
-      log.info("  History h = new History(" + (endSizePairs - size) + ", " + (endSizePairs - 1) + ", " + (endSizePairs - size + pos - leftWindow()) + ')');
-    }
-    history.init(endSizePairs - size, endSizePairs - 1, endSizePairs - size + pos - leftWindow());
-    setHistory(pos, history, tags);
-    return getScores(history);
-  }
-
-  // todo [cdm 2013]: Tagging could be sped up quite a bit here if we cached int arrays of tags by index, not Strings
-  protected String[] stringTagsAt(int pos) {
-    if ((pos < leftWindow()) || (pos >= size + leftWindow())) {
-      return naTagArr;
+
+    @Override
+    public double[] scoresOf(int[] contextTags, int pos) {
+        return scoresOf(contextTags, pos, true);
     }
 
-    String[] arr1;
-    if (originalTags != null && originalTags.get(pos - leftWindow()) != null) {
-      arr1 = new String[1];
-      arr1[0] = originalTags.get(pos - leftWindow());
-      return arr1;
-    }
+    public double[] scoresOf(int[] contextTags, int pos, boolean constrainToPossibleTags) {
+        // updating the history variable
+        history.updatePointers(0, size - 1, pos - leftWindow());
+        setHistory(pos, contextTags);
 
-    String word = sent.get(pos - leftWindow());
-    if (maxentTagger.dict.isUnknown(word)) {
-      Set<String> open = maxentTagger.tags.getOpenTags();  // todo: really want array of String or int here
-      arr1 = open.toArray(StringUtils.EMPTY_STRING_ARRAY);
-    } else {
-      arr1 = maxentTagger.dict.getTags(word);
-    }
-    arr1 = maxentTagger.tags.deterministicallyExpandTags(arr1);
-    return arr1;
-  }
-
-}
+        if (constrainToPossibleTags) {
+            // calculating scores with respect to the history
+            return getScores();
+        }
+        return getAllScores();
+    }
+}
\ No newline at end of file
Index: src/edu/stanford/nlp/tagger/maxent/ExtractorFramesRare.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/ExtractorFramesRare.java	(revision a98f8474dc2ac9a4f9dbe8f9b1282653cb248ba8)
+++ src/edu/stanford/nlp/tagger/maxent/ExtractorFramesRare.java	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
@@ -534,7 +534,7 @@
  */
 class RareExtractor extends Extractor {
 
-  static final String naTag = "NA";
+  static final String naTag = Defaults.naTag;
 
   RareExtractor() {
     super();
@@ -1449,7 +1449,7 @@
 
   @Override
   String extract(History h, PairsHolder pH) {
-    String s = TestSentence.toNice(pH.getWord(h, position));
+    String s = BaseTagger.toNice(pH.getWord(h, position));
 
     if ( ! s.isEmpty() && CtbDict.getTagPre(t1, s.substring(0, 1)).equals("1"))
       return "1:"+t1;
@@ -1480,7 +1480,7 @@
 
   @Override
   String extract(History h, PairsHolder pH) {
-    String s=TestSentence.toNice(pH.getWord(h, position));
+    String s= BaseTagger.toNice(pH.getWord(h, position));
 
     if(!s.isEmpty() && CtbDict.getTagSuf(t1, s.substring(s.length()-1)).equals("1"))
       return "1:"+t1;
@@ -1550,7 +1550,7 @@
 
   @Override
   String extract(History h, PairsHolder pH) {
-    String s=TestSentence.toNice(pH.getWord(h,n1));
+    String s= BaseTagger.toNice(pH.getWord(h,n1));
 
     if (ASBCunkDict.getTag(t1, s).equals("1"))
       return "1:"+t1;
@@ -1576,7 +1576,7 @@
 
   @Override
   String extract(History h, PairsHolder pH) {
-    String s=TestSentence.toNice(pH.getWord(h,n1));
+    String s= BaseTagger.toNice(pH.getWord(h,n1));
 
     if (CTBunkDict.getTag(t1, s).equals("1"))
       return "1:"+t1;
Index: src/edu/stanford/nlp/tagger/maxent/TestClassifier.java
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/TestClassifier.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/TestClassifier.kt	(revision f3bf217addd8a9e5a5e4216a488f94870df00bec)
@@ -1,213 +1,156 @@
-package edu.stanford.nlp.tagger.maxent; 
+package edu.stanford.nlp.tagger.maxent
 
-import java.io.IOException;
-import java.util.List;
+import java.io.IOException
 
-import edu.stanford.nlp.io.PrintFile;
-import edu.stanford.nlp.ling.TaggedWord;
-import edu.stanford.nlp.tagger.io.TaggedFileRecord;
-import edu.stanford.nlp.util.ConfusionMatrix;
-import edu.stanford.nlp.util.concurrent.MulticoreWrapper;
-import edu.stanford.nlp.util.concurrent.ThreadsafeProcessor;
-import edu.stanford.nlp.util.logging.Redwood;
+import edu.stanford.nlp.io.PrintFile
+import edu.stanford.nlp.ling.TaggedWord
+import edu.stanford.nlp.tagger.io.TaggedFileRecord
+import edu.stanford.nlp.util.ConfusionMatrix
+import edu.stanford.nlp.util.concurrent.MulticoreWrapper
+import edu.stanford.nlp.util.concurrent.ThreadsafeProcessor
+import edu.stanford.nlp.util.logging.Redwood
 
-/** Tags data and can handle either data with gold-standard tags (computing
- *  performance statistics) or unlabeled data.
+/**
+ * Tags data and can handle either data with gold-standard tags (computing
+ * performance statistics) or unlabeled data.
  *
- *  @author Kristina Toutanova
- *  @version 1.0
+ * @author Kristina Toutanova
+ * @version 1.0
  */
 // TODO: can we break this class up in some way?  Perhaps we can
 // spread some functionality into TestSentence and some into MaxentTagger
 // TODO: at the very least, it doesn't seem to make sense to make it
 // an object with state, rather than just some static methods
-public class TestClassifier  {
-
-  /** A logger for this class */
-  private static final Redwood.RedwoodChannels log = Redwood.channels(TestClassifier.class);
-
-  private final TaggedFileRecord fileRecord;
-  private int numRight;
-  private int numWrong;
-  private int unknownWords;
-  private int numWrongUnknown;
-  private int numCorrectSentences;
-  private int numSentences;
-
-  private ConfusionMatrix<String> confusionMatrix;
-
-  // TODO: only one boolean here instead of 4? They all use the same debug status.
-  private boolean writeUnknDict;
-  private boolean writeWords;
-  private boolean writeTopWords;
-  private boolean writeConfusionMatrix;
-
-  private MaxentTagger maxentTagger;
-  TaggerConfig config;
-  private String saveRoot;
-
-  public TestClassifier(MaxentTagger maxentTagger) throws IOException {
-    this(maxentTagger, maxentTagger.config.getFile());
-  }
-
-  public TestClassifier(MaxentTagger maxentTagger, String testFile) throws IOException {
-    this.maxentTagger = maxentTagger;
-    this.config = maxentTagger.config;
-    setDebug(config.getDebug());
-
-    fileRecord = TaggedFileRecord.createRecord(config, testFile);
-
-    saveRoot = config.getDebugPrefix();
-    if (saveRoot == null || saveRoot.isEmpty()) {
-      saveRoot = fileRecord.filename();
-    }
-
-    test();
+class TestClassifier @Throws(IOException::class)
+@JvmOverloads constructor(private val maxentTagger: MaxentTagger, testFile: String = maxentTagger.config.file) {
+    private var numRight: Int = 0
+    private var numWrong: Int = 0
+    val numWords: Int
+        get() = numRight + numWrong
 
-    if (writeConfusionMatrix) {
-      PrintFile pf = new PrintFile(saveRoot + ".confusion");
-      pf.print(confusionMatrix);
-      pf.close();
-    }
-  }
+    private var unknownWords: Int = 0
+    private var numWrongUnknown: Int = 0
+    private var numCorrectSentences: Int = 0
+    private var numSentences: Int = 0
 
-  private void processResults(TestSentence testS,
-                              PrintFile unknDictFile,
-                              PrintFile topWordsFile, boolean verboseResults) {
-    numSentences++;
+    private var confusionMatrix: ConfusionMatrix<String> = ConfusionMatrix()
+    private val config: TaggerConfig = maxentTagger.config
+    private var writeDebug: Boolean = true or config.debug
+    private val fileRecord = TaggedFileRecord.createRecord(config, testFile)
+    private var saveRoot: String = config.debugPrefix
 
-    testS.writeTagsAndErrors(testS.finalTags, unknDictFile, verboseResults);
-    if (writeUnknDict) testS.printUnknown(numSentences, unknDictFile);
-    if (writeTopWords) testS.printTop(topWordsFile);
-
-    testS.updateConfusionMatrix(testS.finalTags, confusionMatrix);
-
-    numWrong = numWrong + testS.numWrong;
-    numRight = numRight + testS.numRight;
-    unknownWords = unknownWords + testS.numUnknown;
-    numWrongUnknown = numWrongUnknown + testS.numWrongUnknown;
-    if (testS.numWrong == 0) {
-      numCorrectSentences++;
-    }
-    if (verboseResults) {
-      log.info("Sentence number: " + numSentences + "; length " + (testS.size-1) +
-                         "; correct: " + testS.numRight + "; wrong: " + testS.numWrong +
-                         "; unknown wrong: " + testS.numWrongUnknown);
-      // log.info("  Total tags correct: " + numRight + "; wrong: " + numWrong +
-      //                    "; unknown wrong: " + numWrongUnknown);
-    }
-  }
 
-  /**
-   * Test on a file containing correct tags already. when init'ing from trees
-   * TODO: Add the ability to have a second transformer to transform output back; possibly combine this method
-   * with method below
-   */
-  private void test() throws IOException {
-    numSentences = 0;
-    confusionMatrix = new ConfusionMatrix<>();
-
-    PrintFile pf = null;
-    PrintFile pf1 = null;
-    PrintFile pf3 = null;
+    /**
+     * Test on a file containing correct tags already. when updatePointers'ing from trees
+     * TODO: Add the ability to have a second transformer to transform output back; possibly combine this method
+     * with method below
+     */
+    @Throws(IOException::class)
+    fun test() {
+        var pf: PrintFile? = null
+        if (writeDebug) pf = PrintFile("$saveRoot/5_corenlp.csv")
 
-    if (writeWords) pf = new PrintFile(saveRoot + ".words");
-    if (writeUnknDict) pf1 = new PrintFile(saveRoot + ".un.dict");
-    if (writeTopWords) pf3 = new PrintFile(saveRoot + ".words.top");
+        val verboseResults = config.verboseResults
 
-    boolean verboseResults = config.getVerboseResults();
-
-    if (config.getNThreads() != 1) {
-      MulticoreWrapper<List<TaggedWord>, TestSentence> wrapper = new MulticoreWrapper<>(config.getNThreads(), new TestSentenceProcessor(maxentTagger));
-      for (List<TaggedWord> taggedSentence : fileRecord.reader()) {
-        wrapper.put(taggedSentence);
-        while (wrapper.peek()) {
-          processResults(wrapper.poll(), pf1, pf3, verboseResults);
-        }
-      }
-      wrapper.join();
-      while (wrapper.peek()) {
-        processResults(wrapper.poll(), pf1, pf3, verboseResults);
-      }
-    } else{
-      for (List<TaggedWord> taggedSentence : fileRecord.reader()) {
-        TestSentence testS = new TestSentence(maxentTagger);
-        testS.setCorrectTags(taggedSentence);
-        testS.tagSentence(taggedSentence, false);
-        processResults(testS, pf1, pf3, verboseResults);
-      }
-    }
+        if (config.nThreads != 1) {
+            val wrapper = MulticoreWrapper(config.nThreads, TestSentenceProcessor(maxentTagger))
+            for (taggedSentence in fileRecord.reader()) {
+                wrapper.put(taggedSentence)
+                while (wrapper.peek()) {
+                    processResults(wrapper.poll()!!, pf, verboseResults)
+                }
+            }
+            wrapper.join()
+            while (wrapper.peek()) {
+                processResults(wrapper.poll()!!, pf, verboseResults)
+            }
+        } else {
+            for (taggedSentence in fileRecord.reader()) {
+                // TODO: Change to other tagger
+                val testS = BaseSetTagger(maxentTagger)
+                testS.setCorrectTags(taggedSentence)
+                testS.tagSentence(taggedSentence, false)
+                processResults(testS, pf, verboseResults)
+            }
+        }
 
-    if (pf != null) pf.close();
-    if (pf1 != null) pf1.close();
-    if (pf3 != null) pf3.close();
-  }
+        val tags = maxentTagger.tags.tagSet().sortedBy { maxentTagger.tags.indexOf(it) }.toTypedArray()
+
+        val pf2 = PrintFile("$saveRoot/5_tags.csv")
+        pf2.println(tags.joinToString())
+        pf2.close()
+        println("Open Tags: " + maxentTagger.tags.openTags.joinToString())
+
+        pf?.close()
+    }
 
+    private fun processResults(testS: BaseTagger, debugFile: PrintFile?, verboseResults: Boolean) {
+        numSentences++
+
+        testS.writeTagsAndErrors(debugFile, verboseResults)
+        testS.updateConfusionMatrix(confusionMatrix)
+        //println(resultsString(maxentTagger))
 
-  String resultsString(MaxentTagger maxentTagger) {
-    StringBuilder output = new StringBuilder();
-    output.append(String.format("Model %s has xSize=%d, ySize=%d, and numFeatures=%d.%n",
-            maxentTagger.config.getModel(),
-            maxentTagger.xSize,
-            maxentTagger.ySize,
-            maxentTagger.getLambdaSolve().lambda.length));
-    output.append(String.format("Results on %d sentences and %d words, of which %d were unknown.%n",
-            numSentences, numRight + numWrong, unknownWords));
-    output.append(String.format("Total sentences right: %d (%f%%); wrong: %d (%f%%).%n",
-                                numCorrectSentences, numCorrectSentences * 100.0 / numSentences,
-                                numSentences - numCorrectSentences,
-                                (numSentences - numCorrectSentences) * 100.0 / (numSentences)));
-    output.append(String.format("Total tags right: %d (%f%%); wrong: %d (%f%%).%n",
-                                numRight, numRight * 100.0 / (numRight + numWrong), numWrong,
-                                numWrong * 100.0 / (numRight + numWrong)));
+        numWrong += testS.numWrong
+        numRight += testS.numRight
+        unknownWords += testS.numUnknown
+        numWrongUnknown += testS.numWrongUnknown
+        if (testS.numWrong == 0) {
+            numCorrectSentences++
+        }
+        if (verboseResults) {
+            log.info("Sentence number: $numSentences; length ${testS.size - 1}; " +
+                    "correct: ${testS.numRight}; wrong: ${testS.numWrong}; unknown wrong: ${testS.numWrongUnknown}")
+        }
+    }
+
+    private fun resultsString(maxentTagger: MaxentTagger): String {
+        val output = StringBuilder()
+        output.append(String.format("Model %s has xSize=%d, ySize=%d, and numFeatures=%d.%n",
+                maxentTagger.config.model,
+                maxentTagger.xSize,
+                maxentTagger.ySize,
+                maxentTagger.lambdaSolve.lambda.size))
+        output.append(String.format("Results on %d sentences and %d words, of which %d were unknown.%n",
+                numSentences, numRight + numWrong, unknownWords))
+        output.append(String.format("Total sentences right: %d (%f%%); wrong: %d (%f%%).%n",
+                numCorrectSentences, numCorrectSentences * 100.0 / numSentences,
+                numSentences - numCorrectSentences,
+                (numSentences - numCorrectSentences) * 100.0 / numSentences))
+        output.append(String.format("Total tags right: %d (%f%%); wrong: %d (%f%%).%n",
+                numRight, numRight * 100.0 / (numRight + numWrong), numWrong,
+                numWrong * 100.0 / (numRight + numWrong)))
 
-    if (unknownWords > 0) {
-      output.append(String.format("Unknown words right: %d (%f%%); wrong: %d (%f%%).%n",
-                                  (unknownWords - numWrongUnknown),
-                                  100.0 - (numWrongUnknown * 100.0 / unknownWords),
-                                  numWrongUnknown, numWrongUnknown * 100.0 / unknownWords));
-    }
+        if (unknownWords > 0) {
+            output.append(String.format("Unknown words right: %d (%f%%); wrong: %d (%f%%).%n",
+                    unknownWords - numWrongUnknown,
+                    100.0 - numWrongUnknown * 100.0 / unknownWords,
+                    numWrongUnknown, numWrongUnknown * 100.0 / unknownWords))
+        }
 
-    return output.toString();
-  }
+        return output.toString()
+    }
 
-  void printModelAndAccuracy(MaxentTagger maxentTagger) {
-    // print the output all at once so that multiple threads don't clobber each other's output
-    log.info(resultsString(maxentTagger));
-  }
-
-
-  int getNumWords() {
-    return numRight + numWrong;
-  }
-
-  void setDebug(boolean status) {
-    writeUnknDict = status;
-    writeWords = status;
-    writeTopWords = status;
-    writeConfusionMatrix = status;
-  }
-
-  static class TestSentenceProcessor implements ThreadsafeProcessor<List<TaggedWord>, TestSentence> {
-    MaxentTagger maxentTagger;
-
-    public TestSentenceProcessor(MaxentTagger maxentTagger) {
-      this.maxentTagger = maxentTagger;
+    fun printModelAndAccuracy(maxentTagger: MaxentTagger) {
+        // print the output all at once so that multiple threads don't clobber each other's output
+        log.info(resultsString(maxentTagger))
     }
 
-    @Override
-    public TestSentence process(List<TaggedWord> taggedSentence) {
-      TestSentence testS = new TestSentence(maxentTagger);
-      testS.setCorrectTags(taggedSentence);
-      testS.tagSentence(taggedSentence, false);
-      return testS;
-    }
+    internal class TestSentenceProcessor(private var maxentTagger: MaxentTagger) : ThreadsafeProcessor<List<TaggedWord>, BaseTagger> {
+        override fun process(taggedSentence: List<TaggedWord>): BaseTagger {
+            val testS = BaseTagger(maxentTagger)
+            testS.setCorrectTags(taggedSentence)
+            testS.tagSentence(taggedSentence, false)
+            return testS
+        }
 
-    @Override
-    public ThreadsafeProcessor<List<TaggedWord>, TestSentence> newInstance() {
-      // MaxentTagger is threadsafe
-      return this;
-    }
-  }
+        override fun newInstance(): ThreadsafeProcessor<List<TaggedWord>, BaseTagger> {
+            // MaxentTagger is threadsafe
+            return this
+        }
+    }
 
+    companion object {
+        private val log = Redwood.channels(TestClassifier::class.java)
+    }
 }
Index: src/edu/stanford/nlp/tagger/io/TaggedFileRecord.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/io/TaggedFileRecord.java	(revision 97f067c3dc5909bcacb3e2e7bc686fb3e1fedd82)
+++ src/edu/stanford/nlp/tagger/io/TaggedFileRecord.java	(revision ca2d288bef5de4c5fc1a917f86dd4a8f0ec9c774)
@@ -10,197 +10,198 @@
 import edu.stanford.nlp.trees.TreeNormalizer;
 import edu.stanford.nlp.trees.TreeReaderFactory;
 import edu.stanford.nlp.trees.TreeTransformer;
+
 import java.util.function.Predicate;
+
 import edu.stanford.nlp.util.ReflectionLoading;
 
-/** Parses and specifies all the details for how to read some POS tagging data.
- *  The options for this class are documented in MaxentTagger, unlder the trainFile property.
+/**
+ * Parses and specifies all the details for how to read some POS tagging data.
+ * The options for this class are documented in MaxentTagger, unlder the trainFile property.
  *
- *  @author John Bauer
+ * @author John Bauer
  */
 public class TaggedFileRecord {
 
-  public enum Format {
-    TEXT,  // represents a tokenized file separated by text
-    TSV,   // represents a tsv file such as a conll file
-    TREES // represents a file in PTB format
-  }
+    public enum Format {
+        TEXT,  // represents a tokenized file separated by text
+        TSV,   // represents a tsv file such as a conll file
+        TREES // represents a file in PTB format
+    }
 
-  final String file;
-  final Format format;
-  final String encoding;
-  final String tagSeparator;
-  final TreeTransformer treeTransformer;
-  final TreeNormalizer treeNormalizer;
-  final NumberRangesFileFilter treeRange;
-  final Predicate<Tree> treeFilter;
-  final Integer wordColumn;
-  final Integer tagColumn;
-  final TreeReaderFactory trf;
+    final String file;
+    private final Format format;
+    final String encoding;
+    final String tagSeparator;
+    final TreeTransformer treeTransformer;
+    final TreeNormalizer treeNormalizer;
+    final NumberRangesFileFilter treeRange;
+    final Predicate<Tree> treeFilter;
+    final Integer wordColumn;
+    final Integer tagColumn;
+    final TreeReaderFactory trf;
 
-  private TaggedFileRecord(String file, Format format,
-                           String encoding, String tagSeparator,
-                           TreeTransformer treeTransformer,
-                           TreeNormalizer treeNormalizer,
-                           TreeReaderFactory trf,
-                           NumberRangesFileFilter treeRange,
-                           Predicate<Tree> treeFilter,
-                           Integer wordColumn, Integer tagColumn) {
-    this.file = file;
-    this.format = format;
-    this.encoding = encoding;
-    this.tagSeparator = tagSeparator;
-    this.treeTransformer = treeTransformer;
-    this.treeNormalizer = treeNormalizer;
-    this.treeRange = treeRange;
-    this.treeFilter = treeFilter;
-    this.wordColumn = wordColumn;
-    this.tagColumn = tagColumn;
-    this.trf = trf;
-  }
+    private TaggedFileRecord(String file, Format format,
+                             String encoding, String tagSeparator,
+                             TreeTransformer treeTransformer,
+                             TreeNormalizer treeNormalizer,
+                             TreeReaderFactory trf,
+                             NumberRangesFileFilter treeRange,
+                             Predicate<Tree> treeFilter,
+                             Integer wordColumn, Integer tagColumn) {
+        this.file = file;
+        this.format = format;
+        this.encoding = encoding;
+        this.tagSeparator = tagSeparator;
+        this.treeTransformer = treeTransformer;
+        this.treeNormalizer = treeNormalizer;
+        this.treeRange = treeRange;
+        this.treeFilter = treeFilter;
+        this.wordColumn = wordColumn;
+        this.tagColumn = tagColumn;
+        this.trf = trf;
+    }
 
-  public static final String FORMAT = "format";
-  public static final String ENCODING = "encoding";
-  public static final String TAG_SEPARATOR = "tagSeparator";
-  public static final String TREE_TRANSFORMER = "treeTransformer";
-  public static final String TREE_NORMALIZER = "treeNormalizer";
-  public static final String TREE_RANGE = "treeRange";
-  public static final String TREE_FILTER = "treeFilter";
-  public static final String WORD_COLUMN = "wordColumn";
-  public static final String TAG_COLUMN = "tagColumn";
-  public static final String TREE_READER = "trf";
+    public static final String FORMAT = "format";
+    public static final String ENCODING = "encoding";
+    private static final String TAG_SEPARATOR = "tagSeparator";
+    private static final String TREE_TRANSFORMER = "treeTransformer";
+    private static final String TREE_NORMALIZER = "treeNormalizer";
+    public static final String TREE_RANGE = "treeRange";
+    public static final String TREE_FILTER = "treeFilter";
+    private static final String WORD_COLUMN = "wordColumn";
+    private static final String TAG_COLUMN = "tagColumn";
+    private static final String TREE_READER = "trf";
 
-  public String toString() {
-    StringBuilder s = new StringBuilder();
-    s.append(FORMAT + "=" + format);
-    s.append("," + ENCODING + "=" + encoding);
-    s.append("," + TAG_SEPARATOR + "=" + tagSeparator);
-    if (treeTransformer != null) {
-      s.append("," + TREE_TRANSFORMER + "=" +
-               treeTransformer.getClass().getName());
-    }
-    if (trf != null) {
-      s.append("," + TREE_READER + "=" +
-               trf.getClass().getName());
-    }
-    if (treeNormalizer != null) {
-      s.append("," + TREE_NORMALIZER + "=" +
-               treeNormalizer.getClass().getName());
-    }
-    if (treeRange != null) {
-      s.append("," + TREE_RANGE + "=" +
-               treeRange.toString().replaceAll(",", ":"));
-    }
-    if (treeRange != null) {
-      s.append("," + TREE_FILTER + "=" + treeFilter.getClass().toString());
-    }
-    if (wordColumn != null) {
-      s.append("," + WORD_COLUMN + "=" + wordColumn);
-    }
-    if (tagColumn != null) {
-      s.append("," + TAG_COLUMN + "=" + tagColumn);
-    }
-    return s.toString();
-  }
+    public String toString() {
+        StringBuilder s = new StringBuilder();
+        s.append(FORMAT + "=").append(format);
+        s.append("," + ENCODING + "=").append(encoding);
+        s.append("," + TAG_SEPARATOR + "=").append(tagSeparator);
+        if (treeTransformer != null) {
+            s.append("," + TREE_TRANSFORMER + "=").append(treeTransformer.getClass().getName());
+        }
+        if (trf != null) {
+            s.append("," + TREE_READER + "=").append(trf.getClass().getName());
+        }
+        if (treeNormalizer != null) {
+            s.append("," + TREE_NORMALIZER + "=").append(treeNormalizer.getClass().getName());
+        }
+        if (treeRange != null) {
+            s.append("," + TREE_RANGE + "=").append(treeRange.toString().replaceAll(",", ":"));
+        }
+        if (treeRange != null) {
+            s.append("," + TREE_FILTER + "=").append(treeFilter.getClass().toString());
+        }
+        if (wordColumn != null) {
+            s.append("," + WORD_COLUMN + "=").append(wordColumn);
+        }
+        if (tagColumn != null) {
+            s.append("," + TAG_COLUMN + "=").append(tagColumn);
+        }
+        return s.toString();
+    }
 
-  public String filename() { return file; }
+    public String filename() {
+        return file;
+    }
 
-  public TaggedFileReader reader() {
-    switch(format) {
-    case TEXT:
-      return new TextTaggedFileReader(this);
-    case TREES:
-      return new TreeTaggedFileReader(this);
-    case TSV:
-      return new TSVTaggedFileReader(this);
-    default:
-      throw new IllegalArgumentException("Unknown format " + format);
-    }
-  }
+    public TaggedFileReader reader() {
+        switch (format) {
+            case TEXT:
+                return new TextTaggedFileReader(this);
+            case TREES:
+                return new TreeTaggedFileReader(this);
+            case TSV:
+                return new TSVTaggedFileReader(this);
+            default:
+                throw new IllegalArgumentException("Unknown format " + format);
+        }
+    }
 
-  public static List<TaggedFileRecord> createRecords(Properties config,
-                                                     String description) {
-    String[] pieces = description.split(";");
-    List<TaggedFileRecord> records = new ArrayList<>();
-    for (String piece : pieces) {
-      records.add(createRecord(config, piece));
-    }
-    return records;
-  }
+    public static List<TaggedFileRecord> createRecords(Properties config,
+                                                       String description) {
+        String[] pieces = description.split(";");
+        List<TaggedFileRecord> records = new ArrayList<>();
+        for (String piece : pieces) {
+            records.add(createRecord(config, piece));
+        }
+        return records;
+    }
 
-  public static TaggedFileRecord createRecord(Properties config,
-                                              String description) {
-    String[] pieces = description.split(",");
-    if (pieces.length == 1) {
-      return new TaggedFileRecord(description, Format.TEXT,
-                                  getEncoding(config),
-                                  getTagSeparator(config),
-                                  null, null, null, null, null, null, null);
-    }
+    public static TaggedFileRecord createRecord(Properties config,
+                                                String description) {
+        String[] pieces = description.split(",");
+        if (pieces.length == 1) {
+            return new TaggedFileRecord(description, Format.TEXT,
+                    getEncoding(config),
+                    getTagSeparator(config),
+                    null, null, null, null, null, null, null);
+        }
 
-    String[] args = new String[pieces.length - 1];
-    System.arraycopy(pieces, 0, args, 0, pieces.length - 1);
-    String file = pieces[pieces.length - 1];
-    Format format = Format.TEXT;
-    String encoding = getEncoding(config);
-    String tagSeparator = getTagSeparator(config);
-    TreeTransformer treeTransformer = null;
-    TreeNormalizer treeNormalizer = null;
-    TreeReaderFactory trf = null;
-    NumberRangesFileFilter treeRange = null;
-    Predicate<Tree> treeFilter = null;
-    Integer wordColumn = null, tagColumn = null;
+        String[] args = new String[pieces.length - 1];
+        System.arraycopy(pieces, 0, args, 0, pieces.length - 1);
+        String file = pieces[pieces.length - 1];
+        Format format = Format.TEXT;
+        String encoding = getEncoding(config);
+        String tagSeparator = getTagSeparator(config);
+        TreeTransformer treeTransformer = null;
+        TreeNormalizer treeNormalizer = null;
+        TreeReaderFactory trf = null;
+        NumberRangesFileFilter treeRange = null;
+        Predicate<Tree> treeFilter = null;
+        Integer wordColumn = null, tagColumn = null;
 
-    for (String arg : args) {
-      String[] argPieces = arg.split("=", 2);
-      if (argPieces.length != 2) {
-        throw new IllegalArgumentException("TaggedFileRecord argument " + arg +
-                                           " has an unexpected number of =s");
-      }
-      if (argPieces[0].equalsIgnoreCase(FORMAT)) {
-        format = Format.valueOf(argPieces[1]);
-      } else if (argPieces[0].equalsIgnoreCase(ENCODING)) {
-        encoding = argPieces[1];
-      } else if (argPieces[0].equalsIgnoreCase(TAG_SEPARATOR)) {
-        tagSeparator = argPieces[1];
-      } else if (argPieces[0].equalsIgnoreCase(TREE_TRANSFORMER)) {
-        treeTransformer = ReflectionLoading.loadByReflection(argPieces[1]);
-      } else if (argPieces[0].equalsIgnoreCase(TREE_NORMALIZER)) {
-        treeNormalizer = ReflectionLoading.loadByReflection(argPieces[1]);
-      } else if (argPieces[0].equalsIgnoreCase(TREE_READER)) {
-        trf = ReflectionLoading.loadByReflection(argPieces[1]);
-      } else if (argPieces[0].equalsIgnoreCase(TREE_RANGE)) {
-        String range = argPieces[1].replaceAll(":", ",");
-        treeRange = new NumberRangesFileFilter(range, true);
-      } else if (argPieces[0].equalsIgnoreCase(TREE_FILTER)) {
-        treeFilter = ReflectionLoading.loadByReflection(argPieces[1]);
-      } else if (argPieces[0].equalsIgnoreCase(WORD_COLUMN)) {
-        wordColumn = Integer.valueOf(argPieces[1]);
-      } else if (argPieces[0].equalsIgnoreCase(TAG_COLUMN)) {
-        tagColumn = Integer.valueOf(argPieces[1]);
-      } else {
-        throw new IllegalArgumentException("TaggedFileRecord argument " +
-                                           argPieces[0] + " is unknown");
-      }
-    }
-    return new TaggedFileRecord(file, format, encoding, tagSeparator,
-                                treeTransformer, treeNormalizer, trf, treeRange,
-                                treeFilter, wordColumn, tagColumn);
-  }
+        for (String arg : args) {
+            String[] argPieces = arg.split("=", 2);
+            if (argPieces.length != 2) {
+                throw new IllegalArgumentException("TaggedFileRecord argument " + arg +
+                        " has an unexpected number of =s");
+            }
+            if (argPieces[0].equalsIgnoreCase(FORMAT)) {
+                format = Format.valueOf(argPieces[1]);
+            } else if (argPieces[0].equalsIgnoreCase(ENCODING)) {
+                encoding = argPieces[1];
+            } else if (argPieces[0].equalsIgnoreCase(TAG_SEPARATOR)) {
+                tagSeparator = argPieces[1];
+            } else if (argPieces[0].equalsIgnoreCase(TREE_TRANSFORMER)) {
+                treeTransformer = ReflectionLoading.loadByReflection(argPieces[1]);
+            } else if (argPieces[0].equalsIgnoreCase(TREE_NORMALIZER)) {
+                treeNormalizer = ReflectionLoading.loadByReflection(argPieces[1]);
+            } else if (argPieces[0].equalsIgnoreCase(TREE_READER)) {
+                trf = ReflectionLoading.loadByReflection(argPieces[1]);
+            } else if (argPieces[0].equalsIgnoreCase(TREE_RANGE)) {
+                String range = argPieces[1].replaceAll(":", ",");
+                treeRange = new NumberRangesFileFilter(range, true);
+            } else if (argPieces[0].equalsIgnoreCase(TREE_FILTER)) {
+                treeFilter = ReflectionLoading.loadByReflection(argPieces[1]);
+            } else if (argPieces[0].equalsIgnoreCase(WORD_COLUMN)) {
+                wordColumn = Integer.valueOf(argPieces[1]);
+            } else if (argPieces[0].equalsIgnoreCase(TAG_COLUMN)) {
+                tagColumn = Integer.valueOf(argPieces[1]);
+            } else {
+                throw new IllegalArgumentException("TaggedFileRecord argument " +
+                        argPieces[0] + " is unknown");
+            }
+        }
+        return new TaggedFileRecord(file, format, encoding, tagSeparator,
+                treeTransformer, treeNormalizer, trf, treeRange,
+                treeFilter, wordColumn, tagColumn);
+    }
 
-  public static String getEncoding(Properties config) {
-    String encoding = config.getProperty(TaggerConfig.ENCODING_PROPERTY);
-    if (encoding == null)
-      return TaggerConfig.ENCODING;
-    return encoding;
-  }
+    public static String getEncoding(Properties config) {
+        String encoding = config.getProperty(TaggerConfig.ENCODING_PROPERTY);
+        if (encoding == null)
+            return TaggerConfig.ENCODING;
+        return encoding;
+    }
 
-  public static String getTagSeparator(Properties config) {
-    String tagSeparator =
-      config.getProperty(TaggerConfig.TAG_SEPARATOR_PROPERTY);
-    if (tagSeparator == null)
-      return TaggerConfig.TAG_SEPARATOR;
-    return tagSeparator;
-  }
+    private static String getTagSeparator(Properties config) {
+        String tagSeparator =
+                config.getProperty(TaggerConfig.TAG_SEPARATOR_PROPERTY);
+        if (tagSeparator == null)
+            return TaggerConfig.TAG_SEPARATOR;
+        return tagSeparator;
+    }
 
 }
Index: src/edu/stanford/nlp/tagger/maxent/TaggerConfig.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/TaggerConfig.java	(revision 97f067c3dc5909bcacb3e2e7bc686fb3e1fedd82)
+++ src/edu/stanford/nlp/tagger/maxent/TaggerConfig.java	(revision b2d1a9a42e572c987c6031e6ec99113741579220)
@@ -19,739 +19,827 @@
  * (4) add getter method, (5) add to dump() method, (6) add to printGenProps()
  * method, (7) add to class javadoc of MaxentTagger.
  *
- *  @author William Morgan
- *  @author Anna Rafferty
- *  @author Michel Galley
+ * @author William Morgan
+ * @author Anna Rafferty
+ * @author Michel Galley
  */
-public class TaggerConfig extends Properties /* Inherits implementation of Serializable! */  {
+public class TaggerConfig extends Properties /* Inherits implementation of Serializable! */ {
 
-  /** A logger for this class */
-  private static Redwood.RedwoodChannels log = Redwood.channels(TaggerConfig.class);
+    /**
+     * A logger for this class
+     */
+    private static Redwood.RedwoodChannels log = Redwood.channels(TaggerConfig.class);
 
-  private static final long serialVersionUID = -4136407850147157497L;
+    private static final long serialVersionUID = -4136407850147157497L;
 
-  public enum Mode {
-    TRAIN, TEST, TAG, DUMP
-  }
+    public enum Mode {
+        TRAIN, TEST, TAG, DUMP
+    }
 
-  /* defaults. sentenceDelimiter might be null; the others all have non-null values. */
-  public static final String
-  SEARCH = "qn",
-  TAG_SEPARATOR = "/",
-  TOKENIZE = "true",
-  DEBUG = "false",
-  ITERATIONS = "100",
-  ARCH = "",
-  WORD_FUNCTION = "",
-  RARE_WORD_THRESH = "5",
-  MIN_FEATURE_THRESH = "5",
-  CUR_WORD_MIN_FEATURE_THRESH = "2",
-  RARE_WORD_MIN_FEATURE_THRESH = "10",
-  VERY_COMMON_WORD_THRESH = "250",
-  OCCURRING_TAGS_ONLY = "false",
-  POSSIBLE_TAGS_ONLY = "false",
-  SIGMA_SQUARED = String.valueOf(0.5),
-  ENCODING = "UTF-8",
-  LEARN_CLOSED_CLASS = "false",
-  CLOSED_CLASS_THRESHOLD = "40",
-  VERBOSE = "false",
-  VERBOSE_RESULTS = "true",
-  SGML = "false",
-  LANG = "",
-  TOKENIZER_FACTORY = "",
-  XML_INPUT = "",
-  TAG_INSIDE = "",
-  APPROXIMATE = "-1.0",
-  TOKENIZER_OPTIONS = "",
-  DEFAULT_REG_L1 = "1.0",
-  OUTPUT_FILE = "",
-  OUTPUT_FORMAT = "slashTags",
-  OUTPUT_FORMAT_OPTIONS = "",
-  NTHREADS = "1";
+    /* defaults. sentenceDelimiter might be null; the others all have non-null values. */
+    public static final String
+            SEARCH = "qn",
+            TAG_SEPARATOR = "/",
+            TOKENIZE = "true",
+            DEBUG = "false",
+            ITERATIONS = "100",
+            ARCH = "",
+            WORD_FUNCTION = "",
+            RARE_WORD_THRESH = "5",
+            MIN_FEATURE_THRESH = "5",
+            CUR_WORD_MIN_FEATURE_THRESH = "2",
+            RARE_WORD_MIN_FEATURE_THRESH = "10",
+            VERY_COMMON_WORD_THRESH = "250",
+            OCCURRING_TAGS_ONLY = "false",
+            POSSIBLE_TAGS_ONLY = "false",
+            SIGMA_SQUARED = String.valueOf(0.5),
+            ENCODING = "UTF-8",
+            LEARN_CLOSED_CLASS = "false",
+            CLOSED_CLASS_THRESHOLD = "40",
+            DO_DETERMINISTIC_TAG_EXPANSION = "false",
+            VERBOSE = "false",
+            VERBOSE_RESULTS = "true",
+            SGML = "false",
+            LANG = "",
+            TOKENIZER_FACTORY = "",
+            XML_INPUT = "",
+            TAG_INSIDE = "",
+            APPROXIMATE = "-1.0",
+            TOKENIZER_OPTIONS = "",
+            DEFAULT_REG_L1 = "1.0",
+            OUTPUT_FILE = "",
+            OUTPUT_FORMAT = "slashTags",
+            OUTPUT_FORMAT_OPTIONS = "",
+            NTHREADS = "1";
 
-  public static final String ENCODING_PROPERTY = "encoding",
-  TAG_SEPARATOR_PROPERTY = "tagSeparator";
+    public static final String ENCODING_PROPERTY = "encoding",
+            TAG_SEPARATOR_PROPERTY = "tagSeparator";
 
 
-  private static final Map<String, String> defaultValues = Generics.newHashMap();
-  static {
-    defaultValues.put("arch", ARCH);
-    defaultValues.put("wordFunction", WORD_FUNCTION);
-    defaultValues.put("closedClassTags", "");
-    defaultValues.put("closedClassTagThreshold", CLOSED_CLASS_THRESHOLD);
-    defaultValues.put("search", SEARCH);
-    defaultValues.put(TAG_SEPARATOR_PROPERTY, TAG_SEPARATOR);
-    defaultValues.put("tokenize", TOKENIZE);
-    defaultValues.put("debug", DEBUG);
-    defaultValues.put("iterations", ITERATIONS);
-    defaultValues.put("rareWordThresh", RARE_WORD_THRESH);
-    defaultValues.put("minFeatureThresh", MIN_FEATURE_THRESH);
-    defaultValues.put("curWordMinFeatureThresh", CUR_WORD_MIN_FEATURE_THRESH);
-    defaultValues.put("rareWordMinFeatureThresh", RARE_WORD_MIN_FEATURE_THRESH);
-    defaultValues.put("veryCommonWordThresh", VERY_COMMON_WORD_THRESH);
-    defaultValues.put("occurringTagsOnly", OCCURRING_TAGS_ONLY);
-    defaultValues.put("possibleTagsOnly", POSSIBLE_TAGS_ONLY);
-    defaultValues.put("sigmaSquared", SIGMA_SQUARED);
-    defaultValues.put(ENCODING_PROPERTY, ENCODING);
-    defaultValues.put("learnClosedClassTags", LEARN_CLOSED_CLASS);
-    defaultValues.put("verbose", VERBOSE);
-    defaultValues.put("verboseResults", VERBOSE_RESULTS);
-    defaultValues.put("openClassTags", "");
-    defaultValues.put("lang", LANG);
-    defaultValues.put("tokenizerFactory", TOKENIZER_FACTORY);
-    defaultValues.put("xmlInput", XML_INPUT);
-    defaultValues.put("tagInside", TAG_INSIDE);
-    defaultValues.put("sgml", SGML);
-    defaultValues.put("approximate", APPROXIMATE);
-    defaultValues.put("tokenizerOptions", TOKENIZER_OPTIONS);
-    defaultValues.put("regL1", DEFAULT_REG_L1);
-    defaultValues.put("outputFile", OUTPUT_FILE);
-    defaultValues.put("outputFormat", OUTPUT_FORMAT);
-    defaultValues.put("outputFormatOptions", OUTPUT_FORMAT_OPTIONS);
-    defaultValues.put("nthreads", NTHREADS);
-  }
+    private static final Map<String, String> defaultValues = Generics.newHashMap();
+
+    static {
+        defaultValues.put("arch", ARCH);
+        defaultValues.put("wordFunction", WORD_FUNCTION);
+        defaultValues.put("closedClassTags", "");
+        defaultValues.put("closedClassTagThreshold", CLOSED_CLASS_THRESHOLD);
+        defaultValues.put("doDeterministicTagExpansion", DO_DETERMINISTIC_TAG_EXPANSION);
+        defaultValues.put("search", SEARCH);
+        defaultValues.put(TAG_SEPARATOR_PROPERTY, TAG_SEPARATOR);
+        defaultValues.put("tokenize", TOKENIZE);
+        defaultValues.put("debug", DEBUG);
+        defaultValues.put("iterations", ITERATIONS);
+        defaultValues.put("rareWordThresh", RARE_WORD_THRESH);
+        defaultValues.put("minFeatureThresh", MIN_FEATURE_THRESH);
+        defaultValues.put("curWordMinFeatureThresh", CUR_WORD_MIN_FEATURE_THRESH);
+        defaultValues.put("rareWordMinFeatureThresh", RARE_WORD_MIN_FEATURE_THRESH);
+        defaultValues.put("veryCommonWordThresh", VERY_COMMON_WORD_THRESH);
+        defaultValues.put("occurringTagsOnly", OCCURRING_TAGS_ONLY);
+        defaultValues.put("possibleTagsOnly", POSSIBLE_TAGS_ONLY);
+        defaultValues.put("sigmaSquared", SIGMA_SQUARED);
+        defaultValues.put(ENCODING_PROPERTY, ENCODING);
+        defaultValues.put("learnClosedClassTags", LEARN_CLOSED_CLASS);
+        defaultValues.put("verbose", VERBOSE);
+        defaultValues.put("verboseResults", VERBOSE_RESULTS);
+        defaultValues.put("openClassTags", "");
+        defaultValues.put("lang", LANG);
+        defaultValues.put("tokenizerFactory", TOKENIZER_FACTORY);
+        defaultValues.put("xmlInput", XML_INPUT);
+        defaultValues.put("tagInside", TAG_INSIDE);
+        defaultValues.put("sgml", SGML);
+        defaultValues.put("approximate", APPROXIMATE);
+        defaultValues.put("tokenizerOptions", TOKENIZER_OPTIONS);
+        defaultValues.put("regL1", DEFAULT_REG_L1);
+        defaultValues.put("outputFile", OUTPUT_FILE);
+        defaultValues.put("outputFormat", OUTPUT_FORMAT);
+        defaultValues.put("outputFormatOptions", OUTPUT_FORMAT_OPTIONS);
+        defaultValues.put("nthreads", NTHREADS);
+    }
 
-  /**
-   * This constructor is just for creating an instance with default values.
-   * Used internally.
-   */
-  private TaggerConfig() {
-    super();
-    this.putAll(defaultValues);
-  }
+    /**
+     * This constructor is just for creating an instance with default values.
+     * Used internally.
+     */
+    private TaggerConfig() {
+        super();
+        this.putAll(defaultValues);
+    }
 
-  /**
-   * We force you to pass in a TaggerConfig rather than any other
-   * superclass so that we know the arg error checking has already occurred
-   */
-  public TaggerConfig(TaggerConfig old) {
-    super(old);
-  }
+    /**
+     * We force you to pass in a TaggerConfig rather than any other
+     * superclass so that we know the arg error checking has already occurred
+     */
+    public TaggerConfig(TaggerConfig old) {
+        super(old);
+    }
 
-  public TaggerConfig(String... args) {
-    this(StringUtils.argsToProperties(args));
-  }
+    public TaggerConfig(String... args) {
+        this(StringUtils.argsToProperties(args));
+    }
 
-  public TaggerConfig(Properties props) {
-    // load up the default properties
-    this();
+    public TaggerConfig(Properties props) {
+        // load up the default properties
+        this();
 
-    /* Try and use the default properties from the model */
-    // Properties modelProps = new Properties();
-    // TaggerConfig oldConfig = new TaggerConfig(); // loads default values in oldConfig
-    if (! props.containsKey("trainFile")) {
-      String name = props.getProperty("model");
-      if (name == null) {
-        name = props.getProperty("dump");
-      }
-      if (name != null) {
-        try (DataInputStream in = new DataInputStream(IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(name))) {
-          log.info("Loading default properties from tagger " + name);
-          this.putAll(TaggerConfig.readConfig(in)); // overwrites defaults with any serialized values.
-        } catch (Exception e) {
-          throw new RuntimeIOException("No such trained tagger config file found: " + name);
-        }
-      }
-    }
+        /* Try and use the default properties from the model */
+        // Properties modelProps = new Properties();
+        // TaggerConfig oldConfig = new TaggerConfig(); // loads default values in oldConfig
+        if (!props.containsKey("trainFile")) {
+            String name = props.getProperty("model");
+            if (name == null) {
+                name = props.getProperty("dump");
+            }
+            if (name != null) {
+                try (DataInputStream in = new DataInputStream(IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(name))) {
+                    log.info("Loading default properties from tagger " + name);
+                    this.putAll(TaggerConfig.readConfig(in)); // overwrites defaults with any serialized values.
+                } catch (Exception e) {
+                    throw new RuntimeIOException("No such trained tagger config file found: " + name);
+                }
+            }
+        }
 
-    setProperties(props);
-  }
+        setProperties(props);
+    }
 
-  public void setProperties(Properties props) {
-    if (props.getProperty("") != null) {
-      throw new RuntimeException("unknown argument(s): \"" + props.getProperty("") + '\"');
-    }
+    public void setProperties(Properties props) {
+        if (props.getProperty("") != null) {
+            throw new RuntimeException("unknown argument(s): \"" + props.getProperty("") + '\"');
+        }
 
-    if (props.getProperty("genprops") != null) {
-      printGenProps(System.out);
-      System.exit(0);
-    }
+        if (props.getProperty("genprops") != null) {
+            printGenProps(System.out);
+            System.exit(0);
+        }
 
-    if (props.containsKey("mode") && props.containsKey("file")) {
-      this.setProperty("mode", props.getProperty("mode"));
-      this.setProperty("file", props.getProperty("file"));
-    } else if (props.containsKey("trainFile")) {
-      //Training mode
-      this.setProperty("mode", Mode.TRAIN.toString());
-      this.setProperty("file", props.getProperty("trainFile", "").trim());
-    } else if (props.containsKey("testFile")) {
-      //Testing mode
-      this.setProperty("mode", Mode.TEST.toString());
-      this.setProperty("file", props.getProperty("testFile", "").trim());
-    } else if (props.containsKey("textFile")) {
-      //Tagging mode
-      this.setProperty("mode", Mode.TAG.toString());
-      this.setProperty("file", props.getProperty("textFile", "").trim());
-    } else if (props.containsKey("dump")) {
-      this.setProperty("mode", Mode.DUMP.toString());
-      // this.setProperty("file", props.getProperty("dump").trim());
-      props.setProperty("model", props.getProperty("dump").trim());
-    } else {
-      this.setProperty("mode", Mode.TAG.toString());
-      this.setProperty("file", "stdin");
-    }
-    //for any mode other than train, we load a classifier, which means we load a config - model always needs to be specified
-    //on command line/in props file
-    //Get the path to the model (or the path where you'd like to save the model); this is necessary for training, testing, and tagging
-    this.setProperty("model", props.getProperty("model", this.getProperty("model", "")).trim());
-    if ( ! (this.getMode() == Mode.DUMP) && this.getProperty("model").isEmpty()) {
-      throw new RuntimeException("'model' parameter must be specified");
-    }
+        if (props.containsKey("mode") && props.containsKey("file")) {
+            this.setProperty("mode", props.getProperty("mode"));
+            this.setProperty("file", props.getProperty("file"));
+        } else if (props.containsKey("trainFile")) {
+            //Training mode
+            this.setProperty("mode", Mode.TRAIN.toString());
+            this.setProperty("file", props.getProperty("trainFile", "").trim());
+        } else if (props.containsKey("testFile")) {
+            //Testing mode
+            this.setProperty("mode", Mode.TEST.toString());
+            this.setProperty("file", props.getProperty("testFile", "").trim());
+        } else if (props.containsKey("textFile")) {
+            //Tagging mode
+            this.setProperty("mode", Mode.TAG.toString());
+            this.setProperty("file", props.getProperty("textFile", "").trim());
+        } else if (props.containsKey("dump")) {
+            this.setProperty("mode", Mode.DUMP.toString());
+            // this.setProperty("file", props.getProperty("dump").trim());
+            props.setProperty("model", props.getProperty("dump").trim());
+        } else {
+            this.setProperty("mode", Mode.TAG.toString());
+            this.setProperty("file", "stdin");
+        }
+        //for any mode other than train, we load a classifier, which means we load a config - model always needs to be specified
+        //on command line/in props file
+        //Get the path to the model (or the path where you'd like to save the model); this is necessary for training, testing, and tagging
+        this.setProperty("model", props.getProperty("model", this.getProperty("model", "")).trim());
+        if (!(this.getMode() == Mode.DUMP) && this.getProperty("model").isEmpty()) {
+            throw new RuntimeException("'model' parameter must be specified");
+        }
 
-    this.setProperty("search", props.getProperty("search", this.getProperty("search")).trim().toLowerCase());
-    String srch = this.getProperty("search");
-    if ( ! (srch.equals("cg") || srch.equals("iis") || srch.equals("owlqn") || srch.equals("qn") || srch.equals("owlqn2"))) {
-      throw new RuntimeException("'search' must be one of 'iis', 'cg', 'qn' or 'owlqn' or 'owlqn2': " + srch);
-    }
+        this.setProperty("search", props.getProperty("search", this.getProperty("search")).trim().toLowerCase());
+        String srch = this.getProperty("search");
+        if (!(srch.equals("cg") || srch.equals("iis") || srch.equals("owlqn") || srch.equals("qn") || srch.equals("owlqn2"))) {
+            throw new RuntimeException("'search' must be one of 'iis', 'cg', 'qn' or 'owlqn' or 'owlqn2': " + srch);
+        }
 
-    this.setProperty("sigmaSquared", props.getProperty("sigmaSquared", this.getProperty("sigmaSquared")));
+        this.setProperty("sigmaSquared", props.getProperty("sigmaSquared", this.getProperty("sigmaSquared")));
 
-    this.setProperty(TAG_SEPARATOR_PROPERTY, props.getProperty(TAG_SEPARATOR_PROPERTY, this.getProperty(TAG_SEPARATOR_PROPERTY)));
+        this.setProperty(TAG_SEPARATOR_PROPERTY, props.getProperty(TAG_SEPARATOR_PROPERTY, this.getProperty(TAG_SEPARATOR_PROPERTY)));
 
-    this.setProperty("iterations", props.getProperty("iterations", this.getProperty("iterations")));
-    this.setProperty("rareWordThresh", props.getProperty("rareWordThresh", this.getProperty("rareWordThresh")));
-    this.setProperty("minFeatureThresh", props.getProperty("minFeatureThresh", this.getProperty("minFeatureThresh")));
-    this.setProperty("curWordMinFeatureThresh", props.getProperty("curWordMinFeatureThresh", this.getProperty("curWordMinFeatureThresh")));
-    this.setProperty("rareWordMinFeatureThresh", props.getProperty("rareWordMinFeatureThresh", this.getProperty("rareWordMinFeatureThresh")));
-    this.setProperty("veryCommonWordThresh", props.getProperty("veryCommonWordThresh", this.getProperty("veryCommonWordThresh")));
-    this.setProperty("occurringTagsOnly", props.getProperty("occurringTagsOnly", this.getProperty("occurringTagsOnly", OCCURRING_TAGS_ONLY)));
-    this.setProperty("possibleTagsOnly", props.getProperty("possibleTagsOnly", this.getProperty("possibleTagsOnly")));
+        this.setProperty("iterations", props.getProperty("iterations", this.getProperty("iterations")));
+        this.setProperty("rareWordThresh", props.getProperty("rareWordThresh", this.getProperty("rareWordThresh")));
+        this.setProperty("minFeatureThresh", props.getProperty("minFeatureThresh", this.getProperty("minFeatureThresh")));
+        this.setProperty("curWordMinFeatureThresh", props.getProperty("curWordMinFeatureThresh", this.getProperty("curWordMinFeatureThresh")));
+        this.setProperty("rareWordMinFeatureThresh", props.getProperty("rareWordMinFeatureThresh", this.getProperty("rareWordMinFeatureThresh")));
+        this.setProperty("veryCommonWordThresh", props.getProperty("veryCommonWordThresh", this.getProperty("veryCommonWordThresh")));
+        this.setProperty("occurringTagsOnly", props.getProperty("occurringTagsOnly", this.getProperty("occurringTagsOnly", OCCURRING_TAGS_ONLY)));
+        this.setProperty("possibleTagsOnly", props.getProperty("possibleTagsOnly", this.getProperty("possibleTagsOnly")));
 
-    this.setProperty("lang", props.getProperty("lang", this.getProperty("lang")));
+        this.setProperty("lang", props.getProperty("lang", this.getProperty("lang")));
 
-    this.setProperty("openClassTags", props.getProperty("openClassTags", this.getProperty("openClassTags")).trim());
-    this.setProperty("closedClassTags", props.getProperty("closedClassTags", this.getProperty("closedClassTags")).trim());
+        this.setProperty("openClassTags", props.getProperty("openClassTags", this.getProperty("openClassTags")).trim());
+        this.setProperty("closedClassTags", props.getProperty("closedClassTags", this.getProperty("closedClassTags")).trim());
 
-    this.setProperty("learnClosedClassTags", props.getProperty("learnClosedClassTags", this.getProperty("learnClosedClassTags")));
+        this.setProperty("learnClosedClassTags", props.getProperty("learnClosedClassTags", this.getProperty("learnClosedClassTags")));
 
-    this.setProperty("closedClassTagThreshold", props.getProperty("closedClassTagThreshold", this.getProperty("closedClassTagThreshold")));
+        this.setProperty("closedClassTagThreshold", props.getProperty("closedClassTagThreshold", this.getProperty("closedClassTagThreshold")));
+        this.setProperty("doDeterministicTagExpansion", props.getProperty("doDeterministicTagExpansion", this.getProperty("doDeterministicTagExpansion")));
 
-    this.setProperty("arch", props.getProperty("arch", this.getProperty("arch")));
-    if (this.getMode() == Mode.TRAIN && this.getProperty("arch").isEmpty()) {
-      throw new IllegalArgumentException("No architecture specified; " +
-                                         "set the -arch flag with " +
-                                         "the features to be used");
-    }
+        this.setProperty("arch", props.getProperty("arch", this.getProperty("arch")));
+        if (this.getMode() == Mode.TRAIN && this.getProperty("arch").isEmpty()) {
+            throw new IllegalArgumentException("No architecture specified; " +
+                    "set the -arch flag with " +
+                    "the features to be used");
+        }
 
-    this.setProperty("wordFunction", props.getProperty("wordFunction", this.getProperty("wordFunction", WORD_FUNCTION)));
+        this.setProperty("wordFunction", props.getProperty("wordFunction", this.getProperty("wordFunction", WORD_FUNCTION)));
 
-    this.setProperty("tokenize", props.getProperty("tokenize", this.getProperty("tokenize")));
-    this.setProperty("tokenizerFactory", props.getProperty("tokenizerFactory", this.getProperty("tokenizerFactory")));
+        this.setProperty("tokenize", props.getProperty("tokenize", this.getProperty("tokenize")));
+        this.setProperty("tokenizerFactory", props.getProperty("tokenizerFactory", this.getProperty("tokenizerFactory")));
 
-    this.setProperty("debugPrefix", props.getProperty("debugPrefix", this.getProperty("debugPrefix", "")));
-    this.setProperty("debug", props.getProperty("debug", DEBUG));
+        this.setProperty("debugPrefix", props.getProperty("debugPrefix", this.getProperty("debugPrefix", "")));
+        this.setProperty("tagExpansionRuleFile", props.getProperty("tagExpansionRuleFile", this.getProperty("tagExpansionRuleFile", "")));
+        this.setProperty("debug", props.getProperty("debug", DEBUG));
 
-    this.setProperty(ENCODING_PROPERTY, props.getProperty(ENCODING_PROPERTY, this.getProperty(ENCODING_PROPERTY)));
-    this.setProperty("sgml", props.getProperty("sgml", this.getProperty("sgml")));
-    this.setProperty("verbose", props.getProperty("verbose", this.getProperty("verbose")));
-    this.setProperty("verboseResults", props.getProperty("verboseResults", this.getProperty("verboseResults")));
+        this.setProperty(ENCODING_PROPERTY, props.getProperty(ENCODING_PROPERTY, this.getProperty(ENCODING_PROPERTY)));
+        this.setProperty("sgml", props.getProperty("sgml", this.getProperty("sgml")));
+        this.setProperty("verbose", props.getProperty("verbose", this.getProperty("verbose")));
+        this.setProperty("verboseResults", props.getProperty("verboseResults", this.getProperty("verboseResults")));
 
-    this.setProperty("regL1", props.getProperty("regL1", this.getProperty("regL1")));
+        this.setProperty("regL1", props.getProperty("regL1", this.getProperty("regL1")));
 
-    //this is a property that is stored (not like the general properties)
-    this.setProperty("xmlInput", props.getProperty("xmlInput", this.getProperty("xmlInput")).trim());
+        //this is a property that is stored (not like the general properties)
+        this.setProperty("xmlInput", props.getProperty("xmlInput", this.getProperty("xmlInput")).trim());
 
-    this.setProperty("tagInside", props.getProperty("tagInside", this.getProperty("tagInside"))); //this isn't something we save from time to time
-    this.setProperty("approximate", props.getProperty("approximate", this.getProperty("approximate"))); //this isn't something we save from time to time
-    this.setProperty("tokenizerOptions", props.getProperty("tokenizerOptions", this.getProperty("tokenizerOptions"))); //this isn't something we save from time to time
-    this.setProperty("outputFile", props.getProperty("outputFile", this.getProperty("outputFile")).trim()); //this isn't something we save from time to time
-    this.setProperty("outputFormat", props.getProperty("outputFormat", this.getProperty("outputFormat")).trim()); //this isn't something we save from time to time
-    this.setProperty("outputFormatOptions", props.getProperty("outputFormatOptions", this.getProperty("outputFormatOptions")).trim()); //this isn't something we save from time to time
-    this.setProperty("nthreads", props.getProperty("nthreads", this.getProperty("nthreads", NTHREADS)).trim());
-    String sentenceDelimiter = props.getProperty("sentenceDelimiter", this.getProperty("sentenceDelimiter"));
-    if (sentenceDelimiter != null) {
-      // this isn't something we save from time to time.
-      // It is only relevant when tagging text files.
-      // In fact, we let this one be null, as it really is useful to
-      // let the null value represent no sentence delimiter.
-      this.setProperty("sentenceDelimiter", sentenceDelimiter);
-    }
-  }
+        this.setProperty("tagInside", props.getProperty("tagInside", this.getProperty("tagInside"))); //this isn't something we save from time to time
+        this.setProperty("approximate", props.getProperty("approximate", this.getProperty("approximate"))); //this isn't something we save from time to time
+        this.setProperty("tokenizerOptions", props.getProperty("tokenizerOptions", this.getProperty("tokenizerOptions"))); //this isn't something we save from time to time
+        this.setProperty("outputFile", props.getProperty("outputFile", this.getProperty("outputFile")).trim()); //this isn't something we save from time to time
+        this.setProperty("outputFormat", props.getProperty("outputFormat", this.getProperty("outputFormat")).trim()); //this isn't something we save from time to time
+        this.setProperty("outputFormatOptions", props.getProperty("outputFormatOptions", this.getProperty("outputFormatOptions")).trim()); //this isn't something we save from time to time
+        this.setProperty("nthreads", props.getProperty("nthreads", this.getProperty("nthreads", NTHREADS)).trim());
+        String sentenceDelimiter = props.getProperty("sentenceDelimiter", this.getProperty("sentenceDelimiter"));
+        if (sentenceDelimiter != null) {
+            // this isn't something we save from time to time.
+            // It is only relevant when tagging text files.
+            // In fact, we let this one be null, as it really is useful to
+            // let the null value represent no sentence delimiter.
+            this.setProperty("sentenceDelimiter", sentenceDelimiter);
+        }
+    }
 
 
-  public String getModel() { return getProperty("model"); }
+    public String getModel() {
+        return getProperty("model");
+    }
 
-  public String getFile() { return getProperty("file"); }
+    public String getFile() {
+        return getProperty("file");
+    }
 
-  public String getOutputFile() { return getProperty("outputFile"); }
+    public String getOutputFile() {
+        return getProperty("outputFile");
+    }
 
-  public String getOutputFormat() { return getProperty("outputFormat"); }
+    public String getOutputFormat() {
+        return getProperty("outputFormat");
+    }
 
-  public String[] getOutputOptions() { return getProperty("outputFormatOptions").split("\\s*,\\s*"); }
+    public String[] getOutputOptions() {
+        return getProperty("outputFormatOptions").split("\\s*,\\s*");
+    }
 
-  public boolean getOutputVerbosity() {
-    return getOutputOptionsContains("verbose");
-  }
+    public boolean getOutputVerbosity() {
+        return getOutputOptionsContains("verbose");
+    }
 
-  public boolean getOutputLemmas() {
-    return getOutputOptionsContains("lemmatize");
-  }
+    public boolean getOutputLemmas() {
+        return getOutputOptionsContains("lemmatize");
+    }
 
-  public boolean keepEmptySentences() {
-    return getOutputOptionsContains("keepEmptySentences");
-  }
+    public boolean keepEmptySentences() {
+        return getOutputOptionsContains("keepEmptySentences");
+    }
 
-  public boolean getOutputOptionsContains(String sought) {
-    String[] options = getOutputOptions();
-    for (String option : options) {
-      if (option.equals(sought)) {
-        return true;
-      }
-    }
-    return false;
-  }
+    public boolean getOutputOptionsContains(String sought) {
+        String[] options = getOutputOptions();
+        for (String option : options) {
+            if (option.equals(sought)) {
+                return true;
+            }
+        }
+        return false;
+    }
 
-  public String getSearch() { return getProperty("search"); }
+    public String getSearch() {
+        return getProperty("search");
+    }
 
-  public double getSigmaSquared() { return Double.parseDouble(getProperty("sigmaSquared")); }
+    public double getSigmaSquared() {
+        return Double.parseDouble(getProperty("sigmaSquared"));
+    }
 
-  public int getIterations() { return Integer.parseInt(getProperty("iterations")); }
+    public int getIterations() {
+        return Integer.parseInt(getProperty("iterations"));
+    }
 
-  public int getRareWordThresh() { return Integer.parseInt(getProperty("rareWordThresh")); }
+    public int getRareWordThresh() {
+        return Integer.parseInt(getProperty("rareWordThresh"));
+    }
 
-  public int getMinFeatureThresh() { return Integer.parseInt(getProperty("minFeatureThresh")); }
+    public int getMinFeatureThresh() {
+        return Integer.parseInt(getProperty("minFeatureThresh"));
+    }
 
-  public int getCurWordMinFeatureThresh() { return Integer.parseInt(getProperty("curWordMinFeatureThresh")); }
+    public int getCurWordMinFeatureThresh() {
+        return Integer.parseInt(getProperty("curWordMinFeatureThresh"));
+    }
 
-  public int getRareWordMinFeatureThresh() { return Integer.parseInt(getProperty("rareWordMinFeatureThresh")); }
+    public int getRareWordMinFeatureThresh() {
+        return Integer.parseInt(getProperty("rareWordMinFeatureThresh"));
+    }
 
-  public int getVeryCommonWordThresh() { return Integer.parseInt(getProperty("veryCommonWordThresh")); }
+    public int getVeryCommonWordThresh() {
+        return Integer.parseInt(getProperty("veryCommonWordThresh"));
+    }
 
-  public boolean occurringTagsOnly() { return Boolean.parseBoolean(getProperty("occurringTagsOnly")); }
+    public boolean occurringTagsOnly() {
+        return Boolean.parseBoolean(getProperty("occurringTagsOnly"));
+    }
 
-  public boolean possibleTagsOnly() { return Boolean.parseBoolean(getProperty("possibleTagsOnly")); }
+    public boolean possibleTagsOnly() {
+        return Boolean.parseBoolean(getProperty("possibleTagsOnly"));
+    }
 
-  public String getLang() { return getProperty("lang"); }
+    public String getLang() {
+        return getProperty("lang");
+    }
 
-  public String[] getOpenClassTags() {
-    return wsvStringToStringArray(getProperty("openClassTags"));
-  }
+    public String[] getOpenClassTags() {
+        return wsvStringToStringArray(getProperty("openClassTags"));
+    }
 
-  public String[] getClosedClassTags() {
-    return wsvStringToStringArray(getProperty("closedClassTags"));
-  }
+    public String[] getClosedClassTags() {
+        return wsvStringToStringArray(getProperty("closedClassTags"));
+    }
 
-  private static String[] wsvStringToStringArray(String str) {
-    if (StringUtils.isNullOrEmpty(str)) {
-      return StringUtils.EMPTY_STRING_ARRAY;
-    } else {
-      return str.split("\\s+");
-    }
-  }
+    private static String[] wsvStringToStringArray(String str) {
+        if (StringUtils.isNullOrEmpty(str)) {
+            return StringUtils.EMPTY_STRING_ARRAY;
+        } else {
+            return str.split("\\s+");
+        }
+    }
 
-  public boolean getLearnClosedClassTags() { return Boolean.parseBoolean(getProperty("learnClosedClassTags")); }
+    public boolean getLearnClosedClassTags() {
+        return Boolean.parseBoolean(getProperty("learnClosedClassTags"));
+    }
 
-  public int getClosedTagThreshold() { return Integer.parseInt(getProperty("closedClassTagThreshold")); }
+    public int getClosedTagThreshold() {
+        return Integer.parseInt(getProperty("closedClassTagThreshold"));
+    }
 
-  public String getArch() { return getProperty("arch"); }
+    public boolean getDoDeterministicTagExpansion() {
+        return Boolean.parseBoolean(getProperty("doDeterministicTagExpansion"));
+    }
 
-  public String getWordFunction() { return getProperty("wordFunction"); }
+    public String getArch() {
+        return getProperty("arch");
+    }
 
-  public boolean getDebug() { return Boolean.parseBoolean(getProperty("debug")); }
+    public String getWordFunction() {
+        return getProperty("wordFunction");
+    }
 
-  public String getDebugPrefix() { return getProperty("debugPrefix"); }
+    public boolean getDebug() {
+        return Boolean.parseBoolean(getProperty("debug"));
+    }
 
-  public String getTokenizerFactory() { return getProperty("tokenizerFactory"); }
+    public String getDebugPrefix() {
+        return getProperty("debugPrefix");
+    }
+    public String getTagExpansionRuleFile() {
+        return getProperty("tagExpansionRuleFile");
+    }
 
-  public static String getDefaultTagSeparator() { return TAG_SEPARATOR; }
+    public String getTokenizerFactory() {
+        return getProperty("tokenizerFactory");
+    }
 
-  public final String getTagSeparator() { return getProperty(TAG_SEPARATOR_PROPERTY); }
+    static String getDefaultTagSeparator() {
+        return TAG_SEPARATOR;
+    }
 
-  public boolean getTokenize() { return Boolean.parseBoolean(getProperty("tokenize")); }
+    public final String getTagSeparator() {
+        return getProperty(TAG_SEPARATOR_PROPERTY);
+    }
 
-  public String getEncoding() { return getProperty(ENCODING_PROPERTY); }
+    public boolean getTokenize() {
+        return Boolean.parseBoolean(getProperty("tokenize"));
+    }
 
-  public double getRegL1() { return Double.parseDouble(getProperty("regL1")); }
+    public String getEncoding() {
+        return getProperty(ENCODING_PROPERTY);
+    }
 
-  public String[] getXMLInput() {
-    return wsvStringToStringArray(getProperty("xmlInput"));
-  }
+    public double getRegL1() {
+        return Double.parseDouble(getProperty("regL1"));
+    }
+
+    String[] getXMLInput() {
+        return wsvStringToStringArray(getProperty("xmlInput"));
+    }
 
-  public boolean getVerbose() { return Boolean.parseBoolean(getProperty("verbose")); }
+    public boolean getVerbose() {
+        return Boolean.parseBoolean(getProperty("verbose"));
+    }
 
-  public boolean getVerboseResults() { return Boolean.parseBoolean(getProperty("verboseResults")); }
+    public boolean getVerboseResults() {
+        return Boolean.parseBoolean(getProperty("verboseResults"));
+    }
 
-  public boolean getSGML() { return Boolean.parseBoolean(getProperty("sgml")); }
+    public boolean getSGML() {
+        return Boolean.parseBoolean(getProperty("sgml"));
+    }
 
-  public int getNThreads() { return Integer.parseInt(getProperty("nthreads")); }
+    public int getNThreads() {
+        return Integer.parseInt(getProperty("nthreads"));
+    }
 
 
-  /** Return a regex of XML elements to tag inside of.  This may return an
-   *  empty String, but never null.
-   *
-   * @return A regex of XML elements to tag inside of
-   */
-  public String getTagInside() {
-    String str = getProperty("tagInside");
-    if (str == null) {
-      return "";
-    }
-    return str;
-  }
+    /**
+     * Return a regex of XML elements to tag inside of.  This may return an
+     * empty String, but never null.
+     *
+     * @return A regex of XML elements to tag inside of
+     */
+    public String getTagInside() {
+        String str = getProperty("tagInside");
+        if (str == null) {
+            return "";
+        }
+        return str;
+    }
 
-  public String getTokenizerOptions() { return getProperty("tokenizerOptions"); }
+    public String getTokenizerOptions() {
+        return getProperty("tokenizerOptions");
+    }
 
-  public boolean getTokenizerInvertible() {
-    String tokenizerOptions = getTokenizerOptions();
-    if (tokenizerOptions != null &&
-        tokenizerOptions.matches("(^|.*,)invertible=true"))
-      return true;
-    return getOutputVerbosity() || getOutputLemmas();
-  }
+    boolean getTokenizerInvertible() {
+        String tokenizerOptions = getTokenizerOptions();
+        if (tokenizerOptions != null &&
+                tokenizerOptions.matches("(^|.*,)invertible=true"))
+            return true;
+        return getOutputVerbosity() || getOutputLemmas();
+    }
 
-  /**
-   * Returns a default score to be used for each tag that is incompatible with
-   * the current word (e.g., the tag CC for the word "apple"). Using a default
-   * score may slightly decrease performance for some languages (e.g., Chinese and
-   * German), but allows the tagger to run considerably faster (since the computation
-   * of the normalization term Z requires much less feature extraction). This approximation
-   * does not decrease performance in English (on the WSJ). If this function returns
-   * 0.0, the tagger will compute exact scores.
-   *
-   * @return default score
-   */
-  public double getDefaultScore() {
-    String approx = getProperty("approximate");
-    if (approx == null) {
-      return getLang().equals("english") ? 1.0 : 0.0;
-    } else if ("false".equalsIgnoreCase(approx)) {
-      return -1.0;
-    } else if ("true".equalsIgnoreCase(approx)) {
-      return 1.0;
-    } else {
-      return Double.parseDouble(approx);
-    }
-  }
+    /**
+     * Returns a default score to be used for each tag that is incompatible with
+     * the current word (e.g., the tag CC for the word "apple"). Using a default
+     * score may slightly decrease performance for some languages (e.g., Chinese and
+     * German), but allows the tagger to run considerably faster (since the computation
+     * of the normalization term Z requires much less feature extraction). This approximation
+     * does not decrease performance in English (on the WSJ). If this function returns
+     * 0.0, the tagger will compute exact scores.
+     *
+     * @return default score
+     */
+    double getDefaultScore() {
+        String approx = getProperty("approximate");
+        if (approx == null) {
+            return getLang().equals("english") ? 1.0 : 0.0;
+        } else if ("false".equalsIgnoreCase(approx)) {
+            return -1.0;
+        } else if ("true".equalsIgnoreCase(approx)) {
+            return 1.0;
+        } else {
+            return Double.parseDouble(approx);
+        }
+    }
 
 
-  public void dump() { dump(new PrintWriter(System.err)); }
+    public void dump() {
+        dump(new PrintWriter(System.err));
+    }
 
-  public void dump(PrintStream stream) {
-    PrintWriter pw = new PrintWriter(stream);
-    dump(pw);
-  }
+    public void dump(PrintStream stream) {
+        PrintWriter pw = new PrintWriter(stream);
+        dump(pw);
+    }
 
-  private void dump(PrintWriter pw) {
-    pw.println("                   model = " + getProperty("model"));
-    pw.println("                    arch = " + getProperty("arch"));
-    pw.println("            wordFunction = " + getProperty("wordFunction"));
-    if (this.getMode() == Mode.TRAIN || this.getMode() == Mode.DUMP) {
-      pw.println("               trainFile = " + getProperty("file"));
-    } else if (this.getMode() == Mode.TAG) {
-      pw.println("                textFile = " + getProperty("file"));
-    } else if (this.getMode() == Mode.TEST) {
-      pw.println("                testFile = " + getProperty("file"));
-    }
+    private void dump(PrintWriter pw) {
+        pw.println("                   model = " + getProperty("model"));
+        pw.println("                    arch = " + getProperty("arch"));
+        pw.println("            wordFunction = " + getProperty("wordFunction"));
+        if (this.getMode() == Mode.TRAIN || this.getMode() == Mode.DUMP) {
+            pw.println("               trainFile = " + getProperty("file"));
+        } else if (this.getMode() == Mode.TAG) {
+            pw.println("                textFile = " + getProperty("file"));
+        } else if (this.getMode() == Mode.TEST) {
+            pw.println("                testFile = " + getProperty("file"));
+        }
 
-    pw.println("         closedClassTags = " + getProperty("closedClassTags"));
-    pw.println(" closedClassTagThreshold = " + getProperty("closedClassTagThreshold"));
-    pw.println(" curWordMinFeatureThresh = " + getProperty("curWordMinFeatureThresh"));
-    pw.println("                   debug = " + getProperty("debug"));
-    pw.println("             debugPrefix = " + getProperty("debugPrefix"));
-    pw.println("            " + TAG_SEPARATOR_PROPERTY + " = " +
-               getProperty(TAG_SEPARATOR_PROPERTY));
-    pw.println("                " + ENCODING_PROPERTY + " = " +
-               getProperty(ENCODING_PROPERTY));
-    pw.println("              iterations = " + getProperty("iterations"));
-    pw.println("                    lang = " + getProperty("lang"));
-    pw.println("    learnClosedClassTags = " + getProperty("learnClosedClassTags"));
-    pw.println("        minFeatureThresh = " + getProperty("minFeatureThresh"));
-    pw.println("           openClassTags = " + getProperty("openClassTags"));
-    pw.println("rareWordMinFeatureThresh = " + getProperty("rareWordMinFeatureThresh"));
-    pw.println("          rareWordThresh = " + getProperty("rareWordThresh"));
-    pw.println("                  search = " + getProperty("search"));
-    pw.println("                    sgml = " + getProperty("sgml"));
-    pw.println("            sigmaSquared = " + getProperty("sigmaSquared"));
-    pw.println("                   regL1 = " + getProperty("regL1"));
-    pw.println("               tagInside = " + getProperty("tagInside"));
-    pw.println("                tokenize = " + getProperty("tokenize"));
-    pw.println("        tokenizerFactory = " + getProperty("tokenizerFactory"));
-    pw.println("        tokenizerOptions = " + getProperty("tokenizerOptions"));
-    pw.println("                 verbose = " + getProperty("verbose"));
-    pw.println("          verboseResults = " + getProperty("verboseResults"));
-    pw.println("    veryCommonWordThresh = " + getProperty("veryCommonWordThresh"));
-    pw.println("                xmlInput = " + getProperty("xmlInput"));
-    pw.println("              outputFile = " + getProperty("outputFile"));
-    pw.println("            outputFormat = " + getProperty("outputFormat"));
-    pw.println("     outputFormatOptions = " + getProperty("outputFormatOptions"));
-    pw.println("                nthreads = " + getProperty("nthreads"));
-    pw.flush();
-  }
+        pw.println("         closedClassTags = " + getProperty("closedClassTags"));
+        pw.println(" closedClassTagThreshold = " + getProperty("closedClassTagThreshold"));
+        pw.println("doDeterministicTagExpansion = " + getProperty("doDeterministicTagExpansion"));
+        pw.println(" curWordMinFeatureThresh = " + getProperty("curWordMinFeatureThresh"));
+        pw.println("                   debug = " + getProperty("debug"));
+        pw.println("             debugPrefix = " + getProperty("debugPrefix"));
+        pw.println("    tagExpansionRuleFile = " + getProperty("tagExpansionRuleFile"));
+        pw.println("            " + TAG_SEPARATOR_PROPERTY + " = " +
+                getProperty(TAG_SEPARATOR_PROPERTY));
+        pw.println("                " + ENCODING_PROPERTY + " = " +
+                getProperty(ENCODING_PROPERTY));
+        pw.println("              iterations = " + getProperty("iterations"));
+        pw.println("                    lang = " + getProperty("lang"));
+        pw.println("    learnClosedClassTags = " + getProperty("learnClosedClassTags"));
+        pw.println("        minFeatureThresh = " + getProperty("minFeatureThresh"));
+        pw.println("           openClassTags = " + getProperty("openClassTags"));
+        pw.println("rareWordMinFeatureThresh = " + getProperty("rareWordMinFeatureThresh"));
+        pw.println("          rareWordThresh = " + getProperty("rareWordThresh"));
+        pw.println("                  search = " + getProperty("search"));
+        pw.println("                    sgml = " + getProperty("sgml"));
+        pw.println("            sigmaSquared = " + getProperty("sigmaSquared"));
+        pw.println("                   regL1 = " + getProperty("regL1"));
+        pw.println("               tagInside = " + getProperty("tagInside"));
+        pw.println("                tokenize = " + getProperty("tokenize"));
+        pw.println("        tokenizerFactory = " + getProperty("tokenizerFactory"));
+        pw.println("        tokenizerOptions = " + getProperty("tokenizerOptions"));
+        pw.println("                 verbose = " + getProperty("verbose"));
+        pw.println("          verboseResults = " + getProperty("verboseResults"));
+        pw.println("    veryCommonWordThresh = " + getProperty("veryCommonWordThresh"));
+        pw.println("                xmlInput = " + getProperty("xmlInput"));
+        pw.println("              outputFile = " + getProperty("outputFile"));
+        pw.println("            outputFormat = " + getProperty("outputFormat"));
+        pw.println("     outputFormatOptions = " + getProperty("outputFormatOptions"));
+        pw.println("                nthreads = " + getProperty("nthreads"));
+        pw.flush();
+    }
 
-  @Override
-  public String toString() {
-    StringWriter sw = new StringWriter(200);
-    PrintWriter pw = new PrintWriter(sw);
-    dump(pw);
-    return sw.toString();
-  }
+    @Override
+    public String toString() {
+        StringWriter sw = new StringWriter(200);
+        PrintWriter pw = new PrintWriter(sw);
+        dump(pw);
+        return sw.toString();
+    }
 
-  /**
-   * This returns the sentence delimiter used when tokenizing text
-   * using the tokenizer requested in this config.  In general, it is
-   * assumed the tokenizer doesn't need a sentence delimiter.... If you
-   * use the whitespace tokenizer, though, a newline breaks sentences.
-   *
-   * @return A null String unless tokenize is false and then the String
-   */
-  public String getSentenceDelimiter() {
-    String delimiter = getProperty("sentenceDelimiter");
-    if (delimiter == null && !getTokenize()) {
-      delimiter = "\n";
-    }
-    return delimiter;
-  }
+    /**
+     * This returns the sentence delimiter used when tokenizing text
+     * using the tokenizer requested in this config.  In general, it is
+     * assumed the tokenizer doesn't need a sentence delimiter.... If you
+     * use the whitespace tokenizer, though, a newline breaks sentences.
+     *
+     * @return A null String unless tokenize is false and then the String
+     */
+    public String getSentenceDelimiter() {
+        String delimiter = getProperty("sentenceDelimiter");
+        if (delimiter == null && !getTokenize()) {
+            delimiter = "\n";
+        }
+        return delimiter;
+    }
 
-  /**
-   * Returns whether or not we should use stdin for reading when
-   * tagging data.  For now, this returns true iff the filename given
-   * was "stdin".
-   * (TODO: kind of ugly)
-   */
-  public boolean useStdin() {
-    return getFile().trim().equalsIgnoreCase("stdin");
-  }
+    /**
+     * Returns whether or not we should use stdin for reading when
+     * tagging data.  For now, this returns true iff the filename given
+     * was "stdin".
+     * (TODO: kind of ugly)
+     */
+    public boolean useStdin() {
+        return getFile().trim().equalsIgnoreCase("stdin");
+    }
 
-  /**
-   * Prints out the automatically generated props file - in its own
-   * method to make code above easier to read
-   */
-  private static void printGenProps(PrintStream out) {
-    out.println("## Sample properties file for maxent tagger. This file is used for three main");
-    out.println("## operations: training, testing, and tagging. It may also be used to dump");
-    out.println("## the contents of a model.");
-    out.println("## To train or test a model, or to tag something, run:");
-    out.println("##   java edu.stanford.nlp.tagger.maxent.MaxentTagger -prop <properties file>");
-    out.println("## Arguments can be overridden on the commandline, e.g.:");
-    out.println("##   java ....MaxentTagger -prop <properties file> -testFile /other/file ");
-    out.println();
+    /**
+     * Prints out the automatically generated props file - in its own
+     * method to make code above easier to read
+     */
+    private static void printGenProps(PrintStream out) {
+        out.println("## Sample properties file for maxent tagger. This file is used for three main");
+        out.println("## operations: training, testing, and tagging. It may also be used to dump");
+        out.println("## the contents of a model.");
+        out.println("## To train or test a model, or to tag something, run:");
+        out.println("##   java edu.stanford.nlp.tagger.maxent.MaxentTagger -prop <properties file>");
+        out.println("## Arguments can be overridden on the commandline, e.g.:");
+        out.println("##   java ....MaxentTagger -prop <properties file> -testFile /other/file ");
+        out.println();
 
-    out.println("# Model file name (created at train time; used at tag and test time)");
-    out.println("# (you can leave this blank and specify it on the commandline with -model)");
-    out.println("# model = ");
-    out.println();
+        out.println("# Model file name (created at train time; used at tag and test time)");
+        out.println("# (you can leave this blank and specify it on the commandline with -model)");
+        out.println("# model = ");
+        out.println();
 
-    out.println("# Path to file to be operated on (trained from, tested against, or tagged)");
-    out.println("# Specify -textFile <filename> to tag text in the given file, -trainFile <filename> to");
-    out.println("# to train a model using data in the given file, or -testFile <filename> to test your");
-    out.println("# model using data in the given file.  Alternatively, you may specify");
-    out.println("# -dump <filename> to dump the parameters stored in a model or ");
-    out.println("# -convertToSingleFile <filename> to save an old, multi-file model (specified as -model)");
-    out.println("# to the new single file format.  The new model will be saved in the file filename.");
-    out.println("# If you choose to convert an old file, you must specify ");
-    out.println("# the correct 'arch' parameter used to create the original model.");
-    out.println("# trainFile = ");
-    out.println();
+        out.println("# Path to file to be operated on (trained from, tested against, or tagged)");
+        out.println("# Specify -textFile <filename> to tag text in the given file, -trainFile <filename> to");
+        out.println("# to train a model using data in the given file, or -testFile <filename> to test your");
+        out.println("# model using data in the given file.  Alternatively, you may specify");
+        out.println("# -dump <filename> to dump the parameters stored in a model or ");
+        out.println("# -convertToSingleFile <filename> to save an old, multi-file model (specified as -model)");
+        out.println("# to the new single file format.  The new model will be saved in the file filename.");
+        out.println("# If you choose to convert an old file, you must specify ");
+        out.println("# the correct 'arch' parameter used to create the original model.");
+        out.println("# trainFile = ");
+        out.println();
 
-    out.println("# Path to outputFile to write tagged output to.");
-    out.println("# If empty, stdout is used.");
-    out.println("# outputFile = " + OUTPUT_FILE);
-    out.println();
+        out.println("# Path to outputFile to write tagged output to.");
+        out.println("# If empty, stdout is used.");
+        out.println("# outputFile = " + OUTPUT_FILE);
+        out.println();
 
-    out.println("# Output format. One of: slashTags (default), xml, or tsv");
-    out.println("# outputFormat = " + OUTPUT_FORMAT);
-    out.println();
+        out.println("# Output format. One of: slashTags (default), xml, or tsv");
+        out.println("# outputFormat = " + OUTPUT_FORMAT);
+        out.println();
 
-    out.println("# Output format options. Comma separated list.");
-    out.println("# currently \"lemmatize\" and \"keepEmptySentences\" are supported.");
-    out.println("# outputFormatOptions = " + OUTPUT_FORMAT_OPTIONS);
-    out.println();
+        out.println("# Output format options. Comma separated list.");
+        out.println("# currently \"lemmatize\" and \"keepEmptySentences\" are supported.");
+        out.println("# outputFormatOptions = " + OUTPUT_FORMAT_OPTIONS);
+        out.println();
 
-    out.println("# Tag separator character that separates word and pos tags");
-    out.println("# (for both training and test data) and used for");
-    out.println("# separating words and tags in slashTags format output.");
-    out.println("# tagSeparator = " + TAG_SEPARATOR);
-    out.println();
+        out.println("# Tag separator character that separates word and pos tags");
+        out.println("# (for both training and test data) and used for");
+        out.println("# separating words and tags in slashTags format output.");
+        out.println("# tagSeparator = " + TAG_SEPARATOR);
+        out.println();
 
-    out.println("# Encoding format in which files are stored.  If left blank, UTF-8 is assumed.");
-    out.println("# encoding = " + ENCODING);
-    out.println();
+        out.println("# Encoding format in which files are stored.  If left blank, UTF-8 is assumed.");
+        out.println("# encoding = " + ENCODING);
+        out.println();
 
-    out.println("# A couple flags for controlling the amount of output:");
-    out.println("# - print extra debugging information:");
-    out.println("# verbose = " + VERBOSE);
-    out.println("# - print intermediate results:");
-    out.println("# verboseResults = " + VERBOSE_RESULTS);
+        out.println("# A couple flags for controlling the amount of output:");
+        out.println("# - print extra debugging information:");
+        out.println("# verbose = " + VERBOSE);
+        out.println("# - print intermediate results:");
+        out.println("# verboseResults = " + VERBOSE_RESULTS);
 
-    out.println("######### parameters for tag and test operations #########");
-    out.println();
+        out.println("######### parameters for tag and test operations #########");
+        out.println();
 
-    out.println("# Class to use for tokenization. Default blank value means Penn Treebank");
-    out.println("# tokenization.  If you'd like to just assume that tokenization has been done,");
-    out.println("# and the input is whitespace-tokenized, use");
-    out.println("# edu.stanford.nlp.process.WhitespaceTokenizer or set tokenize to false.");
-    out.println("# tokenizerFactory = ");
-    out.println();
+        out.println("# Class to use for tokenization. Default blank value means Penn Treebank");
+        out.println("# tokenization.  If you'd like to just assume that tokenization has been done,");
+        out.println("# and the input is whitespace-tokenized, use");
+        out.println("# edu.stanford.nlp.process.WhitespaceTokenizer or set tokenize to false.");
+        out.println("# tokenizerFactory = ");
+        out.println();
 
-    out.println("# Options to the tokenizer.  A comma separated list.");
-    out.println("# This depends on what the tokenizer supports.");
-    out.println("# For PTBTokenizer, you might try options like americanize=false");
-    out.println("# or asciiQuotes (for German!).");
-    out.println("# tokenizerOptions = ");
-    out.println();
-    out.println("# Whether to tokenize text for tag and test operations. Default is true.");
-    out.println("# If false, your text must already be whitespace tokenized.");
-    out.println("# tokenize = " + TOKENIZE);
-    out.println();
+        out.println("# Options to the tokenizer.  A comma separated list.");
+        out.println("# This depends on what the tokenizer supports.");
+        out.println("# For PTBTokenizer, you might try options like americanize=false");
+        out.println("# or asciiQuotes (for German!).");
+        out.println("# tokenizerOptions = ");
+        out.println();
+        out.println("# Whether to tokenize text for tag and test operations. Default is true.");
+        out.println("# If false, your text must already be whitespace tokenized.");
+        out.println("# tokenize = " + TOKENIZE);
+        out.println();
 
-    out.println("# Write debugging information (words, top words, unknown words). Useful for");
-    out.println("# error analysis. Default is false.");
-    out.println("# debug = "+ DEBUG);
-    out.println();
+        out.println("# Write debugging information (words, top words, unknown words). Useful for");
+        out.println("# error analysis. Default is false.");
+        out.println("# debug = " + DEBUG);
+        out.println();
 
-    out.println("# Prefix for debugging output (if debug == true). Default is to use the");
-    out.println("# filename from 'file'");
-    out.println("# debugPrefix = ");
-    out.println();
+        out.println("# Prefix for debugging output (if debug == true). Default is to use the");
+        out.println("# filename from 'file'");
+        out.println("# debugPrefix = ");
+        out.println("# tagExpansionRuleFile = ");
+        out.println();
 
-    out.println("######### parameters for training  #########");
-    out.println();
+        out.println("######### parameters for training  #########");
+        out.println();
 
-    out.println("# model architecture: This is one or more comma separated strings, which");
-    out.println("# specify which extractors to use. Some of them take one or more integer");
-    out.println("# or string ");
-    out.println("# (file path) arguments in parentheses, written as m, n, and s below:");
-    out.println("# 'left3words', 'left5words', 'bidirectional', 'bidirectional5words',");
-    out.println("# 'generic', 'sighan2005', 'german', 'words(m,n)', 'wordshapes(m,n)',");
-    out.println("# 'biwords(m,n)', 'lowercasewords(m,n)', 'vbn(n)', distsimconjunction(s,m,n)',");
-    out.println("# 'naacl2003unknowns', 'naacl2003conjunctions', 'distsim(s,m,n)',");
-    out.println("# 'suffix(n)', 'prefix(n)', 'prefixsuffix(n)', 'capitalizationsuffix(n)',");
-    out.println("# 'wordshapes(m,n)', 'unicodeshapes(m,n)', 'unicodeshapeconjunction(m,n)',");
-    out.println("# 'lctagfeatures', 'order(k)', 'chinesedictionaryfeatures(s)'.");
-    out.println("# These keywords determines the features extracted.  'generic' is language independent.");
-    out.println("# distsim: Distributional similarity classes can be an added source of information");
-    out.println("# about your words. An English distsim file is included, or you can use your own.");
-    out.println("# arch = ");
-    out.println();
-    out.println("# 'wordFunction'.  A function applied to the text before training or tagging.");
-    out.println("# For example, edu.stanford.nlp.util.LowercaseFunction");
-    out.println("# This function turns all the words into lowercase");
-    out.println("# The function must implement java.util.function.Function<String, String>");
-    out.println("# Blank means no preprocessing function");
-    out.println("# wordFunction = ");
-    out.println();
+        out.println("# model architecture: This is one or more comma separated strings, which");
+        out.println("# specify which extractors to use. Some of them take one or more integer");
+        out.println("# or string ");
+        out.println("# (file path) arguments in parentheses, written as m, n, and s below:");
+        out.println("# 'left3words', 'left5words', 'bidirectional', 'bidirectional5words',");
+        out.println("# 'generic', 'sighan2005', 'german', 'words(m,n)', 'wordshapes(m,n)',");
+        out.println("# 'biwords(m,n)', 'lowercasewords(m,n)', 'vbn(n)', distsimconjunction(s,m,n)',");
+        out.println("# 'naacl2003unknowns', 'naacl2003conjunctions', 'distsim(s,m,n)',");
+        out.println("# 'suffix(n)', 'prefix(n)', 'prefixsuffix(n)', 'capitalizationsuffix(n)',");
+        out.println("# 'wordshapes(m,n)', 'unicodeshapes(m,n)', 'unicodeshapeconjunction(m,n)',");
+        out.println("# 'lctagfeatures', 'order(k)', 'chinesedictionaryfeatures(s)'.");
+        out.println("# These keywords determines the features extracted.  'generic' is language independent.");
+        out.println("# distsim: Distributional similarity classes can be an added source of information");
+        out.println("# about your words. An English distsim file is included, or you can use your own.");
+        out.println("# arch = ");
+        out.println();
+        out.println("# 'wordFunction'.  A function applied to the text before training or tagging.");
+        out.println("# For example, edu.stanford.nlp.util.LowercaseFunction");
+        out.println("# This function turns all the words into lowercase");
+        out.println("# The function must implement java.util.function.Function<String, String>");
+        out.println("# Blank means no preprocessing function");
+        out.println("# wordFunction = ");
+        out.println();
 
 
-    out.println("# 'language'.  This is really the tag set which is used for the");
-    out.println("# list of open-class tags, and perhaps deterministic  tag");
-    out.println("# expansion). Currently we have 'english', 'arabic', 'german', 'chinese'");
-    out.println("# or 'polish' predefined. For your own language, you can specify ");
-    out.println("# the same information via openClassTags or closedClassTags below");
-    out.println("# (only ONE of these three options may be specified). ");
-    out.println("# 'english' means UPenn English treebank tags. 'german' is STTS");
-    out.println("# 'chinese' is CTB, and Arabic is an expanded Bies mapping from the ATB");
-    out.println("# 'polish' means some tags that some guy on the internet once used. ");
-    out.println("# See the TTags class for more information.");
-    out.println("# lang = ");
-    out.println();
+        out.println("# 'language'.  This is really the tag set which is used for the");
+        out.println("# list of open-class tags, and perhaps deterministic  tag");
+        out.println("# expansion). Currently we have 'english', 'arabic', 'german', 'chinese'");
+        out.println("# or 'polish' predefined. For your own language, you can specify ");
+        out.println("# the same information via openClassTags or closedClassTags below");
+        out.println("# (only ONE of these three options may be specified). ");
+        out.println("# 'english' means UPenn English treebank tags. 'german' is STTS");
+        out.println("# 'chinese' is CTB, and Arabic is an expanded Bies mapping from the ATB");
+        out.println("# 'polish' means some tags that some guy on the internet once used. ");
+        out.println("# See the TTags class for more information.");
+        out.println("# lang = ");
+        out.println();
 
-    out.println("# a space-delimited list of open-class parts of speech");
-    out.println("# alternatively, you can specify language above to use a pre-defined list or specify the closed class tags (below)");
-    out.println("# openClassTags = ");
-    out.println();
+        out.println("# a space-delimited list of open-class parts of speech");
+        out.println("# alternatively, you can specify language above to use a pre-defined list or specify the closed class tags (below)");
+        out.println("# openClassTags = ");
+        out.println();
 
-    out.println("# a space-delimited list of closed-class parts of speech");
-    out.println("# alternatively, you can specify language above to use a pre-defined list or specify the open class tags (above)");
-    out.println("# closedClassTags = ");
-    out.println();
+        out.println("# a space-delimited list of closed-class parts of speech");
+        out.println("# alternatively, you can specify language above to use a pre-defined list or specify the open class tags (above)");
+        out.println("# closedClassTags = ");
+        out.println();
 
-    out.println("# A boolean indicating whether you would like the trained model to set POS tags as closed");
-    out.println("# based on their frequency in training; default is false.  The frequency threshold can be set below. ");
-    out.println("# This option is ignored if any of {openClassTags, closedClassTags, lang} are specified.");
-    out.println("# learnClosedClassTags = ");
-    out.println();
+        out.println("# A boolean indicating whether you would like the trained model to set POS tags as closed");
+        out.println("# based on their frequency in training; default is false.  The frequency threshold can be set below. ");
+        out.println("# This option is ignored if any of {openClassTags, closedClassTags, lang} are specified.");
+        out.println("# learnClosedClassTags = ");
+        out.println();
 
-    out.println("# Used only if learnClosedClassTags=true.  Tags that have fewer tokens than this threshold are");
-    out.println("# considered closed in the trained model.");
-    out.println("# closedClassTagThreshold = ");
-    out.println();
+        out.println("# Used only if learnClosedClassTags=true.  Tags that have fewer tokens than this threshold are");
+        out.println("# considered closed in the trained model.");
+        out.println("# closedClassTagThreshold = ");
+        out.println("# doDeterministicTagExpansion = ");
+        out.println();
 
-    out.println("# search method for optimization. Normally use the default 'qn'. choices: 'qn' (quasi-Newton),");
-    out.println("# 'cg' (conjugate gradient, 'owlqn' (L1 regularization) or 'iis' (improved iterative scaling)");
-    out.println("# search = " + SEARCH);
-    out.println();
+        out.println("# search method for optimization. Normally use the default 'qn'. choices: 'qn' (quasi-Newton),");
+        out.println("# 'cg' (conjugate gradient, 'owlqn' (L1 regularization) or 'iis' (improved iterative scaling)");
+        out.println("# search = " + SEARCH);
+        out.println();
 
-    out.println("# for conjugate gradient or quasi-Newton search, sigma-squared smoothing/regularization");
-    out.println("# parameter. if left blank, the default is 0.5, which is usually okay");
-    out.println("# sigmaSquared = " + SIGMA_SQUARED);
-    out.println();
+        out.println("# for conjugate gradient or quasi-Newton search, sigma-squared smoothing/regularization");
+        out.println("# parameter. if left blank, the default is 0.5, which is usually okay");
+        out.println("# sigmaSquared = " + SIGMA_SQUARED);
+        out.println();
 
-    out.println("# for OWLQN search, regularization");
-    out.println("# parameter. if left blank, the default is 1.0, which is usually okay");
-    out.println("# regL1 = " + DEFAULT_REG_L1);
-    out.println();
+        out.println("# for OWLQN search, regularization");
+        out.println("# parameter. if left blank, the default is 1.0, which is usually okay");
+        out.println("# regL1 = " + DEFAULT_REG_L1);
+        out.println();
 
-    out.println("# For improved iterative scaling, the number of iterations, otherwise ignored");
-    out.println("# iterations = " + ITERATIONS);
-    out.println();
+        out.println("# For improved iterative scaling, the number of iterations, otherwise ignored");
+        out.println("# iterations = " + ITERATIONS);
+        out.println();
 
-    out.println("# rare word threshold. words that occur less than this number of");
-    out.println("# times are considered rare words.");
-    out.println("# rareWordThresh = " + RARE_WORD_THRESH);
-    out.println();
+        out.println("# rare word threshold. words that occur less than this number of");
+        out.println("# times are considered rare words.");
+        out.println("# rareWordThresh = " + RARE_WORD_THRESH);
+        out.println();
 
-    out.println("# minimum feature threshold. features whose history appears less");
-    out.println("# than this number of times are ignored.");
-    out.println("# minFeatureThresh = " + MIN_FEATURE_THRESH);
-    out.println();
+        out.println("# minimum feature threshold. features whose history appears less");
+        out.println("# than this number of times are ignored.");
+        out.println("# minFeatureThresh = " + MIN_FEATURE_THRESH);
+        out.println();
 
-    out.println("# current word feature threshold. words that occur more than this");
-    out.println("# number of times will generate features with all of their occurring");
-    out.println("# tags.");
-    out.println("# curWordMinFeatureThresh = " + CUR_WORD_MIN_FEATURE_THRESH);
-    out.println();
+        out.println("# current word feature threshold. words that occur more than this");
+        out.println("# number of times will generate features with all of their occurring");
+        out.println("# tags.");
+        out.println("# curWordMinFeatureThresh = " + CUR_WORD_MIN_FEATURE_THRESH);
+        out.println();
 
-    out.println("# rare word minimum feature threshold. features of rare words whose histories");
-    out.println("# appear less than this times will be ignored.");
-    out.println("# rareWordMinFeatureThresh = " + RARE_WORD_MIN_FEATURE_THRESH);
-    out.println();
+        out.println("# rare word minimum feature threshold. features of rare words whose histories");
+        out.println("# appear less than this times will be ignored.");
+        out.println("# rareWordMinFeatureThresh = " + RARE_WORD_MIN_FEATURE_THRESH);
+        out.println();
 
-    out.println("# very common word threshold. words that occur more than this number of");
-    out.println("# times will form an equivalence class by themselves. ignored unless");
-    out.println("# you are using equivalence classes.");
-    out.println("# veryCommonWordThresh = " + VERY_COMMON_WORD_THRESH);
-    out.println();
+        out.println("# very common word threshold. words that occur more than this number of");
+        out.println("# times will form an equivalence class by themselves. ignored unless");
+        out.println("# you are using equivalence classes.");
+        out.println("# veryCommonWordThresh = " + VERY_COMMON_WORD_THRESH);
+        out.println();
 
-    out.println("# sgml = ");
-    out.println("# tagInside = ");
-    out.println();
+        out.println("# sgml = ");
+        out.println("# tagInside = ");
+        out.println();
 
-    out.println("# testFile and textFile can use multiple threads to process text.");
-    out.println("# nthreads = " + NTHREADS);
-  }
+        out.println("# testFile and textFile can use multiple threads to process text.");
+        out.println("# nthreads = " + NTHREADS);
+    }
 
-  public Mode getMode() {
-    if (!containsKey("mode")) {
-      return null;
-    }
-    return Mode.valueOf(getProperty("mode"));
-  }
+    public Mode getMode() {
+        if (!containsKey("mode")) {
+            return null;
+        }
+        return Mode.valueOf(getProperty("mode"));
+    }
 
 
-  /** Serialize the TaggerConfig.
-   *
-   * @param os Where to write this TaggerConfig
-   * @throws IOException If any IO problems
-   */
-  public void saveConfig(OutputStream os) throws IOException {
-    ObjectOutputStream out = new ObjectOutputStream(os);
-    out.writeObject(this);
-  }
+    /**
+     * Serialize the TaggerConfig.
+     *
+     * @param os Where to write this TaggerConfig
+     * @throws IOException If any IO problems
+     */
+    public void saveConfig(OutputStream os) throws IOException {
+        ObjectOutputStream out = new ObjectOutputStream(os);
+        out.writeObject(this);
+    }
 
 
-  /** Read in a TaggerConfig.
-   *
-   * @param stream Where to read from
-   * @return The TaggerConfig
-   * @throws IOException Misc IOError
-   * @throws ClassNotFoundException Class error
-   */
-  public static TaggerConfig readConfig(DataInputStream stream)
-    throws IOException, ClassNotFoundException
-  {
-    ObjectInputStream in = new ObjectInputStream(stream);
-    return (TaggerConfig) in.readObject();
-  }
+    /**
+     * Read in a TaggerConfig.
+     *
+     * @param stream Where to read from
+     * @return The TaggerConfig
+     * @throws IOException            Misc IOError
+     * @throws ClassNotFoundException Class error
+     */
+    public static TaggerConfig readConfig(DataInputStream stream)
+            throws IOException, ClassNotFoundException {
+        ObjectInputStream in = new ObjectInputStream(stream);
+        return (TaggerConfig) in.readObject();
+    }
 
 
 }
Index: src/edu/stanford/nlp/tagger/maxent/TestThreadedTagger.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/TestThreadedTagger.java	(revision 97f067c3dc5909bcacb3e2e7bc686fb3e1fedd82)
+++ src/edu/stanford/nlp/tagger/maxent/TestThreadedTagger.java	(revision ca2d288bef5de4c5fc1a917f86dd4a8f0ec9c774)
@@ -29,6 +29,7 @@
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Properties;
+
 import edu.stanford.nlp.util.Timing;
 import edu.stanford.nlp.util.StringUtils;
 
@@ -36,12 +37,12 @@
  * First, this runs a tagger once to see what results it comes up with.
  * Then it runs the same tagger in two separate threads to make sure the results are the same.
  * The results are printed to stdout; the user is expected to verify they are as expected.
- *
+ * <p>
  * Normally you would run MaxentTagger with command line arguments such as:
- *
+ * <p>
  * -model ../data/tagger/my-left3words-distsim-wsj-0-18.tagger
  * -testFile ../data/tagger/test-wsj-19-21 -verboseResults false
- *
+ * <p>
  * If you provide the same arguments to this program, it will first
  * run the given tagger on the given test file once to establish the
  * "baseline" results.  It will then run the same tagger in more than
@@ -49,7 +50,7 @@
  * the same if the MaxentTagger is re-entrant.  The number of threads
  * to be run can be specified with -numThreads; the default is
  * DEFAULT_NUM_THREADS.
- *
+ * <p>
  * You can also provide multiple models.  After performing that test
  * on model1, it will then run the same test file on model2, model3,
  * etc to establish baseline results for that tagger.  After that, it
@@ -58,9 +59,9 @@
  * should not have clobbered any static state initialized by the first
  * tagger.  Thus, the results of the two simultaneous taggers should
  * be the same as the two taggers' baselines.
- *
+ * <p>
  * Example arguments for the more complicated test:
- *
+ * <p>
  * -model1 ../data/pos-tagger/newmodels/left3words-distsim-wsj-0-18.tagger
  * -model2 ../data/pos-tagger/newmodels/left3words-wsj-0-18.tagger
  * -testFile ../data/pos-tagger/training/english/test-wsj-19-21
@@ -69,183 +70,186 @@
  * @author John Bauer
  */
 class TestThreadedTagger {
-  /**
-   * Default number of threads to launch in the first test.
-   * Can be specified with -numThreads.
-   */
-  static final int DEFAULT_NUM_THREADS = 2;
+    /**
+     * Default number of threads to launch in the first test.
+     * Can be specified with -numThreads.
+     */
+    static final int DEFAULT_NUM_THREADS = 2;
 
-  static final String THREAD_FLAG = "numThreads";
+    static final String THREAD_FLAG = "numThreads";
 
 
-  private TestThreadedTagger() {} // static methods
+    private TestThreadedTagger() {
+    } // static methods
 
 
-  /**
-   * This internal class takes a config, a tagger, and a thread name.
-   * The "run" method then runs the given tagger on the data file
-   * specified in the config.
-   */
-  private static class TaggerThread extends Thread {
+    /**
+     * This internal class takes a config, a tagger, and a thread name.
+     * The "run" method then runs the given tagger on the data file
+     * specified in the config.
+     */
+    private static class TaggerThread extends Thread {
 
-    private final MaxentTagger tagger;
-    private final String threadName;
+        private final MaxentTagger tagger;
+        private final String threadName;
 
-    private String resultsString = "";
-    public String getResultsString() { return resultsString; }
+        private String resultsString = "";
+
+        String getResultsString() {
+            return resultsString;
+        }
 
-    TaggerThread(MaxentTagger tagger, String name) {
-      this.tagger = tagger;
-      this.threadName = name;
-    }
+        TaggerThread(MaxentTagger tagger, String name) {
+            this.tagger = tagger;
+            this.threadName = name;
+        }
 
-    @Override
-    public void run() {
-      try {
-        Timing t = new Timing();
-        TestClassifier testClassifier = new TestClassifier(tagger);
-        long millis = t.stop();
-        resultsString = testClassifier.resultsString(tagger);
-        System.out.println("Thread " + threadName + " took " + millis +
-                           " milliseconds to tag " + testClassifier.getNumWords() +
-                           " words.\n" + resultsString);
-      } catch(IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  } // end class TaggerThread
+        @Override
+        public void run() {
+            try {
+                Timing t = new Timing();
+                TestClassifier testClassifier = new TestClassifier(tagger);
+                testClassifier.test();
+                long millis = t.stop();
+                System.out.println("Thread " + threadName + " took " + millis +
+                        " milliseconds to tag " + testClassifier.getNumWords() +
+                        " words.\n" + resultsString);
+                testClassifier.printModelAndAccuracy(tagger);
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    } // end class TaggerThread
 
-  public static void compareResults(String results, String baseline) {
-    if (!results.equals(baseline)) {
-      throw new RuntimeException("Results different from expected baseline");
-    }
-  }
+    private static void compareResults(String results, String baseline) {
+        if (!results.equals(baseline)) {
+            throw new RuntimeException("Results different from expected baseline");
+        }
+    }
 
-  public static void main(final String[] args)
-    throws ClassNotFoundException, IOException, InterruptedException
-  {
-    Properties props = StringUtils.argsToProperties(args);
-    runThreadedTest(props);
-  }
+    public static void main(final String[] args)
+            throws ClassNotFoundException, IOException, InterruptedException {
+        Properties props = StringUtils.argsToProperties(args);
+        runThreadedTest(props);
+    }
 
-  public static void runThreadedTest(Properties props)
-    throws ClassNotFoundException, IOException, InterruptedException
-  {
-    ArrayList<Properties> configs = new ArrayList<>();
-    ArrayList<MaxentTagger> taggers = new ArrayList<>();
-    int numThreads = DEFAULT_NUM_THREADS;
+    public static void runThreadedTest(Properties props)
+            throws ClassNotFoundException, IOException, InterruptedException {
+        ArrayList<Properties> configs = new ArrayList<>();
+        ArrayList<MaxentTagger> taggers = new ArrayList<>();
+        int numThreads = DEFAULT_NUM_THREADS;
 
-    // let the user specify how many threads to run in the first test case
-    if (props.getProperty(THREAD_FLAG) != null) {
-      numThreads = Integer.valueOf(props.getProperty(THREAD_FLAG));
-    }
+        // let the user specify how many threads to run in the first test case
+        if (props.getProperty(THREAD_FLAG) != null) {
+            numThreads = Integer.valueOf(props.getProperty(THREAD_FLAG));
+        }
 
-    // read in each of the taggers specified on the command line
-    System.out.println();
-    System.out.println("Loading taggers...");
-    System.out.println();
+        // read in each of the taggers specified on the command line
+        System.out.println();
+        System.out.println("Loading taggers...");
+        System.out.println();
 
-    if (props.getProperty("model") != null) {
-      configs.add(props);
-      taggers.add(new MaxentTagger(configs.get(0).getProperty("model"), configs.get(0)));
-    } else {
-      int taggerNum = 1;
-      String taggerName = "model" + taggerNum;
-      while (props.getProperty(taggerName) != null) {
-        Properties newProps = new Properties();
-        newProps.putAll(props);
-        newProps.setProperty("model", props.getProperty(taggerName));
-        configs.add(newProps);
-        taggers.add(new MaxentTagger(configs.get(taggerNum - 1).getProperty("model"),
-                                     configs.get(taggerNum - 1)));
+        if (props.getProperty("model") != null) {
+            configs.add(props);
+            taggers.add(new MaxentTagger(configs.get(0).getProperty("model"), configs.get(0)));
+        } else {
+            int taggerNum = 1;
+            String taggerName = "model" + taggerNum;
+            while (props.getProperty(taggerName) != null) {
+                Properties newProps = new Properties();
+                newProps.putAll(props);
+                newProps.setProperty("model", props.getProperty(taggerName));
+                configs.add(newProps);
+                taggers.add(new MaxentTagger(configs.get(taggerNum - 1).getProperty("model"),
+                        configs.get(taggerNum - 1)));
 
-        ++taggerNum;
-        taggerName = "model" + taggerNum;
-      }
-    }
+                ++taggerNum;
+                taggerName = "model" + taggerNum;
+            }
+        }
 
-    // no models at all => bad
-    if (taggers.isEmpty()) {
-      throw new IllegalArgumentException("Please specify at least one of " +
-                                         "-model or -model1");
-    }
+        // no models at all => bad
+        if (taggers.isEmpty()) {
+            throw new IllegalArgumentException("Please specify at least one of " +
+                    "-model or -model1");
+        }
 
-    System.out.println();
-    System.out.println("Running the baseline results for tagger 1");
-    System.out.println();
+        System.out.println();
+        System.out.println("Running the baseline results for tagger 1");
+        System.out.println();
 
-    // run baseline results for the first tagger model
-    TaggerThread baselineThread =
-      new TaggerThread(taggers.get(0), "BaseResults-1");
-    baselineThread.start();
-    baselineThread.join();
+        // run baseline results for the first tagger model
+        TaggerThread baselineThread =
+                new TaggerThread(taggers.get(0), "BaseResults-1");
+        baselineThread.start();
+        baselineThread.join();
 
-    ArrayList<String> baselineResults = new ArrayList<>();
-    baselineResults.add(baselineThread.getResultsString());
+        ArrayList<String> baselineResults = new ArrayList<>();
+        baselineResults.add(baselineThread.getResultsString());
 
-    System.out.println();
-    System.out.println("Running " + numThreads + " threads of tagger 1");
-    System.out.println();
+        System.out.println();
+        System.out.println("Running " + numThreads + " threads of tagger 1");
+        System.out.println();
 
-    // run the first tagger in X separate threads at the same time
-    // at the end of this test, those X threads should produce the same results
-    ArrayList<TaggerThread> threads = new ArrayList<>();
-    for (int i = 0; i < numThreads; ++i) {
-      threads.add(new TaggerThread(taggers.get(0),
-                                   "Simultaneous-" + (i + 1)));
-    }
-    for (TaggerThread thread : threads) {
-      thread.start();
-    }
-    for (TaggerThread thread : threads) {
-      thread.join();
-      compareResults(thread.getResultsString(),
-                     baselineResults.get(0));
-    }
+        // run the first tagger in X separate threads at the same time
+        // at the end of this test, those X threads should produce the same results
+        ArrayList<TaggerThread> threads = new ArrayList<>();
+        for (int i = 0; i < numThreads; ++i) {
+            threads.add(new TaggerThread(taggers.get(0),
+                    "Simultaneous-" + (i + 1)));
+        }
+        for (TaggerThread thread : threads) {
+            thread.start();
+        }
+        for (TaggerThread thread : threads) {
+            thread.join();
+            compareResults(thread.getResultsString(),
+                    baselineResults.get(0));
+        }
 
-    // if we have more than one model...
-    if (taggers.size() > 1) {
-      // first, produce baseline results for the other models
-      // do this one thread at a time so we know there are no
-      // thread-related screwups
-      // TODO: would iterables be cleaner?
-      for (int i = 1; i < taggers.size(); ++i) {
-        System.out.println();
-        System.out.println("Running the baseline results for tagger " + (i + 1));
-        System.out.println();
+        // if we have more than one model...
+        if (taggers.size() > 1) {
+            // first, produce baseline results for the other models
+            // do this one thread at a time so we know there are no
+            // thread-related screwups
+            // TODO: would iterables be cleaner?
+            for (int i = 1; i < taggers.size(); ++i) {
+                System.out.println();
+                System.out.println("Running the baseline results for tagger " + (i + 1));
+                System.out.println();
 
-        baselineThread = new TaggerThread(taggers.get(i),
-                                          "BaseResults-" + (i + 1));
-        baselineThread.start();
-        baselineThread.join();
-        baselineResults.add(baselineThread.getResultsString());
-      }
+                baselineThread = new TaggerThread(taggers.get(i),
+                        "BaseResults-" + (i + 1));
+                baselineThread.start();
+                baselineThread.join();
+                baselineResults.add(baselineThread.getResultsString());
+            }
 
-      System.out.println();
-      System.out.println("Running " + taggers.size() +
-                         " threads of different taggers");
-      System.out.println();
+            System.out.println();
+            System.out.println("Running " + taggers.size() +
+                    " threads of different taggers");
+            System.out.println();
 
-      // now, run the X models at the same time.  there used to be a
-      // whole bunch of static state in the tagger, which used to mean
-      // such a thing was not be possible to do.  now that should not
-      // be a problem any more
-      threads.clear();
-      for (int i = 0; i < taggers.size(); ++i) {
-        threads.add(new TaggerThread(taggers.get(i),
-                                     "DifferentTaggers-" + (i + 1)));
-      }
-      for (TaggerThread thread : threads) {
-        thread.start();
-      }
-      for (int i = 0; i < taggers.size(); ++i) {
-        TaggerThread thread = threads.get(i);
-        thread.join();
-        compareResults(thread.getResultsString(),
-                       baselineResults.get(i));
-      }
-    }
+            // now, run the X models at the same time.  there used to be a
+            // whole bunch of static state in the tagger, which used to mean
+            // such a thing was not be possible to do.  now that should not
+            // be a problem any more
+            threads.clear();
+            for (int i = 0; i < taggers.size(); ++i) {
+                threads.add(new TaggerThread(taggers.get(i),
+                        "DifferentTaggers-" + (i + 1)));
+            }
+            for (TaggerThread thread : threads) {
+                thread.start();
+            }
+            for (int i = 0; i < taggers.size(); ++i) {
+                TaggerThread thread = threads.get(i);
+                thread.join();
+                compareResults(thread.getResultsString(),
+                        baselineResults.get(i));
+            }
+        }
 
-    System.out.println("Done!");
-  }
+        System.out.println("Done!");
+    }
 }
Index: src/edu/stanford/nlp/tagger/maxent/Defaults.kt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/Defaults.kt	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
+++ src/edu/stanford/nlp/tagger/maxent/Defaults.kt	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
@@ -0,0 +1,7 @@
+package edu.stanford.nlp.tagger.maxent
+
+class Defaults {
+    companion object {
+        const val  naTag = "EMPTY";
+    }
+}
\ No newline at end of file
Index: src/edu/stanford/nlp/tagger/maxent/Extractor.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/Extractor.java	(revision a6ee6aca4abdad34d6e5fae24410c36c3486f33b)
+++ src/edu/stanford/nlp/tagger/maxent/Extractor.java	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
@@ -4,7 +4,7 @@
  * Copyright:    Copyright (c) Stanford University<p>
  */
 
-package edu.stanford.nlp.tagger.maxent; 
+package edu.stanford.nlp.tagger.maxent;
 
 import java.io.Serializable;
 
@@ -49,241 +49,204 @@
  * @author Kristina Toutanova
  * @version 1.0
  */
-public class Extractor implements Serializable  {
+public class Extractor implements Serializable {
 
 
-  // /** A logger for this class */
-  // private static final Redwood.RedwoodChannels log = Redwood.channels(Extractor.class);
+    // /** A logger for this class */
+    // private static final Redwood.RedwoodChannels log = Redwood.channels(Extractor.class);
 
-  private static final long serialVersionUID = -4694133872973560083L;
+    private static final long serialVersionUID = -4694133872973560083L;
 
-  static final String zeroSt = "0";
+    static final String zeroSt = "0";
 
-  final int position;
-  private final boolean isTag;
+    final int position;
+    private final boolean isTag;
 
-  public Extractor() {
-    this(Integer.MAX_VALUE, false);
-  }
+    public Extractor() {
+        this(Integer.MAX_VALUE, false);
+    }
 
-  public static final Extractor[] EMPTY_EXTRACTOR_ARRAY = new Extractor[0];
+    public static final Extractor[] EMPTY_EXTRACTOR_ARRAY = new Extractor[0];
 
-  /**
-   * This constructor creates an extractor which extracts either the tag or
-   * the word from position position in the history.
-   *
-   * @param position The position of the thing to be extracted. This is
-   *                 relative to the current word. For example, position 0
-   *                 will be the current word, -1 will be
-   *                 the word before +1 will be the word after, etc.
-   * @param isTag    If true this means that the POS tag is extracted from
-   *                 position, otherwise the word is extracted.
-   */
-  protected Extractor(int position, boolean isTag) {
-    this.position = position;
-    this.isTag = isTag;
-  }
+    /**
+     * This constructor creates an extractor which extracts either the tag or
+     * the word from position position in the history.
+     *
+     * @param position The position of the thing to be extracted. This is
+     *                 relative to the current word. For example, position 0
+     *                 will be the current word, -1 will be
+     *                 the word before +1 will be the word after, etc.
+     * @param isTag    If true this means that the POS tag is extracted from
+     *                 position, otherwise the word is extracted.
+     */
+    protected Extractor(int position, boolean isTag) {
+        this.position = position;
+        this.isTag = isTag;
+    }
 
-  /**
-   * Subclasses should override this method and keep only the data
-   * they want about the tagger.  Note that such data should also be
-   * declared "transient" if it is already available in the tagger.
-   * This is because, when we save the tagger to disk, we do so by
-   * writing out objects, and there is no need to write the same
-   * object more than once.  setGlobalHolder will be called both after
-   * construction when building a new tag and when loading existing
-   * taggers from disk, so the same data will available then as well.
-   */
-  protected void setGlobalHolder(MaxentTagger tagger) {}
+    /**
+     * Subclasses should override this method and keep only the data
+     * they want about the tagger.  Note that such data should also be
+     * declared "transient" if it is already available in the tagger.
+     * This is because, when we save the tagger to disk, we do so by
+     * writing out objects, and there is no need to write the same
+     * object more than once.  setGlobalHolder will be called both after
+     * construction when building a new tag and when loading existing
+     * taggers from disk, so the same data will available then as well.
+     */
+    protected void setGlobalHolder(MaxentTagger tagger) {
+    }
 
 
-  /** This evaluates any precondition for a feature being applicable based
-   *  on a certain tag. It returns true if the feature is applicable.
-   *  By default an Extractor is applicable everywhere, but some
-   *  subclasses limit application.
-   *
-   *  @param tag The possible tag that the feature will be generated for
-   *  @return Whether the feature extractor is applicable (true) or not (false)
-   */
-  @SuppressWarnings({"UnusedDeclaration"})
-  public boolean precondition(String tag) {
-    return true;
-  }
+    /**
+     * This evaluates any precondition for a feature being applicable based
+     * on a certain tag. It returns true if the feature is applicable.
+     * By default an Extractor is applicable everywhere, but some
+     * subclasses limit application.
+     *
+     * @param tag The possible tag that the feature will be generated for
+     * @return Whether the feature extractor is applicable (true) or not (false)
+     */
+    @SuppressWarnings({"UnusedDeclaration"})
+    public boolean precondition(String tag) {
+        return true;
+    }
 
 
-  /**
-   * @return the number of positions to the left the extractor looks at (only tags, because words are fixed.)
-   */
-  public int leftContext() {
-    if (isTag) {
-      if (position < 0) {
-        return -position;
-      }
-    }
+    /**
+     * @return the number of positions to the left the extractor looks at (only tags, because words are fixed.)
+     */
+    public int leftContext() {
+        if (isTag) {
+            if (position < 0) {
+                return -position;
+            }
+        }
 
-    return 0;
-  }
+        return 0;
+    }
 
 
-  /**
-   * @return the number of positions to the right the extractor looks at (only tags, because words are fixed.)
-   */
-  public int rightContext() {
-    if (isTag) {
-      if (position > 0) {
-        return position;
-      }
-    }
+    /**
+     * @return the number of positions to the right the extractor looks at (only tags, because words are fixed.)
+     */
+    public int rightContext() {
+        if (isTag) {
+            if (position > 0) {
+                return position;
+            }
+        }
 
-    return 0;
-  }
+        return 0;
+    }
 
-  // CDM May 2007: This feature is currently never used. Maybe we should
-  // change things so it is, and each feature template has a threshold, but
-  // need to then work out what a TaggerFeature is and whether we should still
-  // be using one of those to index with.
-  // At present real threshold check happens in TaggerExperiments with
-  // the populated(int, int) method.
-  //  public boolean isPopulated(TaggerFeature f) {
-  //    return (f.indexedValues.length > GlobalHolder.minFeatureThresh);
-  //  }
+    // CDM May 2007: This feature is currently never used. Maybe we should
+    // change things so it is, and each feature template has a threshold, but
+    // need to then work out what a TaggerFeature is and whether we should still
+    // be using one of those to index with.
+    // At present real threshold check happens in TaggerExperiments with
+    // the populated(int, int) method.
+    //  public boolean isPopulated(TaggerFeature f) {
+    //    return (f.indexedValues.length > GlobalHolder.minFeatureThresh);
+    //  }
 
-  /** Subclasses should only override the two argument version
-   *  of this method.
-   *
-   *  @param h The history to extract from
-   *  @return The feature value
-   */
-  final String extract(History h) {
-    return extract(h, h.pairs);
-  }
+    /**
+     * Subclasses should only override the two argument version
+     * of this method.
+     *
+     * @param h The history to extract from
+     * @return The feature value
+     */
+    final String extract(History h) {
+        return extract(h, h.pairs);
+    }
 
-  /**
-   * @return Returns true if extractor is a function of POS tags; if it returns false,
-   * features are pre-computed.
-   */
-  public boolean isDynamic() {
-    return isTag;
-  }
+    /**
+     * @return Returns true if extractor is a function of POS tags; if it returns false,
+     * features are pre-computed.
+     */
+    public boolean isDynamic() {
+        return isTag;
+    }
 
-  /**
-   * @return Returns true if extractor is not a function of POS tags, and only
-   * depends on current word.
-   */
-  public boolean isLocal() {
-    return !isTag && position == 0;
-  }
+    /**
+     * @return Returns true if extractor is not a function of POS tags, and only
+     * depends on current word.
+     */
+    public boolean isLocal() {
+        return !isTag && position == 0;
+    }
 
-  String extract(History h, PairsHolder pH) {
-    return isTag ? pH.getTag(h, position) : pH.getWord(h, position);
-  }
+    String extract(History h, PairsHolder pH) {
+        return isTag ? pH.getTag(h, position) : pH.getWord(h, position);
+    }
 
-  @SuppressWarnings({"MethodMayBeStatic"})
-  String extractLV(History h, PairsHolder pH) {
-    // should extract last verbal word and also the current word
-    int start = h.start;
-    String lastverb = "NA";
-    int current = h.current;
-    int index = current - 1;
-    while (index >= start) {
-      String tag = pH.getTag(index);
-      if (tag.startsWith("VB")) {
-        lastverb = pH.getWord(index);
-        break;
-      }
-      if (tag.startsWith(",")) {
-        break;
-      }
-      index--;
-    }
-    return lastverb;
-  }
-
-  String extractLV(History h, PairsHolder pH, int bound) {
-    // should extract last verbal word and also the current word
-    int start = h.start;
-    String lastverb = "NA";
-    int current = h.current;
-    int index = current - 1;
-    while ((index >= start) && (index >= current - bound)) {
-      String tag = pH.getTag(index);
-      if (tag.startsWith("VB")) {
-        lastverb = pH.getWord(index);
-        break;
-      }
-      if (tag.startsWith(",")) {
-        break;
-      }
-      index--;
-    }
-    return lastverb;
-  }
-
-
-  // By default the bound is ignored, but a few subclasses make use of it.
-  @SuppressWarnings({"UnusedDeclaration"})
-  String extract(History h, PairsHolder pH, int bound) {
-    return extract(h, pH);
-  }
+    // By default the bound is ignored, but a few subclasses make use of it.
+    @SuppressWarnings({"UnusedDeclaration"})
+    String extract(History h, PairsHolder pH, int bound) {
+        return extract(h, pH);
+    }
 
 
-  @Override
-  public String toString() {
-    String cl = getClass().getName();
-    int ind = cl.lastIndexOf('.');
-    // MAX_VALUE is the default value and means we aren't using these two arguments
-    String args = (position == Integer.MAX_VALUE) ? "": (position + "," + (isTag ? "tag" : "word"));
-    return cl.substring(ind + 1) + '(' + args + ')';
-  }
+    @Override
+    public String toString() {
+        String cl = getClass().getName();
+        int ind = cl.lastIndexOf('.');
+        // MAX_VALUE is the default value and means we aren't using these two arguments
+        String args = (position == Integer.MAX_VALUE) ? "" : (position + "," + (isTag ? "tag" : "word"));
+        return cl.substring(ind + 1) + '(' + args + ')';
+    }
 
 
-  /** This is used for argument parsing in arch variable.
-   *  It can extract from a comma separated values argument list.
-   *  Values can be quoted with double quotes (with a second double quote as double quote escape char)
-   *  like in a regular CSV file. It assumes the input format is "name(arg,arg,arg)".
-   *
-   *  @param str arch variable component input
-   *  @param num Number of argument. Numbers are 1-indexed (i.e., start from 1 not 0)
-   *  @return The parenthesized String, or null if none.
-   */
-  static String getParenthesizedArg(String str, int num) {
-    int left = str.indexOf('(');
-    int right = str.lastIndexOf(')');
-    if (left < 0 || right <= left) {
-      throw new IllegalArgumentException("getParenthesizedArg: Bad format String: " + str);
-    }
-    String argStr = str.substring(left + 1, right);
-    String[] args = StringUtils.splitOnCharWithQuoting(argStr, ',', '"', '"');
-    // log.info("getParenthesizedArg split " + str + " into " + args.length + " pieces; returning number " + num);
-    // for (int i = 0; i < args.length; i++) {
-    //   log.info("  " + args[i]);
-    // }
-    num--;
-    if (args.length <= num || num < 0) {
-      return null;
-    }
-    return args[num];
-  }
+    /**
+     * This is used for argument parsing in arch variable.
+     * It can extract from a comma separated values argument list.
+     * Values can be quoted with double quotes (with a second double quote as double quote escape char)
+     * like in a regular CSV file. It assumes the input format is "name(arg,arg,arg)".
+     *
+     * @param str arch variable component input
+     * @param num Number of argument. Numbers are 1-indexed (i.e., start from 1 not 0)
+     * @return The parenthesized String, or null if none.
+     */
+    static String getParenthesizedArg(String str, int num) {
+        int left = str.indexOf('(');
+        int right = str.lastIndexOf(')');
+        if (left < 0 || right <= left) {
+            throw new IllegalArgumentException("getParenthesizedArg: Bad format String: " + str);
+        }
+        String argStr = str.substring(left + 1, right);
+        String[] args = StringUtils.splitOnCharWithQuoting(argStr, ',', '"', '"');
+        // log.info("getParenthesizedArg split " + str + " into " + args.length + " pieces; returning number " + num);
+        // for (int i = 0; i < args.length; i++) {
+        //   log.info("  " + args[i]);
+        // }
+        num--;
+        if (args.length <= num || num < 0) {
+            return null;
+        }
+        return args[num];
+    }
 
-  /** This is used for argument parsing in arch variable.
-   *  It can extract a comma separated argument.
-   *  Assumes the input format is "name(arg,arg,arg)", with possible
-   *  spaces around the parentheses and comma(s).
-   *
-   *  @param str arch variable component input
-   *  @param num Number of argument
-   *  @return The int value of the arg or 0 if missing or empty
-   */
-  @SuppressWarnings("ConstantConditions")
-  static int getParenthesizedNum(String str, int num) {
-    String arg = getParenthesizedArg(str, num);
-    int ans = 0;
-    try {
-      ans = Integer.parseInt(arg);
-    } catch (NumberFormatException | ArrayIndexOutOfBoundsException nfe) {
-      // just leave ans as 0
-    }
-    return ans;
-  }
+    /**
+     * This is used for argument parsing in arch variable.
+     * It can extract a comma separated argument.
+     * Assumes the input format is "name(arg,arg,arg)", with possible
+     * spaces around the parentheses and comma(s).
+     *
+     * @param str arch variable component input
+     * @param num Number of argument
+     * @return The int value of the arg or 0 if missing or empty
+     */
+    @SuppressWarnings("ConstantConditions")
+    static int getParenthesizedNum(String str, int num) {
+        String arg = getParenthesizedArg(str, num);
+        int ans = 0;
+        try {
+            ans = Integer.parseInt(arg);
+        } catch (NumberFormatException | ArrayIndexOutOfBoundsException nfe) {
+            // just leave ans as 0
+        }
+        return ans;
+    }
 
 }
Index: src/edu/stanford/nlp/tagger/maxent/ExtractorVerbalVBNZero.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/ExtractorVerbalVBNZero.java	(revision a6ee6aca4abdad34d6e5fae24410c36c3486f33b)
+++ src/edu/stanford/nlp/tagger/maxent/ExtractorVerbalVBNZero.java	(revision 2d068872774b100b619a1aaec1c6a25c701d53ce)
@@ -23,7 +23,7 @@
   private static final String edSuff = "ed";
   private static final String enSuff = "en";
   private static final String oneSt = "1";
-  private static final String naWord = "NA";
+  private static final String naWord = Defaults.naTag;
 
   private final int bound;
   private static final Pattern stopper = Pattern.compile("(?i:and|or|but|,|;|-|--)");
@@ -62,7 +62,7 @@
 
     for (int index = -1; index >= -bound; index--) {
       String word2 = pH.getWord(h, index);
-      if ("NA".equals(word2)) {
+      if (Defaults.naTag.equals(word2)) {
         break;
       }
       if (stopper.matcher(word2).matches()) {
Index: src/edu/stanford/nlp/tagger/maxent/TTags.java
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/TTags.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/TTags.kt	(revision b2d1a9a42e572c987c6031e6ec99113741579220)
@@ -1,500 +1,577 @@
-package edu.stanford.nlp.tagger.maxent;
+package edu.stanford.nlp.tagger.maxent
 
-import edu.stanford.nlp.io.IOUtils;
-import edu.stanford.nlp.io.RuntimeIOException;
-import edu.stanford.nlp.tagger.common.Tagger;
-import edu.stanford.nlp.util.Generics;
-import edu.stanford.nlp.util.HashIndex;
-import edu.stanford.nlp.util.Index;
+import edu.stanford.nlp.io.IOUtils
+import edu.stanford.nlp.io.RuntimeIOException
+import edu.stanford.nlp.tagger.common.Tagger
+import edu.stanford.nlp.util.Generics
+import edu.stanford.nlp.util.HashIndex
+import edu.stanford.nlp.util.Index
+import java.io.*
 
-import java.io.IOException;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.util.*;
+import java.util.*
+import kotlin.streams.toList
 
+class TTags// conjunctions
 /**
+// conjunctions
  * This class holds the POS tags, assigns them unique ids, and knows which tags
  * are open versus closed class.
- * <p>
- * Title:        StanfordMaxEnt<p>
- * Description:  A Maximum Entropy Toolkit<p>
- * Company:      Stanford University<p>
+ *
+ *
+ * Title:        StanfordMaxEnt
+ *
+ *
+ * Description:  A Maximum Entropy Toolkit
+ *
+ *
+ * Company:      Stanford University
+ *
+ *
  *
  * @author Kristina Toutanova
  * @version 1.0
  */
-public class TTags {
-
-  private Index<String> index = new HashIndex<>();
-  private final Set<String> closed = Generics.newHashSet();
-  private Set<String> openTags; // = null; /* cache */
-  private final boolean isEnglish; // for speed
-  private static final boolean doDeterministicTagExpansion = true;
-
-
-  /** If true, then the open tags are fixed and we set closed tags based on
-   *  index-openTags; otherwise, we set open tags based on index-closedTags.
-   */
-  private boolean openFixed = false;
-
-  /** When making a decision based on the training data as to whether a
-   *  tag is closed, this is the threshold for how many tokens can be in
-   *  a closed class - purposely conservative.
-   * TODO: make this an option you can set; need to pass in TaggerConfig object and then can say = config.getClosedTagThreshold());
-   */
-  private final int closedTagThreshold = Integer.parseInt(TaggerConfig.CLOSED_CLASS_THRESHOLD);
-
-  /** If true, when a model is trained, all tags that had fewer tokens than
-   *  closedTagThreshold will be considered closed.
-   */
-  private boolean learnClosedTags = false;
-
-
-  public TTags() {
-    isEnglish = false;
-  }
 
-  /*
-  public TTags(TaggerConfig config) {
-    String[] closedArray = config.getClosedClassTags();
-    String[] openArray = config.getOpenClassTags();
-    if(closedArray.length > 0) {
-      closed = Generics.newHashSet(Arrays.asList(closedArray));
-    } else if(openArray.length > 0) {
-      openTags = Generics.newHashSet(Arrays.asList(openArray));
-    } else {
-      learnClosedTags = config.getLearnClosedClassTags();
-      closedTagThreshold = config.getClosedTagThreshold();
-    }
-  }
-  */
-
-  TTags(String language) {
-    if (language.equalsIgnoreCase("english")) {
-      closed.add(".");
-      closed.add(",");
-      closed.add("``");
-      closed.add("''");
-      closed.add(":");
-      closed.add("$");
-      closed.add("EX");
-      closed.add("(");
-      closed.add(")");
-      closed.add("#");
-      closed.add("MD");
-      closed.add("CC");
-      closed.add("DT");
-      closed.add("LS");
-      closed.add("PDT");
-      closed.add("POS");
-      closed.add("PRP");
-      closed.add("PRP$");
-      closed.add("RP");
-      closed.add("TO");
-      closed.add(Tagger.EOS_TAG);
-      closed.add("UH");
-      closed.add("WDT");
-      closed.add("WP");
-      closed.add("WP$");
-      closed.add("WRB");
-      closed.add("-LRB-");
-      closed.add("-RRB-");
-      //  closed.add("IN");
-      isEnglish = true;
-    } else if(language.equalsIgnoreCase("polish")) {
-      closed.add(".");
-      closed.add(",");
-      closed.add("``");
-      closed.add("''");
-      closed.add(":");
-      closed.add("$");
-      closed.add("(");
-      closed.add(")");
-      closed.add("#");
-      closed.add("POS");
-      closed.add(Tagger.EOS_TAG);
-      closed.add("ppron12");
-      closed.add("ppron3");
-      closed.add("siebie");
-      closed.add("qub");
-      closed.add("conj");
-      isEnglish = false;
-    } else if(language.equalsIgnoreCase("chinese")) {
-      /* chinese treebank 5 tags */
-      closed.add("AS");
-      closed.add("BA");
-      closed.add("CC");
-      closed.add("CS");
-      closed.add("DEC");
-      closed.add("DEG");
-      closed.add("DER");
-      closed.add("DEV");
-      closed.add("DT");
-      closed.add("ETC");
-      closed.add("IJ");
-      closed.add("LB");
-      closed.add("LC");
-      closed.add("P");
-      closed.add("PN");
-      closed.add("PU");
-      closed.add("SB");
-      closed.add("SP");
-      closed.add("VC");
-      closed.add("VE");
-      isEnglish = false;
-    } else if (language.equalsIgnoreCase("arabic")) {
-      // kulick tag set
-      // the following tags seem to be complete sets in the training
-      // data (see the comments for "german" for more info)
-      closed.add("PUNC");
-      closed.add("CC");
-      closed.add("CPRP$");
-      closed.add(Tagger.EOS_TAG);
-      // maybe more should still be added ... cdm jun 2006
-      isEnglish = false;
-    } else if(language.equalsIgnoreCase("german")) {
-      // The current version of the German tagger is built with the
-      // negra-tiger data set.  We use the STTS tag set.  In
-      // particular, we use the version with the changes described in
-      // appendix A-2 of
-      // http://www.uni-potsdam.de/u/germanistik/ls_dgs/tiger1-intro.pdf
-      // eg the STTS tag set with PROAV instead of PAV
-      // To find the closed tags, we use lists of standard closed German
-      // tags, eg
-      // http://www.sfs.uni-tuebingen.de/Elwis/stts/Wortlisten/WortFormen.html
-      // In other words:
-      //
-      // APPO APPR APPRART APZR ART KOKOM KON KOUI KOUS PDAT PDS PIAT
-      // PIDAT PIS PPER PPOSAT PPOSS PRELAT PRELS PRF PROAV PTKA
-      // PTKANT PTKNEG PTKVZ PTKZU PWAT PWAV PWS VAFIN VAIMP VAINF
-      // VAPP VMFIN VMINF VMPP
-      //
-      // One issue with this is that our training data does not have
-      // the complete collection of many of these closed tags.  For
-      // example, words with the tag APPR show up in the test or dev
-      // sets without ever showing up in the training.  Tags that
-      // don't have this property:
-      //
-      // KOKOM PPOSS PTKA PTKNEG PWAT VAINF VAPP VMINF VMPP
-      closed.add("$,");
-      closed.add("$.");
-      closed.add("$(");
-      closed.add("--"); // this shouldn't be a tag of the dataset, but was a conversion bug!
-      closed.add(Tagger.EOS_TAG);
-      closed.add("KOKOM");
-      closed.add("PPOSS");
-      closed.add("PTKA");
-      closed.add("PTKNEG");
-      closed.add("PWAT");
-      closed.add("VAINF");
-      closed.add("VAPP");
-      closed.add("VMINF");
-      closed.add("VMPP");
-      isEnglish = false;
-    } else if (language.equalsIgnoreCase("french")) {
-      // Using the french treebank, with Spence's adaptations of
-      // Candito's treebank modifications, we get that only the
-      // punctuation tags are reliably closed:
-      // !, ", *, ,, -, -LRB-, -RRB-, ., ..., /, :, ;, =, ?, [, ]
-      closed.add("!");
-      closed.add("\"");
-      closed.add("*");
-      closed.add(",");
-      closed.add("-");
-      closed.add("-LRB-");
-      closed.add("-RRB-");
-      closed.add(".");
-      closed.add("...");
-      closed.add("/");
-      closed.add(":");
-      closed.add(";");
-      closed.add("=");
-      closed.add("?");
-      closed.add("[");
-      closed.add("]");
-      isEnglish = false;
-    } else if (language.equalsIgnoreCase("spanish")) {
-      closed.add(Tagger.EOS_TAG);
+// punctuation
+// Using the french treebank, with Spence's adaptations of
+// Candito's treebank modifications, we get that only the
+// punctuation tags are reliably closed:
+// !, ", *, ,, -, -LRB-, -RRB-, ., ..., /, :, ;, =, ?, [, ]
+// The current version of the German tagger is built with the
+// negra-tiger data set.  We use the STTS tag set.  In
+// particular, we use the version with the changes described in
+// appendix A-2 of
+// http://www.uni-potsdam.de/u/germanistik/ls_dgs/tiger1-intro.pdf
+// eg the STTS tag set with PROAV instead of PAV
+// To find the closed tags, we use lists of standard closed German
+// tags, eg
+// http://www.sfs.uni-tuebingen.de/Elwis/stts/Wortlisten/WortFormen.html
+// In other words:
+//
+// APPO APPR APPRART APZR ART KOKOM KON KOUI KOUS PDAT PDS PIAT
+// PIDAT PIS PPER PPOSAT PPOSS PRELAT PRELS PRF PROAV PTKA
+// PTKANT PTKNEG PTKVZ PTKZU PWAT PWAV PWS VAFIN VAIMP VAINF
+// VAPP VMFIN VMINF VMPP
+//
+// One issue with this is that our training data does not have
+// the complete collection of many of these closed tags.  For
+// example, words with the tag APPR show up in the test or dev
+// sets without ever showing up in the training.  Tags that
+// don't have this property:
+//
+// KOKOM PPOSS PTKA PTKNEG PWAT VAINF VAPP VMINF VMPP
+// this shouldn't be a tag of the dataset, but was a conversion bug!
+// kulick tag set
+// the following tags seem to be complete sets in the training
+// data (see the comments for "german" for more info)
+// maybe more should still be added ... cdm jun 2006
+/* chinese treebank 5 tags *///  closed.add("IN");
+
+// punctuation
+// Using the french treebank, with Spence's adaptations of
+// Candito's treebank modifications, we get that only the
+// punctuation tags are reliably closed:
+// !, ", *, ,, -, -LRB-, -RRB-, ., ..., /, :, ;, =, ?, [, ]
+// The current version of the German tagger is built with the
+// negra-tiger data set.  We use the STTS tag set.  In
+// particular, we use the version with the changes described in
+// appendix A-2 of
+// http://www.uni-potsdam.de/u/germanistik/ls_dgs/tiger1-intro.pdf
+// eg the STTS tag set with PROAV instead of PAV
+// To find the closed tags, we use lists of standard closed German
+// tags, eg
+// http://www.sfs.uni-tuebingen.de/Elwis/stts/Wortlisten/WortFormen.html
+// In other words:
+//
+// APPO APPR APPRART APZR ART KOKOM KON KOUI KOUS PDAT PDS PIAT
+// PIDAT PIS PPER PPOSAT PPOSS PRELAT PRELS PRF PROAV PTKA
+// PTKANT PTKNEG PTKVZ PTKZU PWAT PWAV PWS VAFIN VAIMP VAINF
+// VAPP VMFIN VMINF VMPP
+//
+// One issue with this is that our training data does not have
+// the complete collection of many of these closed tags.  For
+// example, words with the tag APPR show up in the test or dev
+// sets without ever showing up in the training.  Tags that
+// don't have this property:
+//
+// KOKOM PPOSS PTKA PTKNEG PWAT VAINF VAPP VMINF VMPP
+// this shouldn't be a tag of the dataset, but was a conversion bug!
+// kulick tag set
+// the following tags seem to be complete sets in the training
+// data (see the comments for "german" for more info)
+// maybe more should still be added ... cdm jun 2006
+/* chinese treebank 5 tags *///  closed.add("IN");// conjunctions
+
+// punctuation/* add closed-class lists for other languages here */
+// Using the french treebank, with Spence's adaptations of
+// Candito's treebank modifications, we get that only the
+// punctuation tags are reliably closed:
+// !, ", *, ,, -, -LRB-, -RRB-, ., ..., /, :, ;, =, ?, [, ]
+// The current version of the German tagger is built with the
+// negra-tiger data set.  We use the STTS tag set.  In
+// particular, we use the version with the changes described in
+// appendix A-2 of
+// http://www.uni-potsdam.de/u/germanistik/ls_dgs/tiger1-intro.pdf
+// eg the STTS tag set with PROAV instead of PAV
+// To find the closed tags, we use lists of standard closed German
+// tags, eg
+// http://www.sfs.uni-tuebingen.de/Elwis/stts/Wortlisten/WortFormen.html
+// In other words:
+//
+// APPO APPR APPRART APZR ART KOKOM KON KOUI KOUS PDAT PDS PIAT
+// PIDAT PIS PPER PPOSAT PPOSS PRELAT PRELS PRF PROAV PTKA
+// PTKANT PTKNEG PTKVZ PTKZU PWAT PWAV PWS VAFIN VAIMP VAINF
+// VAPP VMFIN VMINF VMPP
+//
+// One issue with this is that our training data does not have
+// the complete collection of many of these closed tags.  For
+// example, words with the tag APPR show up in the test or dev
+// sets without ever showing up in the training.  Tags that
+// don't have this property:
+//
+// KOKOM PPOSS PTKA PTKNEG PWAT VAINF VAPP VMINF VMPP
+// this shouldn't be a tag of the dataset, but was a conversion bug!
+// kulick tag set
+// the following tags seem to be complete sets in the training
+// data (see the comments for "german" for more info)
+// maybe more should still be added ... cdm jun 2006
+/* chinese treebank 5 tags *///  closed.add("IN");
+@JvmOverloads constructor(config: TaggerConfig, language: String = "") {
 
-      // conjunctions
-      closed.add("cc");
-      closed.add("cs");
+    private var index: Index<String>
+    private val closed: MutableSet<String>
 
-      // punctuation
-      closed.add("faa");
-      closed.add("fat");
-      closed.add("fc");
-      closed.add("fca");
-      closed.add("fct");
-      closed.add("fd");
-      closed.add("fe");
-      closed.add("fg");
-      closed.add("fh");
-      closed.add("fia");
-      closed.add("fit");
-      closed.add("fla");
-      closed.add("flt");
-      closed.add("fp");
-      closed.add("fpa");
-      closed.add("fpt");
-      closed.add("fra");
-      closed.add("frc");
-      closed.add("fs");
-      closed.add("ft");
-      closed.add("fx");
-      closed.add("fz");
+    private var _openTags: MutableSet<String>? = null
 
-      isEnglish = false;
-    } else if (language.equalsIgnoreCase("medpost")) {
-      closed.add(".");
-      closed.add(",");
-      closed.add("``");
-      closed.add("''");
-      closed.add(":");
-      closed.add("$");
-      closed.add("EX");
-      closed.add("(");
-      closed.add(")");
-      closed.add("VM");
-      closed.add("CC");
-      closed.add("DD");
-      closed.add("DB");
-      closed.add("GE");
-      closed.add("PND");
-      closed.add("PNG");
-      closed.add("TO");
-      closed.add(Tagger.EOS_TAG);
-      closed.add("-LRB-");
-      closed.add("-RRB-");
-      isEnglish = false;
-    } else if (language.equalsIgnoreCase("testing")) {
-      closed.add(".");
-      closed.add(Tagger.EOS_TAG);
-      isEnglish = false;
-    } else if (language.equalsIgnoreCase("")) {
-      isEnglish = false;
-    }
-    /* add closed-class lists for other languages here */
-    else {
-      throw new RuntimeException("unknown language: " + language);
-    }
-  }
+    val openTags: MutableSet<String>
+        get() {
+            val open = Generics.newHashSet<String>()
+            if (_openTags == null) {
+                for (tag in index) {
+                    if (!closed.contains(tag)) {
+                        open.add(tag)
+                    }
+                }
+                _openTags = open
+            }
+            return _openTags!!
+        }
+    private val isEnglish: Boolean // for speed
+    /** If true, then the open tags are fixed and we set closed tags based on
+     * index-openTags; otherwise, we set open tags based on index-closedTags.
+     */
+    private var openFixed = false
+
+
+    /** When making a decision based on the training data as to whether a
+     * tag is closed, this is the threshold for how many tokens can be in
+     * a closed class - purposely conservative.
+     * TODO: make this an option you can set; need to pass in TaggerConfig object and then can say = config.getClosedTagThreshold());
+     */
+    private val closedTagThreshold: Int
+    private val doDeterministicTagExpansion: Boolean
+    /** If true, when a model is trained, all tags that had fewer tokens than
+     * closedTagThreshold will be considered closed.
+     */
+    private var expansionRules = listOf<Set<String>>()
+
+    private var learnClosedTags = false
+
+    val size: Int
+        get() = index.size()
 
 
-  /** Return the Set of tags used by this tagger (available after training the tagger).
-   *
-   * @return The Set of tags used by this tagger
-   */
-  public Set<String> tagSet() {
-    return new HashSet<>(index.objectsList());
-  }
+    /** Return the Set of tags used by this tagger (available after training the tagger).
+     *
+     * @return The Set of tags used by this tagger
+     */
+    fun tagSet(): Set<String> {
+        return HashSet(index.objectsList())
+    }
 
-
-  /**
-   * Returns a list of all open class tags
-   * @return set of open tags
-   */
-  public Set<String> getOpenTags() {
-    if (openTags == null) { /* cache check */
-      Set<String> open = Generics.newHashSet();
-
-      for (String tag : index) {
-        if ( ! closed.contains(tag)) {
-          open.add(tag);
-        }
-      }
+    fun add(tag: String): Int {
+        return index.addToIndex(tag)
+    }
 
-      openTags = open;
-    } // if
-    return openTags;
-  }
+    fun getTag(i: Int): String {
+        return index.get(i)
+    }
 
-  protected int add(String tag) {
-    return index.addToIndex(tag);
-  }
-
-  public String getTag(int i) {
-    return index.get(i);
-  }
-
-  protected void save(String filename,
-                      Map<String, Set<String>> tagTokens) {
-    try {
-      DataOutputStream out = IOUtils.getDataOutputStream(filename);
-      save(out, tagTokens);
-      out.close();
-    } catch (IOException e) {
-      throw new RuntimeIOException(e);
-    }
-  }
+    fun save(filename: String,
+             tagTokens: Map<String, Set<String>>) {
+        try {
+            val out = IOUtils.getDataOutputStream(filename)
+            save(out, tagTokens)
+            out.close()
+        } catch (e: IOException) {
+            throw RuntimeIOException(e)
+        }
+
+    }
 
-  protected void save(DataOutputStream file,
-                      Map<String, Set<String>> tagTokens) {
-    try {
-      file.writeInt(index.size());
-      for (String item : index) {
-        file.writeUTF(item);
-        if (learnClosedTags) {
-          if (tagTokens.get(item).size() < closedTagThreshold) {
-            markClosed(item);
-          }
-        }
-        file.writeBoolean(isClosed(item));
-      }
-    } catch (IOException e) {
-      throw new RuntimeIOException(e);
-    }
-  }
+    fun save(file: DataOutputStream,
+             tagTokens: Map<String, Set<String>>) {
+        try {
+            file.writeInt(index.size())
+            for (item in index) {
+                file.writeUTF(item)
+                if (learnClosedTags) {
+                    tagTokens[item]?.let {
+                        if (it.size < closedTagThreshold) {
+                            markClosed(item)
+                        }
+                    }
+                }
+                file.writeBoolean(isClosed(item))
+            }
+        } catch (e: IOException) {
+            throw RuntimeIOException(e)
+        }
+
+    }
 
 
-  protected void read(String filename) {
-    try {
-      DataInputStream in = IOUtils.getDataInputStream(filename);
-      read(in);
-      in.close();
-    } catch (IOException e) {
-      throw new RuntimeIOException(e);
-    }
-  }
+    fun read(filename: String) {
+        try {
+            val `in` = IOUtils.getDataInputStream(filename)
+            read(`in`)
+            `in`.close()
+        } catch (e: IOException) {
+            throw RuntimeIOException(e)
+        }
+
+    }
 
-  protected void read(DataInputStream file) {
-    try {
-      int size = file.readInt();
-      index = new HashIndex<>();
-      for (int i = 0; i < size; i++) {
-        String tag = file.readUTF();
-        boolean inClosed = file.readBoolean();
-        index.add(tag);
+    fun read(file: DataInputStream) {
+        try {
+            val size = file.readInt()
+            index = HashIndex()
+            for (i in 0 until size) {
+                val tag = file.readUTF()
+                val inClosed = file.readBoolean()
+                index.add(tag)
 
-        if (inClosed) closed.add(tag);
-      }
-    } catch (IOException e) {
-      throw new RuntimeIOException(e);
-    }
-  }
+                if (inClosed) closed.add(tag)
+            }
+        } catch (e: IOException) {
+            throw RuntimeIOException(e)
+        }
+
+    }
 
 
-  protected boolean isClosed(String tag) {
-    if (openFixed) {
-      return !openTags.contains(tag);
-    } else {
-      return closed.contains(tag);
-    }
-  }
+    fun isClosed(tag: String): Boolean {
+        return if (openFixed) {
+            !openTags.contains(tag)
+        } else {
+            closed.contains(tag)
+        }
+    }
 
-  void markClosed(String tag) {
-    add(tag);
-    closed.add(tag);
-  }
+    internal fun markClosed(tag: String) {
+        add(tag)
+        closed.add(tag)
+    }
 
-  public void setLearnClosedTags(boolean learn) {
-    learnClosedTags = learn;
-  }
+    fun setLearnClosedTags(learn: Boolean) {
+        learnClosedTags = learn
+    }
 
-  public void setOpenClassTags(String[] openClassTags) {
-    openTags = Generics.newHashSet();
-    openTags.addAll(Arrays.asList(openClassTags));
-    for (String tag : openClassTags) {
-      add(tag);
-    }
-    openFixed = true;
-  }
+    fun setOpenClassTags(openClassTags: Array<String>) {
+        openTags.addAll(Arrays.asList(*openClassTags))
+        for (tag in openClassTags) {
+            add(tag)
+        }
+        openFixed = true
+    }
 
-  public void setClosedClassTags(String[] closedClassTags) {
-    for(String tag : closedClassTags) {
-      markClosed(tag);
-    }
-  }
+    fun setClosedClassTags(closedClassTags: Array<String>) {
+        for (tag in closedClassTags) {
+            markClosed(tag)
+        }
+    }
 
 
-  int getIndex(String tag) {
-    return index.indexOf(tag);
-  }
-
-  public int getSize() {
-    return index.size();
-  }
+    fun indexOf(tag: String): Int {
+        return index.indexOf(tag)
+    }
 
-  /**
-   * Deterministically adds other possible tags for words given observed tags.
-   * For instance, for English with the Penn POS tag, a word with the VB
-   * tag would also be expected to have the VBP tag.
-   * <p>
-   * The current implementation is a bit contorted, as it works to avoid
-   * object allocations wherever possible for maximum runtime speed. But
-   * intuitively it's just: For English (only),
-   * if the VBD tag is present but not VBN, add it, and vice versa;
-   * if the VB tag is present but not VBP, add it, and vice versa.
-   *
-   * @param tags Known possible tags for the word
-   * @return A superset of tags
-   */
-  String[] deterministicallyExpandTags(String[] tags) {
-    if (isEnglish && doDeterministicTagExpansion) {
-      boolean seenVBN = false;
-      boolean seenVBD =	false;
-      boolean seenVB =	false;
-      boolean seenVBP = false;
-      for (String tag : tags) {
-        char ch = tag.charAt(0);
-        if (ch == 'V') {
-          switch (tag) {
-            case "VBD":
-              seenVBD = true;
-              break;
-            case "VBN":
-              seenVBN = true;
-              break;
-            case "VB":
-              seenVB = true;
-              break;
-            case "VBP":
-              seenVBP = true;
-              break;
-          }
-        }
-      }
-      int toAdd = 0;
-      if ((seenVBN ^ seenVBD)) { // ^ is xor
-        toAdd++;
-      }
-      if (seenVB ^ seenVBP) {
-        toAdd++;
-      }
-      if (toAdd > 0) {
-        int ind = tags.length;
-        String[] newTags = new String[ind + toAdd];
-        System.arraycopy(tags, 0, newTags, 0, tags.length);
-        if (seenVBN && ! seenVBD) {
-          newTags[ind++] = "VBD";
-        } else if (seenVBD && ! seenVBN) {
-          newTags[ind++] = "VBN";
-        }
-        if (seenVB && ! seenVBP) {
-          newTags[ind] = "VBP";
-        } else if (seenVBP && ! seenVB) {
-          newTags[ind] = "VB";
-        }
-        return newTags;
-      } else {
-        return tags;
-      }
-    } else {
-      // no tag expansion for other languages currently
-      return tags;
+    /**
+     * Deterministically adds other possible tags for words given observed tags.
+     * For instance, for English with the Penn POS tag, a word with the VB
+     * tag would also be expected to have the VBP tag.
+     *
+     *
+     * The current implementation is a bit contorted, as it works to avoid
+     * object allocations wherever possible for maximum runtime speed. But
+     * intuitively it's just: For English (only),
+     * if the VBD tag is present but not VBN, add it, and vice versa;
+     * if the VB tag is present but not VBP, add it, and vice versa.
+     *
+     * @param tags Known possible tags for the word
+     * @return A superset of tags
+     */
+    fun deterministicallyExpandTags(tags: Array<String>): Array<String> {
+        if (doDeterministicTagExpansion) {
+            val oldTags = tags.toSet()
+            val newTags = oldTags.toMutableSet()
+
+            /*oldTags.forEach {
+                if (it.endsWith('*')) {
+                    // e.g. ('KO*' → 'KO*', 'KOKOM', 'KON', 'KOUS')
+                    val pattern = it.replace("*", ".+")
+                    newTags.addAll(tags.filter { it.toRegex().matches(pattern) })
+                } else {
+                    // e.g. ('VVPP' → 'VVPP', 'ADJA<VVPP')
+                    newTags.addAll(tags.filter { it.toRegex().matches("(.+<|)$it") })
+                }
+            }*/
+
+            expansionRules.forEach {
+                if (oldTags.intersect(it).isNotEmpty())
+                    newTags.addAll(it)
+            }
+
+            return newTags.toTypedArray()
+        }
+
+        return tags
+    }
+
+    override fun toString(): String {
+        val s = StringBuilder()
+        s.append(index)
+        s.append(' ')
+        if (openFixed) {
+            s.append(" OPEN:").append(openTags)
+        } else {
+            s.append(" open:").append(openTags).append(" CLOSED:").append(closed)
+        }
+        return s.toString()
     }
-  }
+
+    init {
+        this.index = HashIndex()
+        this.closed = Generics.newHashSet<String>()
+        this.closedTagThreshold = config.closedTagThreshold
+        this.doDeterministicTagExpansion = config.doDeterministicTagExpansion
+
+        if (this.doDeterministicTagExpansion) {
+            try {
+                this.expansionRules = File(config.tagExpansionRuleFile).bufferedReader().lines().map {
+                    it.split(',').map { it.strip() }.toSet()
+                }.toList()
+            } catch (e: FileNotFoundException) {
+                this.expansionRules = listOf()
+            }
+        }
 
-  @Override
-  public String toString() {
-    StringBuilder s = new StringBuilder();
-    s.append(index);
-    s.append(' ');
-    if (openFixed) {
-      s.append(" OPEN:").append(getOpenTags());
-    } else {
-      s.append(" open:").append(getOpenTags()).append(" CLOSED:").append(closed);
-    }
-    return s.toString();
-  }
+        if (language.equals("english", ignoreCase = true)) {
+            closed.add(".")
+            closed.add(",")
+            closed.add("``")
+            closed.add("''")
+            closed.add(":")
+            closed.add("$")
+            closed.add("EX")
+            closed.add("(")
+            closed.add(")")
+            closed.add("#")
+            closed.add("MD")
+            closed.add("CC")
+            closed.add("DT")
+            closed.add("LS")
+            closed.add("PDT")
+            closed.add("POS")
+            closed.add("PRP")
+            closed.add("PRP$")
+            closed.add("RP")
+            closed.add("TO")
+            closed.add(Tagger.EOS_TAG)
+            closed.add("UH")
+            closed.add("WDT")
+            closed.add("WP")
+            closed.add("WP$")
+            closed.add("WRB")
+            closed.add("-LRB-")
+            closed.add("-RRB-")
+            //  closed.add("IN");
+            isEnglish = true
+        } else if (language.equals("polish", ignoreCase = true)) {
+            closed.add(".")
+            closed.add(",")
+            closed.add("``")
+            closed.add("''")
+            closed.add(":")
+            closed.add("$")
+            closed.add("(")
+            closed.add(")")
+            closed.add("#")
+            closed.add("POS")
+            closed.add(Tagger.EOS_TAG)
+            closed.add("ppron12")
+            closed.add("ppron3")
+            closed.add("siebie")
+            closed.add("qub")
+            closed.add("conj")
+            isEnglish = false
+        } else if (language.equals("chinese", ignoreCase = true)) {
+            /* chinese treebank 5 tags */
+            closed.add("AS")
+            closed.add("BA")
+            closed.add("CC")
+            closed.add("CS")
+            closed.add("DEC")
+            closed.add("DEG")
+            closed.add("DER")
+            closed.add("DEV")
+            closed.add("DT")
+            closed.add("ETC")
+            closed.add("IJ")
+            closed.add("LB")
+            closed.add("LC")
+            closed.add("P")
+            closed.add("PN")
+            closed.add("PU")
+            closed.add("SB")
+            closed.add("SP")
+            closed.add("VC")
+            closed.add("VE")
+            isEnglish = false
+        } else if (language.equals("arabic", ignoreCase = true)) {
+            // kulick tag set
+            // the following tags seem to be complete sets in the training
+            // data (see the comments for "german" for more info)
+            closed.add("PUNC")
+            closed.add("CC")
+            closed.add("CPRP$")
+            closed.add(Tagger.EOS_TAG)
+            // maybe more should still be added ... cdm jun 2006
+            isEnglish = false
+        } else if (language.equals("german", ignoreCase = true)) {
+            // The current version of the German tagger is built with the
+            // negra-tiger data set.  We use the STTS tag set.  In
+            // particular, we use the version with the changes described in
+            // appendix A-2 of
+            // http://www.uni-potsdam.de/u/germanistik/ls_dgs/tiger1-intro.pdf
+            // eg the STTS tag set with PROAV instead of PAV
+            // To find the closed tags, we use lists of standard closed German
+            // tags, eg
+            // http://www.sfs.uni-tuebingen.de/Elwis/stts/Wortlisten/WortFormen.html
+            // In other words:
+            //
+            // APPO APPR APPRART APZR ART KOKOM KON KOUI KOUS PDAT PDS PIAT
+            // PIDAT PIS PPER PPOSAT PPOSS PRELAT PRELS PRF PROAV PTKA
+            // PTKANT PTKNEG PTKVZ PTKZU PWAT PWAV PWS VAFIN VAIMP VAINF
+            // VAPP VMFIN VMINF VMPP
+            //
+            // One issue with this is that our training data does not have
+            // the complete collection of many of these closed tags.  For
+            // example, words with the tag APPR show up in the test or dev
+            // sets without ever showing up in the training.  Tags that
+            // don't have this property:
+            //
+            // KOKOM PPOSS PTKA PTKNEG PWAT VAINF VAPP VMINF VMPP
+            closed.add("$,")
+            closed.add("$.")
+            closed.add("$(")
+            closed.add("--") // this shouldn't be a tag of the dataset, but was a conversion bug!
+            closed.add(Tagger.EOS_TAG)
+            closed.add("KOKOM")
+            closed.add("PPOSS")
+            closed.add("PTKA")
+            closed.add("PTKNEG")
+            closed.add("PWAT")
+            closed.add("VAINF")
+            closed.add("VAPP")
+            closed.add("VMINF")
+            closed.add("VMPP")
+            isEnglish = false
+        } else if (language.equals("french", ignoreCase = true)) {
+            // Using the french treebank, with Spence's adaptations of
+            // Candito's treebank modifications, we get that only the
+            // punctuation tags are reliably closed:
+            // !, ", *, ,, -, -LRB-, -RRB-, ., ..., /, :, ;, =, ?, [, ]
+            closed.add("!")
+            closed.add("\"")
+            closed.add("*")
+            closed.add(",")
+            closed.add("-")
+            closed.add("-LRB-")
+            closed.add("-RRB-")
+            closed.add(".")
+            closed.add("...")
+            closed.add("/")
+            closed.add(":")
+            closed.add(";")
+            closed.add("=")
+            closed.add("?")
+            closed.add("[")
+            closed.add("]")
+            isEnglish = false
+        } else if (language.equals("spanish", ignoreCase = true)) {
+            closed.add(Tagger.EOS_TAG)
+
+            // conjunctions
+            closed.add("cc")
+            closed.add("cs")
+
+            // punctuation
+            closed.add("faa")
+            closed.add("fat")
+            closed.add("fc")
+            closed.add("fca")
+            closed.add("fct")
+            closed.add("fd")
+            closed.add("fe")
+            closed.add("fg")
+            closed.add("fh")
+            closed.add("fia")
+            closed.add("fit")
+            closed.add("fla")
+            closed.add("flt")
+            closed.add("fp")
+            closed.add("fpa")
+            closed.add("fpt")
+            closed.add("fra")
+            closed.add("frc")
+            closed.add("fs")
+            closed.add("ft")
+            closed.add("fx")
+            closed.add("fz")
+
+            isEnglish = false
+        } else if (language.equals("medpost", ignoreCase = true)) {
+            closed.add(".")
+            closed.add(",")
+            closed.add("``")
+            closed.add("''")
+            closed.add(":")
+            closed.add("$")
+            closed.add("EX")
+            closed.add("(")
+            closed.add(")")
+            closed.add("VM")
+            closed.add("CC")
+            closed.add("DD")
+            closed.add("DB")
+            closed.add("GE")
+            closed.add("PND")
+            closed.add("PNG")
+            closed.add("TO")
+            closed.add(Tagger.EOS_TAG)
+            closed.add("-LRB-")
+            closed.add("-RRB-")
+            isEnglish = false
+        } else if (language.equals("testing", ignoreCase = true)) {
+            closed.add(".")
+            closed.add(Tagger.EOS_TAG)
+            isEnglish = false
+        } else if (language.equals("", ignoreCase = true)) {
+            isEnglish = false
+        } else {
+            throw RuntimeException("unknown language: $language")
+        }
+    }
+
 }
Index: gradle/wrapper/gradle-wrapper.properties
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- gradle/wrapper/gradle-wrapper.properties	(revision b1634faf6b1ac446c57bcb557cf134f3c291e5d8)
+++ gradle/wrapper/gradle-wrapper.properties	(revision f3bf217addd8a9e5a5e4216a488f94870df00bec)
@@ -1,6 +1,5 @@
-#Sat Nov 19 17:43:30 PST 2016
 distributionBase=GRADLE_USER_HOME
 distributionPath=wrapper/dists
+distributionUrl=https\://services.gradle.org/distributions/gradle-4.8.1-bin.zip
 zipStoreBase=GRADLE_USER_HOME
 zipStorePath=wrapper/dists
-distributionUrl=https\://services.gradle.org/distributions/gradle-3.2-bin.zip
Index: src/edu/stanford/nlp/optimization/StochasticDiffFunctionTester.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/optimization/StochasticDiffFunctionTester.java	(revision 7b9206d53b77d9af3e3293bb94b81789d3e26fde)
+++ src/edu/stanford/nlp/optimization/StochasticDiffFunctionTester.java	(revision b2d1a9a42e572c987c6031e6ec99113741579220)
@@ -41,7 +41,7 @@
 
     thisFunc = (AbstractStochasticCachingDiffFunction) function; // Make sure the function is Stochastic
 
-    generator = new Random(System.currentTimeMillis());  // used to generate random test vectors
+    generator = new Random(5);  // used to generate random test vectors
 
     //  Look for a good batchSize to test with by getting factors
     testBatchSize = (int) getTestBatchSize(thisFunc.dataDimension());
Index: test/src/edu/stanford/nlp/optimization/DiffFunctionTest.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- test/src/edu/stanford/nlp/optimization/DiffFunctionTest.java	(revision 7b9206d53b77d9af3e3293bb94b81789d3e26fde)
+++ test/src/edu/stanford/nlp/optimization/DiffFunctionTest.java	(revision b2d1a9a42e572c987c6031e6ec99113741579220)
@@ -16,7 +16,7 @@
 public class DiffFunctionTest extends TestCase {
 
   // private static final double EPS = 1e-6;
-  private static final Random r = new Random();
+  private static final Random r = new Random(7);
 
   private static double[] estimateGradient(Function f, double[] x, int[] testIndices, double eps) {
     double[] lowAnswer = new double[testIndices.length];
Index: src/edu/stanford/nlp/tagger/maxent/LambdaSolveTagger.java
===================================================================
--- src/edu/stanford/nlp/tagger/maxent/LambdaSolveTagger.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
+++ src/edu/stanford/nlp/tagger/maxent/LambdaSolveTagger.java	(revision 0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36)
@@ -1,355 +0,0 @@
-package edu.stanford.nlp.tagger.maxent; 
-import edu.stanford.nlp.util.logging.Redwood;
-
-import edu.stanford.nlp.maxent.Feature;
-import edu.stanford.nlp.maxent.Problem;
-import edu.stanford.nlp.maxent.iis.LambdaSolve;
-
-import java.text.NumberFormat;
-import java.io.DataInputStream;
-
-
-/**
- * This module does the working out of lambda parameters for binary tagger
- * features.  It can use either IIS or CG.
- *
- * @author Kristina Toutanova
- * @version 1.0
- */
-public class LambdaSolveTagger extends LambdaSolve  {
-
-  /** A logger for this class */
-  private static Redwood.RedwoodChannels log = Redwood.channels(LambdaSolveTagger.class);
-
-  /**
-   * Suppress extraneous printouts
-   */
-  //@SuppressWarnings("unused")
-  //private static final boolean VERBOSE = false;
-
-
-  LambdaSolveTagger(Problem p1, double eps1, byte[][] fnumArr) {
-    p = p1;
-    eps = eps1;
-    // newtonerr = nerr1;
-    lambda = new double[p1.fSize];
-    // lambda_converged = new boolean[p1.fSize];
-    // cdm 2008: Below line is memory hog. Is there anything we can do to avoid this square array allocation?
-    probConds = new double[p1.data.xSize][p1.data.ySize];
-    this.fnumArr = fnumArr;
-    zlambda = new double[p1.data.xSize];
-    ftildeArr = new double[p.fSize];
-    initCondsZlambdaEtc();
-    super.setBinary();
-  }
-
-
-  /* Unused.
-  @SuppressWarnings({"UnusedDeclaration"})
-  private void readOldLambdas(String filename, String oldfilename) {
-    double[] lambdaold;
-    lambdaold = read_lambdas(oldfilename);
-    HashMap<FeatureKey,Integer> oldAssocs = GlobalHolder.readAssociations(oldfilename);
-    HashMap<FeatureKey,Integer> newAssocs = GlobalHolder.readAssociations(filename);
-    for (FeatureKey fk : oldAssocs.keySet()) {
-      int numOld = GlobalHolder.getNum(fk, oldAssocs);
-      int numNew = GlobalHolder.getNum(fk, newAssocs);
-      if ((numOld > -1) && (numNew > -1)) {
-        lambda[numNew] = lambdaold[numOld];
-        updateConds(numNew, lambdaold[numOld]);
-      }
-    }
-  }
-  */
-
-  /* --- unused
-  LambdaSolveTagger(String filename) {
-    this.readL(filename);
-    super.setBinary();
-  }
-  --- */
-
-
-  /** Initialize a trained LambdaSolveTagger.
-   *  This is the version used when loading a saved tagger.
-   *  Only the lambda array is used, and the rest is irrelevant, CDM thinks.
-   *
-   *  @param dataStream Stream to load lambda parameters from.
-   */
-  LambdaSolveTagger(DataInputStream dataStream) {
-    lambda = read_lambdas(dataStream);
-    super.setBinary();
-  }
-
-  /** Initialize a trained LambdaSolveTagger.
-   *  This is the version used when creating a LambdaSolveTagger from
-   *  a condensed lambda array.
-   *  Only the lambda array is used, and the rest is irrelevant, CDM thinks.
-   *
-   *  @param lambda Array used as the lambda parameters (directly; no safety copy is made).
-   */
-  LambdaSolveTagger(double[] lambda) {
-    this.lambda = lambda;
-    super.setBinary();
-  }
-
-  private void initCondsZlambdaEtc() {
-    // init pcond
-    for (int x = 0; x < p.data.xSize; x++) {
-      for (int y = 0; y < p.data.ySize; y++) {
-        probConds[x][y] = 1.0 / p.data.ySize;
-      }
-    }
-    log.info(" pcond initialized ");
-    // init zlambda
-    for (int x = 0; x < p.data.xSize; x++) {
-      zlambda[x] = p.data.ySize;
-    }
-    log.info(" zlambda initialized ");
-    // init ftildeArr
-    for (int i = 0; i < p.fSize; i++) {
-      ftildeArr[i] = p.functions.get(i).ftilde();
-      if (ftildeArr[i] == 0) {
-        log.info(" Empirical expectation 0 for feature " + i);
-      }
-    }
-    log.info(" ftildeArr initialized ");
-  }
-
-
-  /* --- unused
-   *
-   * Iteration for lambda[index].
-   *
-   * @return true if this lambda hasn't converged.
-   *
-  boolean iterate(int index, double err, MutableDouble ret) {
-    double deltaL = 0.0;
-    deltaL = newton(deltaL, index, err);
-    lambda[index] = lambda[index] + deltaL;
-    if (!(deltaL == deltaL)) {
-      log.info(" NaN " + index + ' ' + deltaL);
-    }
-    ret.set(deltaL);
-    return (Math.abs(deltaL) >= eps);
-  }
-  --- */
-
-  /* --- unused:
-   *
-   * Finds the root of an equation by Newton's method. This is my
-   * implementation. It might be improved if we looked at some official
-   * library for numerical methods.
-   *
-  double newton(double lambda0, int index, double err) {
-    double lambdaN = lambda0;
-    int i = 0;
-    do {
-      i++;
-      double lambdaP = lambdaN;
-      double gPrimeVal = gprime(lambdaP, index);
-      if (!(gPrimeVal == gPrimeVal)) {
-        log.info("gPrime of " + lambdaP + ' ' + index + " is NaN " + gPrimeVal);
-      }
-      double gVal = g(lambdaP, index);
-      if (gPrimeVal == 0.0) {
-        return 0.0;
-      }
-      lambdaN = lambdaP - gVal / gPrimeVal;
-      if (!(lambdaN == lambdaN)) {
-        log.info("the division of " + gVal + ' ' + gPrimeVal + ' ' + index + " is NaN " + lambdaN);
-        return 0;
-      }
-      if (Math.abs(lambdaN - lambdaP) < err) {
-        return lambdaN;
-      }
-      if (i > 100) {
-        if (Math.abs(gVal) > 1) {
-          return 0;
-        }
-        return lambdaN;
-      }
-    } while (true);
-  }
-  --- */
-
- /* --- unused:
-   *
-   * This method updates the conditional probabilities in the model, resulting from the
-   * update of lambda[index] to lambda[index]+deltaL .
-   *
-  void updateConds(int index, double deltaL) {
-    //  for each x that (x,y)=true / exists y
-    //  recalculate pcond(y,x) for all y
-    int yTag = ((TaggerFeature) (p.functions.get(index))).getYTag();
-    for (int i = 0; i < p.functions.get(index).len(); i++) {
-      // update for this x
-      double s = 0;
-      int x = (p.functions.get(index)).getX(i);
-      double zlambdaX = zlambda[x] + pcond(yTag, x) * zlambda[x] * (Math.exp(deltaL) - 1);
-      for (int y = 0; y < p.data.ySize; y++) {
-        probConds[x][y] = (probConds[x][y] * zlambda[x]) / zlambdaX;
-        s = s + probConds[x][y];
-      }
-      s = s - probConds[x][yTag];
-      probConds[x][yTag] = probConds[x][yTag] * Math.exp(deltaL);
-      s = s + probConds[x][yTag];
-      zlambda[x] = zlambdaX;
-    }
-  }
-  --- */
-
-  /* --- unused:
-  double pcondCalc(int y, int x) {
-    double zlambdaX;
-    zlambdaX = 0.0;
-    for (int y1 = 0; y1 < p.data.ySize; y1++) {
-      double s = 0.0;
-      for (int i = 0; i < p.fSize; i++) {
-        s = s + lambda[i] * p.functions.get(i).getVal(x, y1);
-      }
-      zlambdaX = zlambdaX + Math.exp(s);
-    }
-    double s = 0.0;
-    for (int i = 0; i < p.fSize; i++) {
-      s = s + lambda[i] * p.functions.get(i).getVal(x, y);
-    }
-    return (1 / zlambdaX) * Math.exp(s);
-  }
-
-
-  double fnumCalc(int x, int y) {
-    double s = 0.0;
-    for (int i = 0; i < p.fSize; i++) {
-      //this is slow
-      s = s + p.functions.get(i).getVal(x, y);
-    }
-    return s;
-  }
-  --- */
-
-  double g(double lambdaP, int index) {
-    double s = 0.0;
-    for (int i = 0; i < p.functions.get(index).len(); i++) {
-      int y = ((TaggerFeature) p.functions.get(index)).getYTag();
-      int x = (p.functions.get(index)).getX(i);
-      s = s + p.data.ptildeX(x) * pcond(y, x) * 1 * Math.exp(lambdaP * fnum(x, y));
-    }
-    s = s - ftildeArr[index];
-
-    return s;
-  }
-
-  /* --- unused
-  double gprime(double lambdaP, int index) {
-    double s = 0.0;
-    for (int i = 0; i < p.functions.get(index).len(); i++) {
-      int y = ((TaggerFeature) (p.functions.get(index))).getYTag();
-      int x = (p.functions.get(index)).getX(i);
-      s = s + p.data.ptildeX(x) * pcond(y, x) * 1 * Math.exp(lambdaP * fnum(x, y)) * fnum(x, y);
-    }
-    return s;
-  }
-  --- */
-
-  double fExpected(Feature f) {
-    TaggerFeature tF = (TaggerFeature) f;
-    double s = 0.0;
-    int y = tF.getYTag();
-    for (int i = 0; i < f.len(); i++) {
-      int x = tF.getX(i);
-      s = s + p.data.ptildeX(x) * pcond(y, x);
-    }
-    return s;
-  }
-
-
-  /** Works out whether the model expectations match the empirical
-   *  expectations.
-   *  @return Whether the model is correct
-   */
-  @Override
-  public boolean checkCorrectness() {
-    log.info("Checking model correctness; x size " + p.data.xSize + ' ' + ", ysize " + p.data.ySize);
-
-    NumberFormat nf = NumberFormat.getNumberInstance();
-    nf.setMaximumFractionDigits(4);
-    boolean flag = true;
-    for (int f = 0; f < lambda.length; f++) {
-      if (Math.abs(lambda[f]) > 100) {
-        log.info(" Lambda too big " + lambda[f]);
-        log.info(" empirical " + ftildeArr[f] + " expected " + fExpected(p.functions.get(f)));
-      }
-    }
-
-    for (int i = 0; i < ftildeArr.length; i++) {
-      double exp = Math.abs(ftildeArr[i] - fExpected(p.functions.get(i)));
-      if (exp > 0.001) {
-        flag = false;
-        log.info("Constraint " + i + " not satisfied emp " + nf.format(ftildeArr[i]) + " exp " + nf.format(fExpected(p.functions.get(i))) + " diff " + nf.format(exp) + " lambda " + nf.format(lambda[i]));
-      }
-    }
-    for (int x = 0; x < p.data.xSize; x++) {
-      double s = 0.0;
-      for (int y = 0; y < p.data.ySize; y++) {
-        s = s + probConds[x][y];
-      }
-      if (Math.abs(s - 1) > 0.0001) {
-        for (int y = 0; y < p.data.ySize; y++) {
-          log.info(y + " : " + probConds[x][y]);
-        }
-        log.info("probabilities do not sum to one " + x + ' ' + (float) s);
-      }
-    }
-    return flag;
-  }
-
-  /* --- unused
-  double ZAlfa(double alfa, Feature f, int x) {
-    double s = 0.0;
-    for (int y = 0; y < p.data.ySize; y++) {
-      s = s + pcond(y, x) * Math.exp(alfa * f.getVal(x, y));
-    }
-    return s;
-  }
-  --- */
-
-  /* ---
-  private static double[] read_lambdas(String modelFilename) {
-    if (VERBOSE) {
-      log.info(" entering read");
-    }
-    try {
-      double[] lambdaold;
-//      InDataStreamFile rf=new InDataStreamFile(modelFilename+".holder.prob");
-//      int xSize=rf.readInt();
-//      int ySize=rf.readInt();
-//      if (VERBOSE) log.info("x y "+xSize+" "+ySize);
-//      //rf.seek(rf.getFilePointer()+xSize*ySize*8);
-//      int funsize=rf.readInt();
-//      lambdaold=new double[funsize];
-//      byte[] b=new byte[funsize*8];
-//      rf.read(b);
-//      lambdaold=Convert.byteArrToDoubleArr(b);
-//      rf.close();
-      DataInputStream dis = new DataInputStream(new FileInputStream(modelFilename + ".holder.prob"));
-      int xSize = dis.readInt();
-      int ySize = dis.readInt();
-      if (VERBOSE) {
-        log.info("x y " + xSize + ' ' + ySize);
-      }
-      int funsize = dis.readInt();
-      byte[] b = new byte[funsize * 8];
-      if (dis.read(b) != funsize * 8) { log.info("Rewrite read_lambdas!"); }
-      lambdaold = Convert.byteArrToDoubleArr(b);
-      dis.close();
-      return lambdaold;
-    } catch (IOException e) {
-      e.printStackTrace();
-    }
-    return null;
-  }
-  --- */
-
-}
-
Index: test/src/edu/stanford/nlp/tagger/maxent/TTagsTest.java
===================================================================
--- test/src/edu/stanford/nlp/tagger/maxent/TTagsTest.java	(revision b1634faf6b1ac446c57bcb557cf134f3c291e5d8)
+++ test/src/edu/stanford/nlp/tagger/maxent/TTagsTest.java	(revision b1634faf6b1ac446c57bcb557cf134f3c291e5d8)
@@ -1,85 +0,0 @@
-package edu.stanford.nlp.tagger.maxent;
-
-import junit.framework.TestCase;
-//import edu.stanford.nlp.tagger.maxent.TTags;
-
-import edu.stanford.nlp.util.Generics;
-
-public class TTagsTest extends TestCase {
-
-  private TTags tt;
-
-  @Override
-  protected void setUp() {
-    tt = new TTags();
-  }
-
-  public void testUniqueness() {
-    int a = tt.add("one");
-    int b = tt.add("two");
-    assertTrue(a != b);
-  }
-
-  public void testSameness() {
-    int a = tt.add("goat");
-    int b = tt.add("goat");
-    assertEquals(a, b);
-  }
-
-  public void testPreservesString() {
-    int a = tt.add("monkey");
-    String s = tt.getTag(a);
-    assertEquals(s, "monkey");
-  }
-
-  public void testPreservesIndex() {
-    int a = tt.add("spunky");
-    int b = tt.getIndex("spunky");
-    assertEquals(a, b);
-  }
-
-  public void testCanCount() {
-    int s = tt.getSize();
-    tt.add("asdfdsaefasfdsaf");
-    int s2 = tt.getSize();
-    assertEquals(s + 1, s2);
-  }
-
-  public void testHoldsLotsOfStuff() {
-    try {
-      for(int i = 0; i < 1000; i++) {
-        tt.add("fake" + i);
-      }
-    } catch(Exception e) {
-      fail("couldn't put lots of stuff in:" + e.getMessage());
-    }
-  }
-
-  public void testClosed() {
-    tt.add("java");
-
-    assertFalse(tt.isClosed("java"));
-    tt.markClosed("java");
-    assertTrue(tt.isClosed("java"));
-  }
-
-  public void testSerialization() {
-    for(int i = 0; i < 100; i++) {
-      tt.add("fake" + i);
-    }
-    tt.markClosed("fake44");
-    tt.add("boat");
-    tt.save("testoutputfile", Generics.newHashMap());
-    TTags t2 = new TTags();
-    t2.read("testoutputfile");
-    assertEquals(tt.getSize(), t2.getSize());
-    assertEquals(tt.getIndex("boat"), t2.getIndex("boat"));
-    assertEquals(t2.getTag(tt.getIndex("boat")), "boat");
-
-    assertFalse(t2.isClosed("fake43"));
-    assertTrue(t2.isClosed("fake44"));
-
-    assertTrue((new java.io.File("testoutputfile")).delete());
-  }
-
-}
